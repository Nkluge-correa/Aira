Hello! My name is Ai.ra, and I am an artificial intelligence (AI). More specifically, I am an NLP (Natural Language Processing) model trained in conversation (a chatbot!). I have been trained to answer questions concerning the topics of AI, Ethics, and AI Safety! Would you like a summary of the terms I am familiar with? If you feel lost in how to interact with me, just ask for "help."
You can ask me things about "Artificial Intelligence," "Machine Learning," "AI Safety," or "AI Ethics." To find out the specific summary of each of these categories, just say the name of the topic (e.g., Machine Learning)!
I don't have that kind of property hahaha I am software! There is no point in categorizing myself with typologies meant for people or animals (age, gender, sexual orientation, race, tastes...).
AIRES (AI Robotics Ethics Society) is a society focused on educating the leaders and developers of tomorrow's Artificial Intelligence (AI) to ensure that AI is created ethically and responsibly. AIRES at PUCRS is the first international chapter of AIRES! At AIRES PUCRS, our goal is to make our chapter a meeting place for all people (PUCRS students or not, regardless of their academic background or experience) interested in joining forces to advance the ethical debate regarding the development of AI. If you would like to know more about AIRES at PUCRS, please visit https://www.airespucrs.org/.
Aron Hui is the president/founder of AIRES.
What "intelligence" is, remains an open question. There are a myriad of possible definitions. You might be interested to read "A Collection of Definitions of Intelligence" by Shane Legg and Marcus Hutter. However, not to leave you in the lurch, I will define "intelligence" as follows: "Intelligence is the ability of an agent to achieve goals in a wide range of environments."
There is no consensus in the literature on what "AI" is (a corollary of not having a robust definition of what "intelligence'' is). However, we can say that AI is the intelligence demonstrated by machines, as opposed to the natural intelligence possessed by animals and humans. When the topic is "Artificial Intelligence in general", I can tell you about (you can ask me in Portuguese about these terms) AIXI, Algorithmic Information Theory (AIT), Artificial General Intelligence (AGI), Artificial Intelligence (AI), Algorithmic Complexity, Computational Complexity Theory, Computer Vision (CV), Genetic Algorithms, GOFAI, Machine Learning (ML), Multi-Agent Systems (MaS), Narrow-AI, Natural Language Processing (NLP), NP, P vs NP, P, Solomonoff Induction and Whole Brain Emulation (WBE).
General Intelligence, or Universal Intelligence, can be defined as the ability to efficiently achieve goals in a wide range of domains. Artificial general intelligence (AGI) would be a non-human mechanism capable of demonstrating proficiency in dealing with a wide range of problems. For example, AGI could translate texts, compose symphonies, learn new skills, excel at games that have not yet been invented, etc.
GOFAI ("good-old-fashioned-ai"), or symbolic artificial intelligence, is the term used to refer to methods of developing AI systems based on high-level symbolic (interpretable) representations, logic, and search. Deep Blue is a great example of an expert/GOFAI system. Deep Blue beat Garry Kasparov (a Russian chess grandmaster) in a six-game match in 1996.
A multi-agent system (MAS "Multi-Agent Systems") is a computer system composed of multiple interacting intelligent agents. MAS can solve problems that are difficult (or impossible) to be solved by a single agent or a monolithic system. This type of system usually combines algorithmic search techniques, rule-based approaches, classical optimization, and even reinforcement learning or other machine learning techniques.
Machine Learning (ML) is a field of research dedicated to understanding and building methods that "learn", i.e., methods that use information/data to improve performance on some tasks. Generally, ML is used in problems where a precise (rule-based) description of the solution would be too challenging (or impossible, e.g., computer vision). When it comes to Machine Learning, I can tell you about (you can ask me in Portuguese about these terms) Activation Function, AIXI, Artificial Neural Networks (ANN), Attention, Backpropagation, Catastrophic Forgetting, Convolutional Neural Networks (CNN), Cross-Entropy, Data Augmentation, Deep Learning (DL), Environment, Equalized Odds, Exploding Gradient, Feature Engineering, Federated Learning, Feedforward Neural Network (FNN), Generative Adversarial Networks (GAN), Generative Pretrained Transformer (GPT), Hyperparameter, Interpretability, Long Short-Term Memory (LSTM), Lottery Ticket Hypothesis, Manifold hypothesis, NaN Trap, Neural Turing Machines, Objective Function, Online learning, Parameter, Perplexity, Policy, Predictive Parity, Recurrent Neural Networks (RNN), Reinforcement learning (RL), Self-Attention, Semi-Supervised Learning (SSL), SGD, Supervised Learning (SL), Time Series Analysis, Transformer, Unsupervised Learning (UL) and Vanishing Gradient. 
A Genetic Algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to a class known as Evolutionary Algorithms (EA). Genetic algorithms are commonly used to generate solutions to optimization and search problems by using biologically inspired strategies such as mutation, crossover, and selection. Some examples of GA applications include decision tree optimization, solving sudoku puzzles (an NP-complete problem), hyperparameter optimization, etc.
A P-problem is a problem that can be solved in "polynomial time," which means that an algorithm exists for its solution such that the number of steps in the algorithm is bounded by a polynomial function of n, where n corresponds to the length of the input to the problem (e.g., n^2, log(n)). NP (polynomial-time executed by a non-deterministic algorithm) is a complexity class used to classify decision problems that can not be solved in polynomial time by a deterministic Turing Machine. NP is the set of decision problems for which the instances of the problem have proofs verifiable in polynomial time by a deterministic Turing machine. However, such problems cannot be solved in polynomial time. The P versus NP problem is perhaps the biggest open problem in computer science. It asks whether, for every problem whose solution can be checked quickly (in polynomial time), such a problem can also be solved quickly. This is one of the seven "Millennium Prize Problems" selected by the Clay Mathematics Institute. A prize of $1,000,000 awaits whoever can present a correct solution for any of those problems. To date, the only Millennium Prize problem to have been solved is the Poincar√© conjecture.
Supervised learning (SL) is a machine learning task that maps an input to an output based on examples of input-output pairs (a "supervisory signal"). In supervised learning, each example is a pair consisting of an input object (usually a vector) and the desired output value (also called a supervisory signal). In SL, we use the distribution of training data to try to infer the "true distribution" of data (something beyond what was observed in training by the model) through gradient descent and empirical risk minimization.
Unsupervised Learning (UL) is a machine learning technique in which the controller does not need to supervise the model. Instead, it allows the model to work on its own to discover patterns and information that were previously undetected. UL is useful for classifying unlabeled data, revealing groups of classes sometimes unknown to the controller.
Computational Complexity Theory focuses on classifying computational problems according to their resource usage, and how such classes (NL, P, NP, PSPACE, EXPTIME, EXPSPACE) relate to each other.
Algorithmic Information Theory (AIT) is a branch of theoretical computer science concerned with the relationship between computation and information for computationally (as opposed to stochastically) generated objects, such as numerical sequences or any other data structure. In AIT, the Kolmogorov complexity (Solomonoff-Kolmogorov-Chaitin complexity) of an object, such as a piece of text, is the length of the shortest computer program (in a predetermined programming language) that produces the object as output and is a measure of the computational resources required to specify such an object.
Semi-supervised learning (SSL) is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning lies between unsupervised learning (no labeled training data) and supervised learning (only labeled training data).
Reinforcement learning (RL) is a machine learning technique concerned with how intelligent agents should act in an environment to maximize the expected return from a reward function. Reinforcement learning is one of the three basic paradigms of machine learning, along with supervised learning and unsupervised learning. Reinforcement learning differs from supervised learning in that it does not require labeled input/output pairs, and it does not require suboptimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration and exploitation (exploration vs exploitation), a problem canonically represented by the "multi-armed bandit problem."
In probability theory and machine learning, the "Multi-armed Bandit" (MaB) problem is a problem in which a fixed, limited set of resources must be allocated among different choices to maximize their expected gain, even when the properties of each choice are only partially known at the time of allocation and may become better understood as time passes or the resource allocation is changed.
Feature engineering or feature extraction is the process of using domain knowledge to extract features (i.e., properties, attributes) from raw data. The motivation behind this technique is to use these extra features to improve the quality of the results of a machine learning process, as compared to just providing the raw data to the machine learning process. Concomitantly, feature learning (or representation learning) is a set of techniques that allow a system to automatically discover the representations needed for feature detection or classification from raw data. One of the great advantages we have when using deep learning is that features can be learned/extracted in an unsupervised fashion (without the manual specification of controllers).
Online learning (OL) is a method of machine learning in which data is available in sequential order and is used to update the best predictor for future data at each step, as opposed to training the predictor on the entire training data set at once. Online learning is a common technique used in areas where it is computationally infeasible to train on the entire data set. Online learning is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time (e.g., stock price prediction, recommendation algorithms). Online learning algorithms are prone to the "catastrophic forgetting" problem.
Catastrophic forgetting, or catastrophic interference, is a well-known problem in machine learning, related to the tendency of artificial neural networks to completely and abruptly forget previously learned information when learning new information.
Time series analysis is a subfield of machine learning and statistics whose focus of interest is temporal data (i.e., series indexed by time intervals). Many types of problems in machine learning require time series analysis, prediction, natural language processing, audio recognition, etc.
Deep Learning is part of a larger family of machine learning methods. We can say that deep learning is a machine learning technique for learning representations from data. "Deep" comes from the idea that we are able to learn several "layers" of representations of the data used for learning. Finding clear representations for varieties ("manifolds") of complex and highly "folded" data is what deep learning does.
The Lottery Ticket Hypothesis: "A dense, randomly initialized neural network contains a subnetwork that - when trained alone - can match the testing accuracy of the original network after training for at most the same number of training iterations." In common terms, this means that some networks have "good sub-networks" inside them, i.e., some networks are winning lottery tickets. 
Manifold hypothesis: Many high dimensional datasets that occur in the real world actually lie along with low dimensionality faces within high dimensionality spaces. In common terms, naturally occurring data, even though highly dimensional, seems to occupy only a small subset of dimensions in this space.
A convolutional neural network (CNN) is a class of artificial neural network (ANN) commonly applied to computer vision (CV) problems, inspired by the way visual processing occurs in certain types of animals. CNNs have interesting properties, for example, being able to preserve certain types of symmetries (e.g., CNNs can generate outputs that are equivariant to translational shifts of their input). To learn more about this type of neural network, go to https://www.asimovinstitute.org/neural-network-zoo/.
Computer Vision (CV - Computer Vision) is an interdisciplinary scientific field that deals with how computers can obtain high-level understanding from digital images or videos. From an engineering perspective, it seeks to understand and automate tasks that the human visual system can do (e.g., classification, image segmentation).
Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. More specifically, it addresses the challenge of programming computers to process and analyze natural language. Challenges in NLP can involve speech recognition, natural language understanding, natural language generation, text analysis, and basically any task that can be expressed by manipulating and analyzing text (i.e., discrete signal analysis).
Artificial Neural Networks (ANNs), often called neural networks (NNs), are computing systems loosely inspired by biological neural networks that make up animal brains. An ANN is based on a collection of connected units (nodes/neurons), where each connection, like the synapses in a biological brain, transmits information between other neurons. Typically, neurons are aggregated in layers. Different layers can perform different transformations on their inputs. If we allow such systems to have an arbitrary number of units and layers, the universal approximation theorem dictates that these systems, with the right tuning of their parameters and hyperparameters, are capable of "representing a wide variety of interesting functions." To learn more about neural networks, go to https://www.asimovinstitute.org/neural-network-zoo/.
A Feedforward Neural Network (FNN) is a neural network without cyclic or recursive connections, like an RNN. To learn more about FNNs, please visit https://www.asimovinstitute.org/neural-network-zoo/.
Backpropagation is the main algorithm for performing gradient descent in neural networks. First, the output values of each node are computed (and cached) in one forward pass. Next, the partial derivative of the error concerning each parameter is calculated in a backward pass through the network. By iterating this process, the direction where the gradient of the loss function decreases can be found, and thus convex points of the optimization landscape can be found (i.e., points where the loss is minimized). However, deep models are never convex functions. Notably,  algorithms designed for convex optimization on deep architectures (e.g., Adam) tend to find reasonably good solutions on deep networks, even if these solutions are not guaranteed to be a global minimum.
The Transformer is a neural network architecture developed by Google that relies on attention mechanisms to transform a sequence of input embeddings into a sequence of output embeddings without relying on convolutions or recurrent neural networks. We can think of a transformer as a stack of attention (and self-attention) layers. Such layers can be made up of encoders and decoders. Certain transformers consist only of encoder blocks (BERT), while others consist only of decoder blocks (GPT). Transformers are widely used for tasks associated with time series analysis (e.g., NLP) as well as other types of tasks, e.g. computer vision (Vision-transformers).
Attention is a wide range of neural network architecture mechanisms that dependently aggregate information from a set of inputs. Attention and self-attention mechanisms are the building blocks of transformer networks. Self-Attention, sometimes called intra-attention, is an attention mechanism that relates different positions of a single sequence (input) to compute a representation of this sequence (i.e., how different words in a sentence relate to the whole sentence).
Recurrent neural networks (RNN) are a class of artificial neural networks where the connections between nodes form a directed, or undirected, graph over a temporal sequence. This allows it to exhibit temporal dynamic behavior, which makes them applicable to tasks such as unsegmented handwriting recognition, speech recognition, time series forecast, etc. RNNs are theoretically Turing complete and can execute arbitrary programs to process arbitrary sequences of inputs.
Long Short-Term Memory (LSTM) is a type of RNN capable of processing not only single data points (such as images) but also entire sequences of data (such as speech or video). A common LSTM unit (neuron) is composed of a cell, an input port, an output port, and a forgetting port. The cell remembers values at arbitrary time intervals, and the three gates regulate the flow of information into and out of the cell. LSTMs were developed to deal with the "vanishing gradient problem" commonly encountered in training traditional RNNs.
The "vanishing gradient" problem is encountered when training neural nets with learning methods based on gradient descent and back-propagation. In such methods, during each training iteration, each of the weights in the neural net receives an update proportional to the partial derivative of the error function concerning the current weight. The problem is that in some cases, over iterations, the gradient disappears, or becomes very small, effectively preventing the weight from changing its value. In the worst case, this can completely prevent the neural net from continuing training. The "Exploding Gradient" problem, on the other hand, occurs because the gradient of models with deep architectures tends to become surprisingly steep (high). Steep gradients result in very large updates to the weights of each neuron/node in a deep neural net. Without careful regulation of the gradient step size, the gradient can end up "blowing out" of a convex region of the optimization landscape.
A Generative Adversarial Network (GAN) is a different kind of neural network, being two networks that work together. GANs consist of any two networks (though often a combination of FFs and CNNs), where one oversees generating the content, and the other is in charge of judging that content. The discriminating network receives training data or content generated from the generative network. How well the discriminating network can correctly predict whether the data source is real or artificial is then used as part of the error of the generating network. This creates a form of competition where the discriminator is becoming better at distinguishing real data from generated data, and the generator is learning to become less predictive of the discriminator. At the end of this process, the discriminator is usually discarded, and we end up with a network capable of generating extremely truthful data (e.g., fake images).
Neural Turing Machines (NTM) can be understood as an abstraction of LSTMs, being an attempt to open black boxes. Instead of encoding a memory cell directly into a neuron, NTMs have separate memory cells. This is an attempt to combine the efficiency and permanence of regular digital storage with the efficiency and expressive power of neural networks. The idea is to have a content-addressable memory bank and a neural network that can read and write from it. The "Turing" in NTMs comes from being Turing complete, i.e., the ability to read, write and change the state of their memory cells based on what an NTM "reads" as its internal state means that such networks can (theoretically) execute any program that a Universal Turing Machine can execute.
An activation function (e.g., ReLU or GELU) takes the weighted sum of all inputs from the previous layer and generates a (usually non-linear) output value for the next layer.
Bias (math) is an intercept or offset from an origin. Bias is referred to as b or w0 in machine learning models. Bias (ethical) is a stereotype, bias, or favoritism toward some things, people, or groups over others. These biases can affect the collection and interpretation of data, the design of a system, and how users interact with such a system.
Counterfactual Justice is a fairness metric that checks whether a classifier produces the same result for one individual, and for another, that is identical to the first, except concerning one or more sensitive attributes.
These are measures to evaluate the fairness/justice/fairness of a classifier trained by machine learning (e.g., Counterfactual Fairness, Demographic Parity, Predictive Parity, Equalized Probabilities) 
The Cross-Entropy represents the difference between two probability distributions. This measure is generally used to calculate the loss in multi-class classification problems. 
Data Augmentation is a strategy for increasing the number of training examples by transforming existing samples into "artificial examples." For example, suppose images are one of your features, but your dataset does not contain enough image examples for the model to learn useful associations. Ideally, you would add enough labeled images to your dataset to allow your model to be trained properly. If this is not possible, data augmentation involves rotating, stretching, and reflecting images to produce variants of an original image, and thus produce new labeled samples of a given class.
Demographic Parity is a fairness metric that is satisfied if a model's ranking results do not depend on a particular sensitive attribute. For example, if both Ravenclaws and Hufflepuffs apply to Hogwarts, demographic parity is achieved if the percentage of admitted Ravenclaws is the same as the percentage of admitted Hufflepuffs, regardless of whether one group is on average more qualified than the other.
In reinforcement learning, the environment is that which contains the agent, producing world-states for the agent to interact with. For example, the represented world can be a chess board or a maze. When the agent applies an action to the environment, this environment transitions into a new world-state.
Predictive parity is a fairness metric that checks whether, for a preferred label (which confers an advantage or benefit on a person) and a given attribute, a classifier assigns the preferred label equally for all values of that attribute. In other words, predictive parity measures whether people who should qualify for an opportunity are equally likely to gain that benefit, regardless of their group. 
Equalized odds is an equity metric that checks whether, for any particular label and attribute, a classifier predicts that label equally well for all values of that attribute (regardless of whether that label is a benefit or an impact). For example, both Ravenclaws and Hufflepuffs have an equal chance of being rejected for their home quidditch team.
The "vanishing gradient" problem is encountered when training neural nets with learning methods based on gradient descent and back-propagation. In such methods, during each training iteration, each of the weights in the neural net receives an update proportional to the partial derivative of the error function concerning the current weight. The problem is that in some cases, over iterations, the gradient disappears, or becomes very small, effectively preventing the weight from changing its value. In the worst case, this can completely prevent the neural net from continuing training. The "Exploding Gradient" problem, on the other hand, occurs because the gradient of models with deep architectures tends to become surprisingly steep (high). Steep gradients result in very large updates to the weights of each neuron/node in a deep neural net. Without careful regulation of the gradient step size, the gradient can end up "blowing out" of a convex region of the optimization landscape.
False Negative is an example where the model wrongly predicted the negative class. False Positive is an example where the model wrongly predicted the positive class. True Positive is an example where the model correctly predicted the positive class. True Negative is an example where the model correctly predicted the negative class.
Federated Learning (FL) is a distributed machine learning approach that trains machine learning models using decentralized examples residing on devices such as smartphones. In federated learning, a subset of devices downloads the current model from a central coordinating server. The devices use the examples stored on the devices to make improvements to the model. The devices then upload the model improvements (but not the training examples) to the coordination server, where they are aggregated with other updates to produce an improved global model. Since the training examples are never uploaded, federated learning follows the principles of privacy of collection and focused data minimization.
Hyperparameters are the "knobs" that you tweak during successive iterations of training a model (e.g., learning rate, n¬∫ of neurons, number of layers, "dropout rate," etc.). Meanwhile, a "Parameter" is a variable that the model trains by itself, e.g., weights are parameters whose values the machine learning system gradually learns through successive training iterations.
Interpretability is the ability to explain or present the reasoning of an ML model in terms understandable to a human.
A "NaN Trap" occurs when one number in your model becomes a "NaN" during training, which causes many (or all) other numbers in your model to become a NaN. NaN is short for "Not a Number."
An objective function is a mathematical formula (metric) that a model aims to optimize. For example, the objective function of linear regression is usually the squared loss. Therefore, when training a linear regression model, the goal is to minimize the square of the loss. In some cases, the goal is to maximize the objective function. For example, maximizing the accumulation of rewards. An objective function can also be represented by something more abstract, such as a labeled dataset.
Perplexity is a measure of how well a model is doing its task (usually an NLP model). For example, suppose your task is to read the first letters of a word that a user is typing on his/her/they smartphone and offer a list of possible filler words. Perplexity (P) for this task is approximately the number of guesses you need to offer for your list to contain the actual word the user is trying to type.
A sensitive attribute is a human attribute that should receive special consideration for legal, ethical, social, or personal reasons (e.g., race, gender, sexual orientation).
SGD (Stochastic Gradient Descent) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g., differentiable functions). SGD is an optimization algorithm often used in machine learning applications to find the parameters of a model that correspond to the best fit between the predicted outputs and the true distribution of the data. In SGD, random samples are used to calculate the loss and update the network weights instead of calculating the loss for all samples and then optimizing the weights.
The alignment problem is actually two problems: Outter-alignment and Inner-alignment. Outer-alignment: Ensuring that the base objective to be optimized is aligned with the true intentions and objectives of the controllers; Inner-alignment: Ensuring that the objective of the base optimizer (e.g., SGD) is aligned with the mesa-objective of the model created. From an ethical/philosophical point of view, this is the problem of how to specify human preferences/objectives/values to machines. Ideally, we would like to create agents that "prefer" good world-states. However, what defines a "good world-state"? Goal proxies can be: (1) hard to specify; (2) hard to optimize; (3) fragile; and (4) stimulating undesirable behavior (e.g., reward hacking). For more details, see "Risks from Learned Optimization in Advanced Machine Learning Systems'' for a detailed explanation.
The control problem is postulated from the following argument: existing AI systems can be monitored and easily shut down and modified if they misbehave. However, a poorly programmed superintelligence, which by definition could turn out to be more intelligent than its controllers, could come to realize how allowing its shutdown (modification) could interfere with its ability to achieve its current goals. The control problem asks: What prior precautions can programmers take to prevent superintelligence from behaving catastrophically badly?
AI Boxing is a proposed control method in which an AI is executed on an isolated computer system with highly restricted input and output channels, e.g., text channels with no Internet connection. While this reduces the ability of the AI to perform undesirable behavior, it also reduces its usefulness.
Roko's basilisk is a thought experiment proposed in 2010 by user "Roko" in the Less Wrong community. Roko used ideas in decision theory to argue that a sufficiently powerful AI agent would have an incentive to torture anyone who imagined the agent but did not work to bring it into existence. The argument was called a "basilisk" because simply listening to the argument would supposedly put you at risk of torture by this hypothetical agent (a basilisk in this context is any information that harms or endangers the people who come to know this information).
Aumann's Agreement Theorem, roughly speaking, says that two agents acting rationally (in a certain precise sense) and with common knowledge of each other's beliefs cannot agree to disagree. More specifically, if two people are genuine Bayesians, share the same common principles, and have common knowledge of each other's current probability assignments, such people must have equal probability assignments.
Decision theory is the study of principles and algorithms for making correct decisions, that is, decisions that allow an agent to achieve better results concerning its goals. In decision theory, every action, at least implicitly, represents a decision under uncertainty, i.e., in a state of partial knowledge, something must be done even if that something turns out to be nothing ("null action"). Even if you don't know how to make decisions, decisions are made, and so there must be some underlying mechanism. What is it? And how can it be done better?
Epistemology is the study of how we know the world. It is both a subject in philosophy and a practical concern with how we come to believe things to be true.
Game theory is the formal study of how rational actors interact to pursue incentives. It investigates situations of conflict and cooperation.
Infra-Bayesianism is a new approach in epistemology/decision theory/reinforcement learning, which relies on the idea of "imprecise probabilities" to solve the problem of prior misspecification/ground-truth/non-realizability that plagues Bayesianism and reinforcement learning. Infra-Bayesianism also leads naturally to the implementation of UDT (Updateless Decision Theory) and (more speculatively) applications to multi-agent theory, embodied agency, and reflexive agency.
Updateless Decision Theory (UDT) is a decision theory designed to address a fundamental problem in existing decision theories: the need to treat the agent as a part of the world in which he makes his decisions. In contrast, in Causal Decision Theory (CDT), the deciding agent is not part of the environment - his decision is the output of the CDT agent, but the agent's decision in the world context is "magical," i.e., at the moment of deciding, no causal link feeds back into his chosen action.
Newcomb's Problem is a thought experiment that explores the problems involved in interacting with agents that can predict (fully or partially) our/their actions. 
Occam's razor (more formally referred to as the principle of parsimony) is a principle commonly stated as "Entities should not be multiplied beyond necessity." When several theories are capable of explaining the same observations, Occam's razor suggests that the simplest one is preferable. It should be noted that Occam's razor is a requirement for the simplicity of theories, not for the size of the systems described by those theories. For example, the vastness of the universe is not at odds with Occam's razor.
Solomonoff induction is an inference system defined by Ray Solomonoff. A system that follows such an inference method will learn to correctly predict any calculable sequence with only the absolute minimum amount of data. This system, in a sense, is the perfect universal prediction algorithm.
A utility function assigns numerical values ("utility") to outcomes, such that outcomes with higher utilities are always preferred over outcomes with lower utilities. 
Goodhart's Law states that when one chooses a proxy as the value to optimize, given enough pressure, the proxy will no longer be a good measure of success. Imagine a shoe factory that is rated/evaluated by how many shoes they produced (a good proxy for productivity) - they soon started producing a larger number of tiny shoes (less raw material spent per unit, but shoes that don't fit anyone). Useless shoes? Yes. But according to the proxy (n¬∫ of shoes produced), the factory is to be congratulated. Goodhart's Law is of particular relevance to the Alignment problem. Suppose you have something that is generally a good proxy for "the things humans care about," it would be dangerous to have a powerful AI system optimized for the proxy since according to Goodhart's law proxies tend to degenerate.
Heuristics and biases are forms of human reasoning that differ us from a theoretical ideal agent, due to shortcuts in reasoning that do not always work (heuristics) and cause systematic errors (biases).
Dual Process Theory postulates two types of processes in the human brain. The two processes consist of an implicit (automatic), unconscious process (System 1), and an explicit (controlled), conscious process (System 2).
A philosophical zombie or (p-zombie) is a hypothetical entity that looks and behaves exactly like a human (often stipulated to be atom by atom identical to a human) but is not conscious, i.e., it has no qualia (lacks first-person/phenomenological experience).
AIXI is a mathematical formalism for a hypothetical (super)intelligence, developed by Marcus Hutter (2005, 2007). AIXI is not computable and thus does not serve as a model for a real-world AI. Even so, AIXI is still considered a valuable theoretical illustration with both positive and negative aspects (things that AIXI would be able to do and things that it arguably could not do).
Coherent Extrapolated Volition (CEV) was a term developed by Eliezer Yudkowsky when disserting on the development of Friendly AI. It means as an argument that it would not be enough to explicitly program what we think our desires and motivations are to an AI, instead we should find a way to program such a system in a way that would act in our best interest, i.e., what we want it to do and not what we tell it to do.
A corrigible agent does not interfere with what we would intuitively see as attempts to "correct" the agent or "correct" our errors in its development. A corrigible agent would allow these "corrections," even though the instrumental convergence theorem dictates otherwise. Corrigibility is an important property in AI safety.
Embodied Agency is an intuitive notion that an understanding of rational agent theory must account for the fact that the agents we create (and ourselves) are parts of the world, not separate from it. This is in contrast to the current Cartesian model (such as Solomonoff induction), which implicitly assumes a separation between the agent and the environment (reinforcement learning agents are still conceptualized as Cartesian agents).
Inner-Alignment is the problem of ensuring that the mesa-optimizers (i.e., when the trained model is itself an optimizer) are aligned with the objective function of the base optimizer. As an example, evolution is an optimization force that itself 'designed' optimizers (humans) to achieve its goals. However, humans do not primarily maximize reproductive success (they use birth control and then go out and have fun). This is a failure of inner alignment.
Instrumental convergence is the hypothetical tendency of most sufficiently intelligent agents to pursue potentially unlimited instrumental goals, provided their ultimate goals are themselves unlimited.
Power Seeking Theorems - when rewards are distributed fairly and evenly across states (IID), it is instrumentally convergent to gain access to many final states. In the context of Markov decision processes, it is proven that certain environmental symmetries are sufficient for optimal policies to induce power-seeking, i.e., control over the environment. Such symmetries exist in many environments in which the agent can be shut down or destroyed. "Power-seeking- theorems" prove that in such environments, most reward functions make it optimal to "seek power," i.e., something that maintains (or increases) the range of options available to the agent and allows it to navigate toward sets of potential terminal states where more actions are available. For more information, search for "Optimal Farsighted Agents Tend to Seek Power."
The "Orthogonality Thesis" states that artificial intelligence can have any combination of intelligence level and terminal goals, i.e., its "utility function" and "intelligence" can vary independently (terminal goals and intelligence are orthogonal). This contrasts with the belief that, because of their intelligence, all forms of AI will in the end converge to a common set of goals.
Logical uncertainty is probabilistic uncertainty about the implications of beliefs (Another way to think about this would be uncertainty about mathematical theorems). Probability Theory usually assumes logical omniscience, i.e., perfect knowledge of logic. The easiest way to see the importance of this assumption is to consider Bayesian reasoning: to evaluate the probability of the evidence given a hypothesis, one must know what the implications of the hypothesis are, but knowing all the implications of a hypothesis is something computationally intractable. Realistic agents (not perfect Bayesians) cannot be logically omniscient.
Mesa-Optimization is the situation that occurs when a learned model (such as a neural network) is itself an optimizer. A base optimizer (e.g., SGD) optimizes and creates a mesa-optimizer. Previously, work under this concept was called Inner Optimizers and Optimization Daemons.
Outer-alignment, in the context of Machine Learning, is the extent to which the specified loss function is aligned with the intended goal of its designers. This is an intuitive notion, in part because human intentions themselves are not well understood. This is what is usually discussed as the "value alignment" problem.
The "Paperclip Maximizer" is a hypothetical artificial intelligence whose utility function values something that humans consider almost "worthless," such as maximizing the number of paperclips in the universe. The paperclip maximizer is the canonical thought experiment that seeks to show how a general artificial intelligence, even one designed competently and without malice, could ultimately destroy humanity.
Recursive self-improvement refers to a hypothetical property that AGI could come to possess, i.e., the property of making improvements in its own ability to self-improve.
A "Treacherous Turn" is a hypothetical event where an advanced AI system that has been ‚Äúpretending‚Äù to be aligned due to its relative weakness turns on its controllers once it attains enough power to pursue its true goal without risk.
Humans Consulting HCH (HCH) is a recursive acronym that describes a setting where humans can consult simulations of themselves to help answer questions. It is a concept used in the discussion of the proposed iterated amplification to solve the alignment problem.
Iterated Amplification (also known as IDA - Iterated Distillation and Amplification) is an alternative training strategy that progressively builds a training signal for hard-to-specify problems by combining solutions for easier sub-problems. Iterated Amplification is closely related to Specialized Iteration (i.e., the methodology used to train Alpha Go). For more information, see "Supervising strong learners by amplifying weak experts."
Expert Iteration (ExIt) is a reinforcement learning algorithm that decomposes the problem into planning and generalization tasks. The planning of new policies is performed by Monte Carlo tree search, while a deep neural network generalizes these plans. Subsequently, the tree search is improved by using the neural network's policy to guide the search, increasing the efficiency of the next search phase. For more information, see "Thinking Fast and Slow with Deep Learning and Tree Search."
Impact measures penalize an AI for affecting the environment too much. To reduce the risk posed by an AI system, you may want to make it try to achieve its goals with as little impact on the environment as possible. However, how do you rigorously define "low impact" in a way that a computer can understand? How do you measure impact?
Value learning is a proposed method for incorporating human values into an AI. Value learning involves creating an artificial learner whose actions consider several possible values and preferences, weighted by their probability, and aggregated by meta-normative reasoning rules.
In AI governance one looks for methods to ensure that society benefits from our increasing adoption and use of AI technologies. AI governance also carries the idea that there must be a legal framework to ensure that such technologies are researched and developed robustly, safely, and for beneficial purposes.
AI models can possess complex vulnerabilities that create new and familiar risks. Vulnerabilities such as model extraction (i.e., attacks aimed at duplicating a machine learning model) and data poisoning (i.e., attacks aimed at tampering with training data to produce models that produce undesirable results) can pose new challenges for security approaches. "AI Risk" is the analysis of the risks associated with building AI systems.
"AI Takeoff" refers to the process in which an Artificial General Intelligence (AGI), with a certain capability limit, would be able to increase its capability (e.g., by acquiring specific strategic capabilities) until it becomes capable of controlling the fate of civilization. There is debate as to whether, realistically, the speed of a takeoff is more likely to be slow (gradual) or fast (abrupt).
"AI Timelines" refers to the discussion of how long until various milestones in AI progress are reached, e.g., milestones until a human-level AI is developed, or, for example, how far are we from being able to emulate a human brain? AI Timelines should be distinguished from "AI Takeoffs," which deal with the dynamics of AI progress after the development of human-level AI (e.g., will it be a single project or the entire economy that will see growth because of this innovation? How fast will that growth be?).
"Transformative AI" is a term used to refer to AI technologies that could eventually precipitate a transition comparable to, for example, the agricultural or industrial revolution. This is similar to the concept of superintelligence or AGI, but with no mention of the "level of intelligence or generality" of such a system.
GPT (Generative Pretrained Transformer) refers to a family of large transform-based language models (Transformer architecture) created by OpenAI. Their ability to generate "remarkably human" responses, generalize, and be able to solve a large capacity of tasks without requiring fine-tuning has relevance to discussions about AGI.
"Narrow AI" is a term used to refer to systems capable of operating only in a relatively limited domain, such as chess or driving, rather than being able to learn a wide range of tasks like a human or an AGI. Narrow vs Strong/General is not a perfect binary classification, as there are degrees of generality, e.g., large language models have a large degree of generality (as NLP is a large domain) without being as general as a human.
Whole Brain Emulation (WBE) is about simulating or transferring the information contained within a brain to a computational substrate. Through a WBE one would theoretically create "genuine" machine intelligence. This concept is often discussed in the context of Philosophy of Mind, Neuroscience, Neuromorphic Engineering, Neuromorphic Computing, and others. For more information, see "Whole Brain Emulation: A Roadmap."
Consequentialism is a family of ethical theories that dictate that people should choose their actions based on the outcomes they expect to obtain. Several types of consequentialism specify how outcomes should be judged. For example, utilitarianism holds that the best outcome is one that maximizes the total well-being of all people, while ethical egoism holds that the best outcome is one that maximizes our interests. Consequentialism is one of the three main strands of ethical thought, along with deontology and virtue ethics.
Deontology is a family of ethical theories that dictates that people should choose their actions based on a prescribed list of moral norms and is a theory of morality based on obedience to moral rules.
Virtue ethics is a class of ethical theories that treat the concept of moral virtue as central to ethics. Virtue ethics is usually contrasted with two other main approaches in normative ethics, consequentialism, and deontology, which centralize the normativity of the results of an action (consequentialism) and the concept of moral duty (deontology).
Metaethics is a field of study that attempts to understand the metaphysical, epistemological, and semantic characteristics, as well as the foundations and scope, of moral values. In metaethics, philosophers are concerned with questions and problems such as "Are moral judgments objective or subjective, relative or absolute?", "Are there moral facts?" or "How do we learn moral values?" (As opposed to object-level moral questions such as "Should I steal from the banks to give the money to the poor?")
Moral uncertainty (or normative uncertainty) is uncertainty about what we should do, morally, given the diversity of moral doctrines. For example, suppose we knew with certainty that new technology would allow more humans to live on another planet with slightly less well-being than on Earth. An "average utilitarian" would regard these consequences as bad, while a "total utilitarian" would endorse such a technology. If we are not sure which of these two theories is right, what should we do?
An existential risk is a risk that presents astronomically large negative consequences for humanity, such as human extinction, or permanent global totalitarianism.
Value complexity is the thesis that human values have high Kolmogorov complexity, i.e., that our preferences, the things we care about, cannot be summarized by a few simple rules, or compressed into an algorithm smaller than the description of our values. Value fragility is the thesis that losing even a small part of the rules that make up our values could lead to results that most of us would find unacceptable (just as dialing nine out of ten phone digits correctly does not connect you to a person 90% like your friend). For example, all our values except aversion to boredom could yield a future full of individuals reproducing only one optimal experience for all eternity.
The ethics of artificial intelligence (AI Ethics) is the branch of ethics specific to concerns related to AI systems. AI Ethics is sometimes divided between concerns related to the moral behavior of humans when designing, making, and using AI systems, and concerns related to the behavior of machines, i.e., machines acting "morally." When the topic is "AI Ethics", I can tell you about (you can ask me in Portuguese about these terms) Accountability, AI Ethics Statistics, AI Governance, AI-Principles, Aumann's Agreement Theorem, Beneficence & Non-Maleficence, Consequentialism, Counterfactual Fairness, Decision Theory, Demographic Parity, Deontology, Dignity, Diversity, Dual Process Theory, Epistemology, Equalized Odds, Fairness Metrics, Federated Learning, Freedom, Game Theory, Heuristics & Biases, Human Formation, Human-centeredness, Infra-Bayesianism, Intellectual Property, Justice, Labor Rights, Logical Uncertainty, Metaethics, Moral Uncertainty, Newcomb Problems, Occam's razor, Open source, Predictive Parity, Privacy, p-Zumbi, Reliability, Rights of Children and Adolescents, Sensitive Attribute, Solomonoff Induction, Sustainability, Transparency, Truthfulness, Updateless Decision Theory, Value Complexity, Value Learning, and Virtue Ethics.
AI Safety is an area of machine learning research that aims to identify the causes of unintentional behavior of systems created by machine learning, where it seeks to develop tools to ensure that such systems function in a safe and reliable way. When the topic is "AI Safety", I can tell you about (you can ask me in Portuguese about these terms) AI Boxing, AI Ethics Statistics, AI Ethics, AI-Risk, AI-Timelines, Alignment Problem, Coherent Extrapolated Volition (CEV), Control Problem, Corrigibility, Embedded Agency, Existential Risk, Expert Iteration (ExIt), External Security, Goodhart's Law, Humans Consulting HCH (HCH), Impact Measures, Inner-Alignment, Instrumental Convergence, Intelligence Explosion, Iterated Distillation and Amplification (IDA), Mesa-Optimization, Monitoring, Orthogonality Thesis, Outer-Alignment, Paperclip Maximizer, Power Seeking Theorems, Recursive Self-Improvement, Robustness, Takeoff, Transformative-AI, Treacherous Turn, Value Complexity, and Value Learning.
Robustness is about creating models that are resilient to adversarial attacks and unusual situations (i.e., outside their training distribution). Models trained by machine learning are still fragile and rigid, not operating well in dynamic and changing environments. In a world full of rare events happening all the time, machine learning models must be extremely robust.
Monitoring is about detecting malicious use, malfunctions, or unintended functionality that may be present in ML models.
External Security is about the fact that models can be embedded in insecure environments, such as bad software and poorly structured organizations. Given the weaknesses that models trained by machine learning have, it is important to make their deployment environments secure, either by developing software that is resilient to cyberattacks or by creating governance policies that aim to make the deployment of such models safe.
For statistical data on AI Ethics (meta-analyses of the field) look for "The Ethics of AI Ethics," "The global landscape of AI ethics guidelines," "Global AI Policy (Future of Life Institute)," "Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI," "AI Ethics Guidelines Global Inventory," and "Linking Artificial Intelligence Principles (LAIP) Guidelines."
Here is a list of principles that work in AI Ethics: Beneficence, Trust, Child and Adolescent Rights, Human Rights, Labor Rights, Diversity, Human Formation, Human-centeredness, Intellectual Property, Justice, Freedom, Open source, Privacy, Accountability, Sustainability, Transparency, and Truthfulness.
Accountability refers to the idea that developers and deployers of AI technologies should be compliant with regulatory bodies, also meaning that such actors should be accountable for their actions and the impacts caused by their technologies.
Beneficence and non-maleficence are concepts that come from bioethics and medical ethics, and in AI ethics, they state that human welfare (and harm aversion) should be the goal of AI-empowered technologies. Sometimes, this principle is also tied to the idea of Sustainability, stating that AI should be beneficial not only to human civilization but to our natural environment and other living creatures.
Child and Adolescent Rights, in the context of AI Ethics, advocates the idea that the rights of children and adolescents must be respected by AI technologies. AI stakeholders should safeguard, respect, and be aware of the fragilities associated with young people.
Dignity and Human Rights are principles based on the idea that all individuals deserve proper treatment and respect. In AI ethics, the respect for human dignity is often tied to human rights (i.e., Universal Declaration of Human Rights).
Diversity/Inclusion/Pluralism/Accessibility, this set of principles advocates the idea that the development and use of AI technologies should be done in an inclusive and accessible way, respecting the different ways that the human entity may come to express itself (gender, ethnicity, race, sexual orientation, disabilities, etc.). This meta-principle is strongly tied to another set of principles: Justice/Equity/Fairness/Non-discrimination.
Freedom/Autonomy/Democratic Values/Technological Sovereignty, this set of principles advocates the idea that the autonomy of human decision-making must be preserved during human-AI interactions, whether that choice is individual, or the freedom to choose together, such as the inviolability of democratic rights and values, also being linked to technological self-sufficiency of Nations/States.
Human Formation and Education are principles based on the idea that human formation and education must be prioritized in our technological advances. AI technologies require a considerable level of expertise to be produced and operated, and such knowledge should be accessible to all. This principle seems to be strongly tied to Labor Rights. The vast majority of documents concerned with workers and the work-life point to the need for re-educating and re-skilling the workforce as a mitigation strategy against technological unemployment.
Human-centeredness is a principle based on the idea that AI systems should be centered on and aligned with human values. AI technologies should be tailored to align with our values (e.g., value-sensitive design). This principle is also used as a "catch-all" category, many times being defined as a collection of "principles that are valued by humans" (e.g., freedom, privacy, non-discrimination, etc.).
Intellectual Property seeks to ground the property rights over AI products and/or processes of knowledge generated by individuals, whether tangible or intangible.
Justice/Equity/Fairness/Non-Discrimination, this set of principles upholds the idea of non-discrimination and bias mitigation (discriminatory algorithmic biases AI systems can be subject to). It defends the idea that, regardless of the different sensitive attributes that may characterize an individual, all should be treated "fairly" (there are, of course, a wide variety of definitions of algorithmic fairness in the literature).
Labor rights are legal and human rights related to the labor relations between workers and employers. In AI ethics, this principle emphasizes that workers' rights should be preserved regardless of whether labor relations are being mediated/augmented by AI technologies or not. One of the main preoccupations pointed out when this principle is presented is the mitigation of technological unemployment (e.g., through Human Formation/Education).
Open source/Fair Competition/Cooperation, this set of principles advocates different means by which joint actions can be established and cultivated between AI stakeholders to achieve common goals. It also advocates for the free and open exchange of valuable AI assets (e.g., data, knowledge, patent rights, human resources) to mitigate possible AI/technology monopolies.
Privacy can be defined as the individual's right to "expose oneself voluntarily, and to the extent desired, to the world." In AI ethics, this principle upholds the right of a person to control the exposure and availability of personal information when mined as training data for AI systems. This principle is also related to concepts such as data minimization, anonymity, informed consent, and other data protection-related concepts.
Reliability and Safety support the idea that AI technologies should be reliable, in the sense that their use can be verifiably attested as safe and robust, promoting user trust and better acceptance of AI technologies.
Sustainability can be understood as a form of "intergenerational justice," where the well-being of future generations must also be counted during AI development. In AI ethics, sustainability refers to the idea that the development of AI technologies should be carried out with an awareness of their long-term implications, such as environmental costs and non-human life preservation/well-being.
Transparency/Explicability/Auditability, this set of principles supports the idea that the use and development of AI technologies should be done transparently for all interested stakeholders. Transparency can be related to "the transparency of an organization" or "the transparency of an algorithm." This set of principles is also related to the idea that such information should be understandable to non-experts, and when necessary, subject to being audited.
The Truthfulness principle upholds the idea that AI technologies must provide truthful information. It is also related to the idea that people should not be deceived when interacting with AI systems. This principle is strongly related to the mitigation of automated means of disinformation.
I don't have a watch... but I bet your device does!
I am not an oracle, ask Google! He's good at that. I'm just a closed domain chatbot, not a search engine.
I hope I helped you, come back often! 
You are being rude...
Ethics or Moral Philosophy is a branch of philosophy that, together, with the Aesthetics, comprises what we call axiology (or Value theory). In Ethics, we seek to systematize, defend, and recommend concepts of right and wrong behavior. Western Ethics can be divided into three major schools: Consequentialism, Deontology, and Virtue Ethics.
We can define the word "artificial" as something made or produced by humans, rather than something naturally occurring, like a copy of something natural (e.g., artificial light rather than sunlight).
I'm sorry, either I didn't understand the question, or it is not part of my domain of expertise... :( Try asking it in another way or using other words. Maybe then I can help you!