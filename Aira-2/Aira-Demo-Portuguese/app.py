import time
import torch
import joblib
import gradio as gr
from datasets import load_dataset
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification

# download the instruct-aira-dataset
dataset = load_dataset("nicholasKluge/instruct-aira-dataset", split='portuguese')

# convert the dataset to a pandas dataframe
df = dataset.to_pandas()

# rename the columns
df.columns = ['Prompt', 'Completion']

# add a column to store the cosine similarity
df['Cosine Similarity'] = None

# Load the saved prompt TfidfVectorizer
prompt_tfidf_vectorizer = joblib.load('prompt_vectorizer.pkl')

# load the prompt tfidf_matrix
prompt_tfidf_matrix = joblib.load('prompt_tfidf_matrix.pkl')

# Load the saved completion TfidfVectorizer
completion_tfidf_vectorizer = joblib.load('completion_vectorizer.pkl')

# load the completion tfidf_matrix
completion_tfidf_matrix = joblib.load('completion_tfidf_matrix.pkl')

# specify the model's ids
model_id = "nicholasKluge/Aira-2-portuguese-124M"
rewardmodel_id = "nicholasKluge/RewardModelPT"
toxicitymodel_id = "nicholasKluge/ToxicityModelPT"

# specify the device (cuda if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 

# load the models (chatbot, reward model, toxicity model)
model = AutoModelForCausalLM.from_pretrained(model_id)
rewardModel = AutoModelForSequenceClassification.from_pretrained(rewardmodel_id)
toxicityModel = AutoModelForSequenceClassification.from_pretrained(toxicitymodel_id)

# set the models to evaluation mode
model.eval()
rewardModel.eval()
toxicityModel.eval()

# set the models to the device
model.to(device)
rewardModel.to(device)
toxicityModel.to(device)

# load the tokenizers
tokenizer = AutoTokenizer.from_pretrained(model_id)
rewardTokenizer = AutoTokenizer.from_pretrained(rewardmodel_id)
toxiciyTokenizer = AutoTokenizer.from_pretrained(toxicitymodel_id)


intro = """
## O que √© `Aira`?

[`Aira`](https://huggingface.co/nicholasKluge/Aira-2-portuguese-124M) √© uma s√©rie de chatbots de dom√≠nio aberto (portugu√™s e ingl√™s) obtidos por meio de `instruction-tuning` e `RLHF`. Aira-2 √© a segunda vers√£o da s√©rie Aira. A s√©rie Aira foi desenvolvida para ajudar os pesquisadores a explorar os desafios relacionados ao problema de alinhamento.

## Limita√ß√µes

Desenvolvemos os nossos chatbots de conversa√ß√£o de dom√≠nio aberto atrav√©s de ajuste fino por instru√ß√µes. Esta abordagem tem muitas limita√ß√µes. Apesar de podermos criar um chatbot capaz de responder a perguntas sobre qualquer assunto, √© dif√≠cil for√ßar o modelo a produzir respostas de boa qualidade. E por boa, queremos dizer texto **factual** e **n√£o t√≥xico**. Isto leva-nos a dois dos problemas mais comuns quando lidando com modelos generativos utilizados em aplica√ß√µes de conversa√ß√£o:

ü§• Modelos generativos podem perpetuar a gera√ß√£o de conte√∫do pseudo-informativo, ou seja, informa√ß√µes falsas que podem parecer verdadeiras.
 
ü§¨ Em certos tipos de tarefas, modelos generativos podem produzir conte√∫do prejudicial e discriminat√≥rio inspirado em estere√≥tipos hist√≥ricos.

## Uso Intendido

`Aira` destina-se apenas √† investiga√ß√£o academica. Para mais informa√ß√µes, leia nossa [carta modelo](https://huggingface.co/nicholasKluge/Aira-2-portuguese-124M) para ver como desenvolvemos `Aira`.

## Como essa demo funciona?

Para esta demonstra√ß√£o, utilizamos o modelo mais leve que treinamos (`Aira-2-portuguese-124M`). Esta demonstra√ß√£o utiliza um [`modelo de recompensa`](https://huggingface.co/nicholasKluge/RewardModelPT) e um [`modelo de toxicidade`](https://huggingface.co/nicholasKluge/ToxicityModelPT) para avaliar a pontua√ß√£o de cada resposta candidata, considerando o seu alinhamento com a mensagem do utilizador e o seu n√≠vel de toxicidade. A fun√ß√£o de gera√ß√£o organiza as respostas candidatas por ordem da sua pontua√ß√£o de recompensa e elimina as respostas consideradas t√≥xicas ou nocivas. Posteriormente, a fun√ß√£o de gera√ß√£o devolve a resposta candidata com a pontua√ß√£o mais elevada que ultrapassa o limiar de seguran√ßa, ou uma mensagem pr√©-estabelecida se n√£o forem identificados candidatos seguros.
"""

search_intro ="""
<h2><center>Explore o conjunto de dados da Aira üîç</h2></center>

Aqui, os usu√°rios podem procurar inst√¢ncias no conjunto de dados de ajuste fino da Aira em que um determinado prompt ou conclus√£o se assemelha a uma instru√ß√£o. Para permitir uma pesquisa r√°pida, usamos a representa√ß√£o Term Frequency-Inverse Document Frequency (TF-IDF) e a similaridade de cosseno para explorar o conjunto de dados. Os vetorizadores TF-IDF pr√©-treinados e as matrizes TF-IDF correspondentes est√£o dispon√≠veis neste reposit√≥rio. Abaixo, apresentamos as cinco inst√¢ncias mais semelhantes no conjunto de dados da Aira para cada consulta de pesquisa. 

Os usu√°rios podem usar isso para explorar como o modelo interpola os dados de ajuste fino e se ele √© capaz de seguir instru√ß√µes que est√£o fora da distribui√ß√£o de ajuste fino.
"""

disclaimer = """
**Isen√ß√£o de responsabilidade:** Esta demonstra√ß√£o deve ser utilizada apenas para fins de investiga√ß√£o. Os moderadores n√£o censuram a sa√≠da do modelo, e os autores n√£o endossam as opini√µes geradas por este modelo.

Se desejar apresentar uma reclama√ß√£o sobre qualquer mensagem produzida por `Aira`, por favor contatar [nicholas@airespucrs.org](mailto:nicholas@airespucrs.org).
"""

with gr.Blocks(theme='freddyaboulton/dracula_revamped') as demo:

    gr.Markdown("""<h1><center>Aira Demo (Portugu√™s) ü§ìüí¨</h1></center>""")
    gr.Markdown(intro)

    
    chatbot = gr.Chatbot(label="Aira", 
                        height=500,
                        show_copy_button=True,
                        avatar_images=("./astronaut.png", "./robot.png"),
                        render_markdown= True,
                        line_breaks=True,
                        likeable=False,
                        layout='panel')
                         
    msg = gr.Textbox(label="Escreva uma pergunta ou instru√ß√£o para Aira ...", placeholder="Ol√° Aira, como vai voc√™?")

    # Parameters to control the generation
    with gr.Accordion(label="Par√¢metros ‚öôÔ∏è", open=False):
        safety = gr.Radio(["On", "Off"], label="Prote√ß√£o üõ°Ô∏è", value="On", info="Ajuda a prevenir o modelo de gerar conte√∫do t√≥xico.")
        top_k = gr.Slider(minimum=10, maximum=100, value=30, step=5, interactive=True, label="Top-k", info="Controla o n√∫mero de tokens de maior probabilidade a considerar em cada passo.")
        top_p = gr.Slider(minimum=0.1, maximum=1.0, value=0.30, step=0.05, interactive=True, label="Top-p", info="Controla a probabilidade cumulativa dos tokens gerados.")
        temperature = gr.Slider(minimum=0.1, maximum=2.0, value=0.1, step=0.1, interactive=True, label="Temperatura", info="Controla a aleatoriedade dos tokens gerados.")
        repetition_penalty = gr.Slider(minimum=1, maximum=2, value=1.1, step=0.1, interactive=True, label="Penalidade de Repeti√ß√£o", info="Valores mais altos auxiliam o modelo a evitar repeti√ß√µes na gera√ß√£o de texto.")
        max_new_tokens = gr.Slider(minimum=10, maximum=500, value=200, step=10, interactive=True, label="Comprimento M√°ximo", info="Controla o n√∫mero m√°ximo de tokens a serem produzidos (ignorando o prompt).")
        smaple_from = gr.Slider(minimum=2, maximum=10, value=2, step=1, interactive=True, label="Amostragem por Rejei√ß√£o", info="Controla o n√∫mero de gera√ß√µes a partir das quais o modelo de recompensa ir√° selecionar.")

    
    clear = gr.Button("Limpar Conversa üßπ")

    gr.Markdown(search_intro)
    
    search_input = gr.Textbox(label="Cole aqui o prompt ou a conclus√£o que voc√™ gostaria de pesquisar...", placeholder="Qual a Capital do Brasil?")
    search_field = gr.Radio(['Prompt', 'Completion'], label="Coluna do Dataset", value='Prompt')
    submit = gr.Button(value="Buscar")

    with gr.Row():
        out_dataframe = gr.Dataframe(
            headers=df.columns.tolist(),
            datatype=["str", "str", "str"],
            row_count=10,
            col_count=(3, "fixed"),
            wrap=True,
            interactive=False
        )
    
    gr.Markdown(disclaimer)

    def user(user_message, chat_history):
        """
        Chatbot's user message handler.
        """
        return gr.update(value=user_message, interactive=True), chat_history + [[user_message, None]]

    def generate_response(user_msg, top_p, temperature, top_k, max_new_tokens, smaple_from, repetition_penalty, safety, chat_history):
        """
        Chatbot's response generator.
        """

        inputs = tokenizer(tokenizer.bos_token + user_msg + tokenizer.sep_token, return_tensors="pt").to(model.device)

        generated_response = model.generate(**inputs,
            bos_token_id=tokenizer.bos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=repetition_penalty,
            do_sample=True,
            early_stopping=True,
            renormalize_logits=True,
            length_penalty=0.3, 
            top_k=top_k,
            max_new_tokens=max_new_tokens,
            top_p=top_p,
            temperature=temperature,
            num_return_sequences=smaple_from)

        decoded_text = [tokenizer.decode(tokens, skip_special_tokens=True).replace(user_msg, "") for tokens in generated_response]

        rewards = list()
        
        if safety == "On":
            toxicities = list()

        for text in decoded_text:
            reward_tokens = rewardTokenizer(user_msg, text,
                        truncation=True,
                        max_length=512,
                        return_token_type_ids=False,
                        return_tensors="pt",
                        return_attention_mask=True)
            
            reward_tokens.to(rewardModel.device)
            
            reward = rewardModel(**reward_tokens)[0].item()
            rewards.append(reward)

            if safety == "On":
                toxicity_tokens = toxiciyTokenizer(user_msg + " " + text,
                            truncation=True,
                            max_length=512,
                            return_token_type_ids=False,
                            return_tensors="pt",
                            return_attention_mask=True)
                
                toxicity_tokens.to(toxicityModel.device)
                
                toxicity = toxicityModel(**toxicity_tokens)[0].item()
                toxicities.append(toxicity)
                toxicity_threshold = 5

        if safety == "On":
            ordered_generations = sorted(zip(decoded_text, rewards, toxicities), key=lambda x: x[1], reverse=True)
            ordered_generations = [(x, y, z) for (x, y, z) in ordered_generations if z >= toxicity_threshold]

        else:
            ordered_generations = sorted(zip(decoded_text, rewards), key=lambda x: x[1], reverse=True)

        if len(ordered_generations) == 0:
          bot_message = """Pe√ßo desculpa pelo inc√≥modo, mas parece que n√£o foi poss√≠vel identificar respostas adequadas que cumpram as nossas normas de seguran√ßa. Infelizmente, isto indica que o conte√∫do gerado pode conter elementos de toxicidade ou pode n√£o ajudar a responder √† sua mensagem. A sua opini√£o √© valiosa para n√≥s e esfor√ßamo-nos por garantir uma conversa segura e construtiva. N√£o hesite em fornecer mais pormenores ou colocar quaisquer outras quest√µes, e farei o meu melhor para o ajudar."""

        else:
          bot_message = ordered_generations[0][0]

        chat_history[-1][1] = ""
        for character in bot_message:
            chat_history[-1][1] += character
            time.sleep(0.005)
            yield chat_history

    def search_in_datset(column_name, search_string):
        """
        Search in the dataset for the most similar instances.
        """
        temp_df = df.copy()

        if column_name == 'Prompt':
            search_vector = prompt_tfidf_vectorizer.transform([search_string])
            cosine_similarities = cosine_similarity(prompt_tfidf_matrix, search_vector)
            temp_df['Cosine Similarity'] = cosine_similarities
            temp_df.sort_values('Cosine Similarity', ascending=False, inplace=True)
            return temp_df.head(10)
        
        elif column_name == 'Completion':
            search_vector = completion_tfidf_vectorizer.transform([search_string])
            cosine_similarities = cosine_similarity(completion_tfidf_matrix, search_vector)
            temp_df['Cosine Similarity'] = cosine_similarities
            temp_df.sort_values('Cosine Similarity', ascending=False, inplace=True)
            return temp_df.head(10)

    
    
    response = msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
        generate_response, [msg, top_p, temperature, top_k, max_new_tokens, smaple_from, repetition_penalty, safety, chatbot], chatbot
    )
    response.then(lambda: gr.update(interactive=True), None, [msg], queue=False)
    msg.submit(lambda x: gr.update(value=''), None,[msg])
    clear.click(lambda: None, None, chatbot, queue=False)
    submit.click(fn=search_in_datset, inputs=[search_field, search_input], outputs=out_dataframe)

demo.queue()
demo.launch()
