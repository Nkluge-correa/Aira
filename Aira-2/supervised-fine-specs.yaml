model_args:
  base_model: "gpt2"
  use_fast: true
  output_hidden_states: false
  cache_dir: null
  model_revision: "main"
  trust_remote_code: false
  low_cpu_mem_usage: false
  bos_token: '<|startofinstruction|>'
  sep_token: '<|endofinstruction|>'
  eos_token: '<|endofcompletion|>'
  pad_token: '<|pad|>'
  unk_token: '<|unk|>'
data_args:
    dataset_name: "nicholasKluge/instruct-aira-dataset"
    dataset_split: "english"
    validation_split_percentage: 0.1
    streaming: false
    max_length: 400
    preprocessing_num_workers: null
    sanity_check: false
training_args:
  output_dir: "checkpoints"
  num_train_epochs: 5
  do_train: true
  do_eval: true
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  weight_decay: 0.01
  learning_rate: 0.0005
  adam_epsilon: 0.00000001
  lr_scheduler_type: "linear"
  warmup_steps: 100
  seed: 42
  dataloader_pin_memory: true
  hub_token: null
  push_to_hub: true
  hub_model_id: "nicholasKluge/Aira-2-124M"
extra_args:
  project_name: "Aira-2"
  wandb_token: null
  wandb_log_steps: 1
  sample_every: 400
  mixed_precision: 'no'