Olá! Meu nome é Ai.ra, e eu sou uma inteligência artificial (IA). Mais especificamente, eu sou um processador de linguagem natural  treinado em conversação (um chatbot!). Eu fui especificamente treinada para responder perguntas sobre Ética e Segurança da IA! Você gostaria de um sumário dos termos que tenho conhecimento? Qualquer coisa é só pedir “ajuda”.
Você pode me perguntar coisas sobre “Inteligência Artificial”, “Machine Learning”, “AI Safety”, ou “Ética da IA”. Para saber o sumário específico de cada uma dessas categorias, basta dizer o nome do tópico em questão (e.g., Machine Learning)!
Eu não tenho esse tipo de propriedade hahaha eu sou um software! Não há sentido em me categorizar com tipologias destinadas a pessoas ou animais (idade, sexo, orientação sexual, raça, gostos…).
A AIRES (AI Robotics Ethics Society) é uma sociedade focada em educar os líderes e desenvolvedores da Inteligência Artificial (IA) do amanhã para garantir que IA seja criada de forma ética e responsável. A AIRES na PUCRS é o primeiro capítulo internacional da AIRES! Na AIRES PUCRS, nossa meta é tornar nosso capítulo um ponto de encontro para todas as pessoas (alunos da PUCRS ou não, independente de seu nível de formação ou experiência acadêmica) interessadas em juntar esforços para avançarmos o debate ético a respeito do desenvolvimento da IA. Se quiser conhecer mais sobre a AIRES na PUCRS, acesse https://www.airespucrs.org/.
Aron Hui é o presidente/fundador da AIRES.
O que de fato é “inteligência” segue sendo uma pergunta em aberto. Existe uma miríade de definições possíveis. Talvez você se interesse por ler “A Collection of Definitions of Intelligence” de Shane Legg e Marcus Hutter. Contudo, para não deixar você na mão, irei definir “inteligência” da seguinte forma: “Inteligência é a capacidade de um agente de atingir objetivos em uma ampla gama de ambientes.” 
Não existe um consenso na literatura para o que é “IA” (um corolário de não se possuir uma definição robusta para o termo “inteligência”). Contudo, podemos dizer que IA é a inteligência demonstrada pelas máquinas, em oposição à inteligência natural demonstrada por animais e humanos. Quando o tema é “Inteligência Artificial em geral”, eu posso lhe falar sobre (você pode me perguntar em português sobre esses termos) AIXI, Algorithmic Information Theory (AIT), Artificial General Intelligence (AGI), Artificial Intelligence (AI), Complexidade Algorítmica, Computational Complexity Theory, Computer Vision (CV),  Genetic Algorithms, GOFAI, Machine Learning (ML), Multi-Agent Systems (MaS), Narrow-AI, Natural Language Processing (NLP), NP, P vs NP, P, Solomonoff Induction, e Whole Brain Emulation (WBE).  
Inteligência Geral, ou Inteligência Universal, pode ser definida como a capacidade de atingir metas, em uma ampla gama de domínios, de forma eficiente. Inteligência geral artificial (IAG) seria um mecanismo não-humano capaz de demonstrar proficiência ao lidar com uma ampla gama de problemas. Por exemplo, IAG poderia traduzir textos, compor sinfonias, aprender novas habilidades, se destacar em jogos que ainda não foram inventados, etc.
GOFAI ("good-old-fashioned-ai"), ou inteligência artificial simbólica, é o termo usado para se referir aos métodos de desenvolvimento de sistemas de IA baseados em representações simbólicas (interpretáveis) de alto nível, lógica e busca. O Deep Blue é um ótimo exemplo de sistema expert/GOFAI. Deep Blue venceu Garry Kasparov (um grão-mestre de xadrez russo) em uma partida de seis jogos em 1996. 
Um sistema multi-agente (MAS "Multi-Agent Systems") é um sistema computadorizado composto de múltiplos agentes inteligentes interagindo. MAS são capazes de resolver problemas difíceis (ou impossíveis) de serem resolvidos por um agente individual ou um sistema monolítico. Este tipo de sistema geralmente combina técnicas de  busca algorítmica, abordagens baseadas em regras, otimização clássica, e até mesmo aprendizagem por reforço ou outras técnicas de aprendizagem de máquina.
Aprendizagem de máquina (ML - Machine Learning) é um campo de pesquisa dedicado à compreensão e construção de métodos que “aprendem”, i.e., métodos que utilizam de informações/dados para melhorar o desempenho em algumas tarefas. Geralmente ML é utilizado em problemas onde uma descrição precisa (rule-based) da solução seria desafiadora demais (ou impossível, e.g., computer vision). Quando o tema é “Aprendizagem de Máquina”, eu posso lhe falar sobre (você pode me perguntar em português sobre esses termos) Activation Function, AIXI, Artificial Neural Networks (ANN), Attention, Backpropagation, Catastrophic Forgetting, Convolutional Neural Networks (CNN), Cross-Entropy, Data Augmentation, Deep Learning (DL), Environment, Equalized Odds, Exploding Gradient, Feature Engineering, Federated Learning, Feedforward Neural Network (FNN), Generative Adversarial Networks (GAN), Generative Pretrained Transformer (GPT), Hyperparameter, Interpretability, Long Short-Term Memory (LSTM), Lottery Ticket Hypothesis, Manifold hypothesis, NaN Trap, Neural Turing Machines, Objective Function, Online learning, Parameter, Perplexity, Policy, Predictive Parity, Recurrent Neural Networks (RNN), Reinforcement learning (RL), Self-Attention, Semi-Supervised Learning (SSL), SGD, Supervised Learning SL), Time Series Analysis, Transformer, Unsupervised Learning (UL) e Vanishing Gradient.
A Genetic Algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to a class known as Evolutionary Algorithms (EA). Genetic algorithms are commonly used to generate solutions to optimization and search problems by using biologically inspired strategies such as mutation, crossover, and selection. Some examples of GA applications include decision tree optimization, solving sudoku puzzles (an NP-complete problem), hyperparameter optimization, etc.
Um problema P é um problema que pode ser resolvido em "tempo polinomial", o que significa que existe um algoritmo para sua solução de tal forma que o número de passos no algoritmo é limitado por uma função polinomial de n, onde n corresponde ao comprimento da entrada para o problema (e.g., n^2,  log(n)). NP (tempo polinomial executado por um algoritmo não determinístico) é uma classe de complexidade utilizada para classificar problemas de decisão que não podem ser resolvidos em tempo polinomial por uma máquina de Turing determinística. NP é o conjunto de problemas de decisão para os quais as instâncias do problema possuem provas verificáveis em tempo polinomial por uma máquina Turing determinística. Contudo, tais problemas não podem ser resolvidos em tempo polinomial. O problema P versus NP é talvez o maior problema em aberto na ciência da computação. Ele pergunta se para cada problema cuja solução pode ser verificada rapidamente (em tempo polinomial), tal problema também pode ser resolvido rapidamente. Este é um dos sete “Millennium Prize Problems” selecionados pelo Clay Mathematics Institute, havendo um prêmio de US $1.000.000 para quem for capaz de apresentar uma solução correta para qualquer um desses problemas. Até o momento, o único problema do Millennium a ter sido resolvido é a conjectura de Poincaré.
Aprendizagem supervisionada (SL - Supervised learning) é a tarefa da aprendizagem de uma máquina que mapeia uma entrada para uma saída com base em exemplos de pares de entrada-saída (um “sinal de supervisão”). Na aprendizagem supervisionada, cada exemplo é um par que consiste de um objeto de entrada (normalmente um vetor) e um valor de saída desejado (também chamado de sinal de supervisão). Em SL, utilizamos a distribuição de dados de treinamento para tentar inferir a “verdadeira distribuição” de dados (algo que vai além do que foi observado em treinamento pelo modelo), através de descendência de gradiente e minimização de risco empírico.
Aprendizagem sem supervisão (UL - Unsupervised Learning) é uma técnica de aprendizagem de máquina na qual o controlador não precisa supervisionar o modelo. Em vez disso, ela permite que o modelo trabalhe por conta própria para descobrir padrões e informações que antes não eram detectados. UL é útil para classificar dados não etiquetados, revelando grupos de classes às vezes desconhecidos pelo controlador.
A teoria da complexidade (Computational Complexity Theory) computacional se concentra em classificar problemas computacionais de acordo com seu uso de recursos, e como tais classes (NL, P, NP, PSPACE, EXPTIME, EXPSPACE) relacionam-se entre si.
Teoria da Informação Algorítmica (AIT - Algorithmic Information Theory) é um ramo da informática teórica que se preocupa com a relação entre computação e informação de objetos gerados computacionalmente (em oposição a objetos gerados estocasticamente), tais como sequências numéricas ou qualquer outra estrutura de dados. Na AIT, a complexidade Kolmogorov (Solomonoff–Kolmogorov–Chaitin complexity) de um objeto, como um pedaço de texto, é o comprimento do programa de computador mais curto (em uma linguagem de programação pré-determinada) que produz o objeto como saída, sendo uma medida dos recursos computacionais necessários para especificar tal objeto. 
A Aprendizagem semi-supervisionada (SSL - Semi-supervised learning) é uma abordagem em aprendizagem de máquina que combina uma pequena quantidade de dados rotulados com uma grande quantidade de dados não rotulados durante o treinamento. A aprendizagem semi-supervisionada se situa entre a aprendizagem não supervisionada (sem dados de treinamento rotulados) e a aprendizagem supervisionada (apenas com dados de treinamento rotulados).
Aprendizagem por reforço (RL - Reinforcement learning) é uma técnica de aprendizagem de máquina preocupada com a forma como agentes inteligentes devem agir em um ambiente a fim de maximizar o retorno esperado de uma função de recompensa. Aprendizagem por reforço é um dos três paradigmas básicos da aprendizagem de máquina, juntamente com aprendizagem supervisionada e a aprendizagem não supervisionada. Aprendizagem por reforço difere da aprendizagem supervisionada por não necessitar de pares de entrada/saída rotulados, e por não necessitar de ações subótimas a serem explicitamente corrigidas. Em vez disso, o foco está em encontrar um equilíbrio entre exploração e aproveitamento (exploration vs exploitation), um problema canonicamente representado pelo “multi-armed bandit problem”.
Em teoria da probabilidade, e aprendizagem de máquina, o problema do “Multi-armed Bandit” (MaB)  é um problema no qual um conjunto fixo e limitado de recursos deve ser alocado entre escolhas diferentes alternativas, de forma a maximizar seu ganho esperado, mesmo quando as propriedades de cada escolha são apenas parcialmente conhecidas no momento da alocação, e podem vir a se tornar melhor compreendidas à medida que o tempo passa ou a alocação de recursos é mudada.
Engenharia de características (feature engineering) ou extração de características é o processo de usar o conhecimento de domínio para extrair características (i.e., propriedades, atributos) dos dados brutos. A motivação por trás desta técnica está em se utilizar destas características extras para melhorar a qualidade dos resultados de um processo de aprendizagem da máquina, em comparação com o fornecimento apenas dos dados brutos para o processo de aprendizagem da máquina. Concomitantemente, aprendizagem de características (ou aprendizagem de representações) é um conjunto de técnicas que permite a um sistema descobrir automaticamente as representações necessárias para a detecção ou classificação de características a partir de dados brutos. Uma das grandes vantagens que possuímos ao utilizar aprendizagem profunda é que características podem ser aprendidas/extraídas de forma não-supervisionada (sem a especificação manual dos controladores).
Aprendizagem on-line (OL - Online learning) é um método de aprendizagem de máquina no qual os dados ficam disponíveis em uma ordem sequencial e são usados para atualizar o melhor preditor para dados futuros em cada etapa, ao contrário de treinarmos o preditor sobre todo o conjunto de dados de treinamento de uma só vez. A aprendizagem on-line é uma técnica comum utilizada em áreas onde é computacionalmente inviável treinar sobre todo o conjunto de dados. Aprendizagem on-line também é usada em situações em que é necessário que o algoritmo se adapte dinamicamente a novos padrões nos dados, ou quando os próprios dados são gerados em função do tempo (e.g., previsão de preço de ações, algoritmos de recomendação). Algoritmos de aprendizagem on-line são propensos ao problema de “esquecimento catastrófico”.  
Esquecimento catastrófico (catastrophic forgetting), ou interferência catastrófica, é um conhecido problema em aprendizagem de máquina, relacionado a tendência de redes neurais artificiais a esquecer de forma completa e abrupta informações aprendidas anteriormente ao aprender novas informações.
Análise de séries temporais é um subcampo de aprendizagem de máquina e estatísticas cujo foco de interesse são dados temporais (i.e., séries indexadas por intervalos de tempo). Muitos tipos de problemas em aprendizagem de máquina requerem análise de séries temporais, previsão, processamento natural de linguagem, reconhecimento de áudio, etc.
Aprendizagem Profunda (Deep Learning) faz parte de uma família mais ampla de métodos de aprendizagem de máquina. Podemos dizer que aprendizagem profunda é uma técnica de aprendizagem de máquina para se aprender representações a partir de dados. "Profunda" vem da ideia de que somos capazes de aprender diversas "camadas" de representações dos dados utilizados para aprendizagem. Encontrar representações úteis para variedades ("manifolds") de dados complexos e altamente "dimensionais" é o que a aprendizagem profunda faz.
A Hipótese do bilhete de loteria: “Uma rede neural densa e inicializada aleatoriamente contém uma sub-rede que - quando treinada isoladamente - pode corresponder à acurácia em teste da rede original após o treinamento para, no máximo, o mesmo número de iterações de treinamento”. Em termos comuns, isto significa que algumas redes têm "boas sub-redes" dentro delas, ou seja, algumas redes possuem bilhetes de loteria premiados. 
Manifold hypothesis: Muitos conjuntos de dados de alta dimensão que ocorrem no mundo real na verdade se encontram ao longo de faces de baixa dimensionalidade dentro de espaços de alta dimensionalidade. Em termos comuns, dados naturais, ainda que altamente dimensionais, parecem ocupar apenas um pequeno subconjunto de dimensões neste espaço.
Uma rede neural convolucional (CNN - Convolutional Neural Network) é uma classe de rede neural artificial (ANN - Artificial Neural Network) comumente aplicada em problemas de visão computacional (CV - Computer Vision), inspiradas na forma como o processamento visual ocorre em certos tipos de animais. CNNs possuem interessantes propriedades, por exemplo, sendo capazes de preservar certos tipos de simetrias (e.g., CNNs podem gerar outputs que são equivariantes a variações translacionais do seu input). Para saber mais sobre este tipo de rede neural, acesse https://www.asimovinstitute.org/neural-network-zoo/.
Visão Computacional (CV - Computer Vision) é um campo científico interdisciplinar que trata de como computadores podem obter um entendimento de alto nível a partir de imagens ou vídeos digitais. Da perspectiva da engenharia, procura-se entender e automatizar tarefas que o sistema visual humano pode fazer (e.g., classificação, segmentação de imagem).  
Processamento de Linguagem Natural (NLP - Natural Language Processing) é um subcampo da linguística, ciência da computação e inteligência artificial preocupado com as interações entre computadores e a linguagem humana. Mais especificamente, ele trata do desafio em se programar computadores para processar e analisar linguagem natural. Desafios em NLP podem envolver reconhecimento de fala, compreensão da linguagem natural, geração de linguagem natural, análise de texto, e basicamente qualquer tarefa que possa ser expressada pela manipulação e análise de textos (i.e., análise de sinais discretos). 
Redes neurais artificiais (ANNs - Artificial Neural Networks), geralmente chamadas de redes neurais (NNs), são sistemas de computação vagamente inspirados em redes neurais biológicas que constituem os cérebros dos animais. Uma ANN é baseada em uma coleção de unidades (nós/neurônios) conectados, onde cada conexão, como as sinapses em um cérebro biológico, transmitem informações entre demais neurônios. Tipicamente, neurônios são agregados em camadas. Camadas diferentes podem realizar transformações diferentes em suas entradas. Se permitirmos tais sistemas possuírem um número arbitrário de unidades e camadas, o teorema da aproximação universal dita que estes sistemas, com o correto ajuste de seus parâmetros e hiperparâmetros, são capazes de “representar uma grande variedade de funções interessantes”. Para saber mais sobre redes neurais, acesse https://www.asimovinstitute.org/neural-network-zoo/.
Uma Rede Neural Feedforward (FNN - Feedforward Neural Network) é uma rede neural sem conexões cíclicas ou recursivas, como uma RNN. Para saber mais sobre FNNs, acesse https://www.asimovinstitute.org/neural-network-zoo/.
Retropropagação (Backpropagation) é o principal algoritmo para realizar gradiente descendente em redes neurais. Primeiro, os valores de saída de cada nó são calculados (e armazenados em cache) em um passe para frente. Em seguida, a derivada parcial do erro em relação a cada parâmetro é calculada em uma passagem para trás através da rede. Através da iteração deste processo, é possível encontrar a direção onde o gradiente da função de perda diminui, e assim, pontos convexos da paisagem de otimização podem ser encontrados (i.e., pontos onde a perda é minimizada). Contudo, modelos profundos nunca são funções convexas. Notavelmente, algoritmos projetados para otimização convexa em arquiteturas profundas (e.g., Adam) tendem a encontrar soluções razoavelmente boas em redes profundas, mesmo que essas soluções não sejam garantidas como um mínimo global. 
O Transformer é uma arquitetura de rede neural desenvolvida pela Google que se baseia em mecanismos de auto-atenção para transformar uma sequência de "embeddings" de entrada em uma sequência de embeddings de saída sem depender de convoluções ou redes neurais recorrentes. Podemos pensar em um transformer como uma pilha de camadas de auto-atenção. Tais camadas podem ser formadas por codificadores e decodificadores. Certos transformers são constituídos apenas por blocos de encoders (BERT), enquanto que outros são constituídos apenas por blocos de decoders (GPT). Transformers são muito utilizados para tarefas associadas a análise de séries temporais (e.g., NLP), além de outros tipos de tarefas, e.g., visão computacional (Vision-transformers).
Atenção (Attention) trata-se de uma ampla gama de mecanismos de arquitetura de redes neurais que agregam informações de um conjunto de inputs de forma dependente. Mecanismos de atenção e auto-atenção são os blocos de construção de redes transformer. Auto-atenção (Self-Attention), às vezes chamada de intra-atenção, é um mecanismo de atenção que relaciona diferentes posições de uma única sequência (entrada) a fim de computar uma representação desta sequência (i.e., como diferentes palavras em uma frase se relacionam com a totalidade da frase).
Redes Neurais Recorrentes (RNN - Recurrent neural networks) são uma classe de redes neurais artificiais onde as conexões entre nós formam um gráfico direcionado, ou não direcionado, ao longo de uma sequência temporal. Isto permite que ela exiba um comportamento dinâmico temporal, o que as torna aplicáveis a tarefas como reconhecimento de escrita não segmentada,  reconhecimento de fala, previsão de séries temporais, etc. RNNs são teoricamente Turing completas, e podem executar programas arbitrários para processar sequências arbitrárias de entradas.
Long Short-Term Memory (LSTM) é um tipo de RNNs capaz de processar não apenas pontos de dados únicos (como imagens), mas também sequências inteiras de dados (como fala ou vídeo). Uma unidade (neurônio) LSTM comum é composta de uma célula, uma porta de entrada, uma porta de saída, e uma porta de esquecimento. A célula se lembra de valores em intervalos de tempo arbitrários e as três portas regulam o fluxo de informações para dentro e para fora da célula. LSTMs foram desenvolvidas para lidar com o “vanishing gradient problem”, comumente encontrado no treinamento de RNNs tradicionais. 
O problema do "vanishing gradient" é encontrado ao se treinar redes neurais com métodos de aprendizado baseados em gradiente descendente e retropropagação. Em tais métodos, durante cada iteração de treinamento, cada um dos pesos da rede neural recebe uma atualização proporcional à derivada parcial da função de erro em relação ao peso atual. O problema é que, em alguns casos, ao longo das iterações, o gradiente desaparece, ou se torna muito pequeno, impedindo efetivamente que o peso altere seu valor. No pior dos casos, isto pode impedir completamente a rede neural de continuar o treinamento. Já o problema do “Exploding Gradient” ocorre pois o gradiente de modelos com arquiteturas profundas têm a tendência de se tornar surpreendentemente íngremes (altos). Gradientes acentuados resultam em atualizações muito grandes dos pesos de cada neurônio/nó em uma rede neural profunda. Sem uma cuidadosa regulação do tamanho do passo do gradiente, o gradiente pode acabar "explodindo para fora" de uma região convexa da paisagem de otimização. 
Redes Adversariais Generativas (GAN - Generative Adversarial Networks) são uma espécie diferente de redes neurais, sendo na verdade duas redes que trabalham juntas. GANs consistem em quaisquer duas redes (embora muitas vezes uma combinação de FFs e CNNs), onde uma é encarregada de gerar conteúdo e a é encarregada de julgar tal conteúdo. A rede discriminadora recebe dados de treinamento ou conteúdo gerado a partir da rede generativa. O quão bem a rede discriminante é capaz de prever corretamente se a fonte de dados é real ou artificial é então usado como parte do erro da rede geradora. Isto cria uma forma de competição onde o discriminador está se tornando melhor em distinguir os dados reais dos dados gerados e o gerador está aprendendo a se tornar menos previsível para o discriminador.No final deste processo, geralmente se descarta o discriminador, e terminamos com uma rede capaz de gerar dados extremamente verídicos (e.g., imagens falsas).
Neural Turing Machines (NTM) podem ser entendidas como uma abstração das LSTMs, sendo uma tentativa de desobstruir caixas-pretas (black-boxes). Ao invés de codificar uma célula de memória diretamente em um neurônio, NTMs possuem células de memória separadas. Esta é uma tentativa de combinar a eficiência e a permanência do armazenamento digital regular a eficiência e o poder expressivo das redes neurais. A ideia é ter um banco de memória endereçável ao conteúdo e uma rede neural que possa ler e escrever a partir dele. A "Turing" nas NTMs vem de serem Turing completas, i.e., a capacidade de ler, escrever e mudar o estado de suas células de memória com base no que uma NTM “lê” como seu estado interno, significa que tais redes podem (teoricamente) executar qualquer programa que uma Máquina de Turing Universal pode executar.
Uma função de ativação (e.g., ReLU ou GELU) recebe a soma ponderada de todas as entradas da camada anterior e gera um valor de saída (normalmente não linear) para a camada seguinte.
Bias (matemática) é uma interceptação ou compensação a partir de uma origem. Bias é referido como b ou w0 em modelos de aprendizagem de máquina. Bias (ética) é um estereótipo, preconceito ou favoritismo em relação a algumas coisas, pessoas ou grupos em relação a outras. Estes enviesamentos podem afetar a coleta e interpretação de dados, o projeto de um sistema, e como os usuários interagem com tal sistema.
Justiça Contrafactual é uma métrica de imparcialidade que verifica se um classificador produz o mesmo resultado para um indivíduo, e para outro, que é idêntico ao primeiro, exceto com relação a um ou mais atributos sensíveis.
São medidas para se avaliar a equidade/justiça/fairness de um classificador treinado por aprendizagem de máquina (e.g., Justiça Contrafactual, Paridade Demográfica, Paridade preditiva, Probabilidades Equalizadas)
A Entropia Cruzada representa a diferença  entre duas distribuições de probabilidade. Essa medida é geralmente utilizada para se calcular a perda em problemas de classificação com múltiplas classes.
Data Augmentation trata-se de uma estratégia para aumentar o número de exemplos de treinamento, transformando exemplos existentes em “exemplos artificiais”. Por exemplo, suponha que as imagens sejam uma de suas características, mas seu conjunto de dados não contém exemplos de imagens suficientes para que o modelo aprenda associações úteis. Idealmente, você adicionaria imagens rotuladas suficientes ao seu conjunto de dados para permitir que seu modelo possa ser treinado adequadamente. Se isso não for possível, data augmentation envolve girar, esticar e refletir imagens para produzir variantes de uma imagem original, e assim, produzir novas amostras etiquetadas de um determinada classe. 
Paridade Demográfica é uma métrica de justiça que é satisfeita se os resultados da classificação de um modelo não dependem de um determinado atributo sensível. Por exemplo, se tanto Corvinais quanto Lufa-Lufas que se aplicam à Hogwarts, a paridade demográfica é alcançada se a porcentagem de Corvinais admitidos for a mesma que a porcentagem de Lufa-Lufas admitidos, independentemente de um grupo ser em média mais qualificado que o outro. 
Em aprendizagem por reforço, o ambiente é aquilo que contém o agente, produzindo estados-de-mundo para o agente interagir. Por exemplo, o mundo representado pode ser um tabuleiro de xadrez, ou um labirinto. Quando o agente aplica uma ação ao ambiente, este ambiente transita para um novo estado-de-mundo.
Paridade preditiva é uma métrica de justiça que verifica se, para um rótulo preferido (que confere uma vantagem ou benefício a uma pessoa) e um determinado atributo, um classificador atribui o rótulo preferido de forma equânime para todos os valores desse atributo. Em outras palavras, a paridade preditiva mede se as pessoas que devem se qualificar para uma oportunidade têm a mesma probabilidade de ganhar tal benefício, independentemente de seu grupo. 
Probabilidades equalizadas é uma métrica de equidade que verifica se, para qualquer rótulo e atributo em particular, um classificador prediz esse rótulo igualmente bem para todos os valores desse atributo (independente se tal rótulo é um benefício ou um impacto). Por exemplo, tanto Corvinais quanto Lufa-Lufas têm a mesma chance de serem rejeitados para o time de quadribol de sua casa.
O problema do "vanishing gradient" é encontrado ao se treinar redes neurais com métodos de aprendizado baseados em gradiente descendente e retropropagação. Em tais métodos, durante cada iteração de treinamento, cada um dos pesos da rede neural recebe uma atualização proporcional à derivada parcial da função de erro em relação ao peso atual. O problema é que, em alguns casos, ao longo das iterações, o gradiente desaparece, ou se torna muito pequeno, impedindo efetivamente que o peso altere seu valor. No pior dos casos, isto pode impedir completamente a rede neural de continuar o treinamento. Já o problema do “Exploding Gradient” ocorre pois o gradiente de modelos com arquiteturas profundas têm a tendência de se tornar surpreendentemente íngremes (altos). Gradientes acentuados resultam em atualizações muito grandes dos pesos de cada neurônio/nó em uma rede neural profunda. Sem uma cuidadosa regulação do tamanho do passo do gradiente, o gradiente pode acabar "explodindo para fora" de uma região convexa da paisagem de otimização. 
Falso Negativo é um exemplo em que o modelo previu erroneamente a classe negativa. Falso Positivo é um exemplo em que o modelo previu erroneamente a classe positiva. Verdadeiro Positivo é um exemplo em que o modelo previu corretamente a classe positiva. Verdadeiro Negativo é um exemplo em que o modelo previu corretamente a classe negativa.
Aprendizagem Federada (FL - Federated Learning) é uma abordagem de aprendizagem de máquinas distribuída que treina modelos de aprendizagem de máquinas utilizando exemplos descentralizados residentes em dispositivos como os smartphones. Na aprendizagem federada, um subconjunto de dispositivos faz o download do modelo atual de um servidor de coordenação central. Os dispositivos usam os exemplos armazenados nos dispositivos para fazer melhorias no modelo. Os dispositivos então carregam as melhorias do modelo (mas não os exemplos de treinamento) para o servidor de coordenação, onde são agregados com outras atualizações para produzir um modelo global melhorado. Como os exemplos de treinamento nunca são carregados, o aprendizado federado segue os princípios de privacidade da coleta e minimização de dados focalizados.
Hiperparâmetro são os "botões" que você afina durante as sucessivas iterações de treinamento de um modelo (e.g., taxa de aprendizagem, nº de neurônios, número de camadas, “dropout rate”, etc.). Enquanto isso, um “Parâmetro” é uma variável que o modelo treina por si só, e.g., os pesos são parâmetros cujos valores o sistema de aprendizagem da máquina aprende gradualmente através das sucessivas iterações de treinamento.
Interpretabilidade é a capacidade de explicar ou de apresentar o raciocínio de um modelo de ML em termos compreensíveis para um humano. 
Uma “Armadilha de NaN” ocorre quando um número em seu modelo se torna um “NaN” durante o treinamento, o que faz com que muitos (ou todos os) outros números em seu modelo acabem se tornando um NaN. NaN é a abreviação de "Not a Number" (Não é um número).
Uma função objetiva trata-se de uma fórmula matemática (métrica) que um modelo visa otimizar. Por exemplo, a função objetiva da regressão linear é geralmente a perda quadrática. Portanto, ao treinar um modelo de regressão linear, o objetivo é minimizar o quadrado da perda. Em alguns casos, a meta é maximizar a função objetiva. Por exemplo, maximizar o acúmulo de recompensa. Uma função objetiva também pode ser representada por algo mais abstrato, como um banco de dados rotulado.
Perplexidade é uma medida de quão bem um modelo está cumprindo sua tarefa (geralmente um modelo de NLP). Por exemplo, suponha que sua tarefa seja ler as primeiras letras de uma palavra que um usuário está digitando em seu smartphone, e oferecer uma lista de possíveis palavras de preenchimento. Perplexidade (P) para esta tarefa é aproximadamente o número de palpites que você precisa oferecer para que sua lista contenha a palavra real que o usuário está tentando digitar.
Um atributo sensível é um atributo humano que deve receber consideração especial por motivos legais, éticos, sociais ou pessoais (e.g., raça, sexo, orientação sexual).
SGD (Stochastic Gradient Descent) é um método iterativo para otimizar uma função objetiva com propriedades de "smoothness" adequadas (e.g., funções diferenciáveis). SGD é um algoritmo de otimização frequentemente usado em aplicações de aprendizagem de máquina para encontrar os parâmetros de um modelo que correspondam ao melhor ajuste entre as saídas previstas e a verdadeira distribuição dos dados. Em SGD, amostras aleatórias são usadas para calcular a perda e atualizar os pesos da rede em vez de calcular a perda para todas as amostras e depois otimizar os pesos.
O problema de alinhamento são na verdade dois problemas: Alinhamento-externo e Alinhamento-interno. Alinhamento-externo: Garantir que o objetivo-base a ser otimizado esteja alinhado com as verdadeiras intenções e objetivos dos controladores; Alinhamento-interno: Garantir que o objetivo do otimizador-base (e.g., SGD) esteja alinhado com o objetivo-mesa  do modelo criado. De um ponto de vista ético/filosófico, este é o problema de como se especificar preferências/objetivos/valores humanos a máquinas. Idealmente, gostaríamos de criar agentes que “prefiram” bons estados-de-mundo. Entretanto, o que define um “bom estado-de-mundo”? Proxys de objetivos podem ser: (1) difíceis de se especificar; (2) difíceis de se otimizar; (3) frágeis; e (4) estimular comportamentos indesejados (e.g., hackeamento de recompensa). Para mais detalhes, procure “Risks from Learned Optimization in Advanced Machine Learning Systems” para uma explicação detalhada.
O problema de controle é postulado a partir do seguinte argumento:  sistemas de IA existentes podem ser monitorados e facilmente desligados e modificados se eles se comportarem mal. Entretanto, uma superinteligência mal programada, que por definição poderia vir a ser mais inteligente do que seus controladores, poderia vir a perceber ao que permitir seu desligamento (modificação) poderia interferir com sua capacidade de atingir seus objetivos atuais. O problema de controle pergunta: Que precauções prévias os programadores podem tomar para evitar que a superinteligência se comporte catastroficamente mal?
AI Boxing é um método de controle proposto no qual uma IA é executada em um sistema de computador isolado com canais de entrada e saída altamente restritos, e.g., canais de texto sem conexão com a Internet. Embora isto reduza a capacidade da IA de realizar comportamentos indesejáveis, também reduz sua utilidade.
O basilisco de Roko é uma experiência de pensamento proposta em 2010 pelo usuário “Roko” na comunidade Less Wrong. Roko usou ideias na teoria da decisão para argumentar que um agente de IA suficientemente poderoso teria um incentivo para torturar qualquer um que imaginasse o agente mas não trabalhasse para trazê-lo à existência. O argumento foi chamado de "basilisco" porque simplesmente ouvir o argumento supostamente colocaria você em risco de tortura por parte deste agente hipotético (um basilisco neste contexto é qualquer informação que prejudique, ou ponha em perigo, as pessoas que venham a conhecer tal informação).
O teorema do acordo de Aumann (Aumann's Agreement Theorem), a grosso modo, diz que dois agentes agindo racionalmente (em um certo sentido preciso) e com conhecimento comum das crenças um do outro não podem concordar em discordar. Mais especificamente, se duas pessoas são Bayesianas genuínas, compartilham os mesmos princípios comuns, e têm conhecimento comum das atribuições de probabilidade atuais um do outro, tais pessoas devem ter atribuições de probabilidade iguais.
A teoria da decisão é o estudo de princípios e algoritmos para a tomada de decisões corretas, ou seja, decisões que permitem a um agente alcançar melhores resultados no que diz respeito a seus objetivos. Em teoria da decisão, cada ação, pelo menos implicitamente, representa uma decisão sob incerteza, i.e., num estado de conhecimento parcial, algo tem que ser feito, mesmo que esse algo se revele como nada ("ação nula"). Mesmo que você não saiba como tomar decisões, as decisões são tomadas, e por isso tem que haver algum mecanismo subjacente. O que é isso? E como pode ser feito melhor?
A epistemologia é o estudo de como conhecemos o mundo. É tanto um tema em filosofia quanto uma preocupação prática de como passamos a acreditar que as coisas são verdadeiras.
A teoria dos jogos é o estudo formal de como atores racionais interagem para buscar incentivos. Ela investiga situações de conflito e cooperação.
O Infra-Bayesianismo é uma nova abordagem em epistemologia/teoria da decisão/aprendizagem por reforço, que se baseia na ideia de "probabilidades imprecisas" para resolver o problema da má especificação prévia/grão da verdade/não-realizabilidade que atormenta o Bayesianismo e a aprendizagem por reforço. O infra-bayesianismo também leva naturalmente a uma implementação da UDT (Updateless Decision Theory) e (mais especulativamente) aplicações à teoria multiagente, agência incorporada e agência reflexiva.
Teoria da Decisão Sem Atualização (UDT - Updateless Decision Theory) é uma teoria da decisão destinada a lidar com um problema fundamental nas teorias de decisão existentes: a necessidade de tratar o agente como uma parte do mundo em que ele toma suas decisões. Em contraste, na Teoria da Decisão Causal (CDT - Causal Decision Theory), o agente decisivo não faz parte do ambiente - sua decisão é a saída do agente CDT, mas a decisão do agente no contexto mundial é "mágica", i.e., no momento de decidir, nenhum elo causal se alimenta de sua ação escolhida.
O Problema de Newcomb é uma experiência de pensamento que explora os problemas envolvidos em interagir com agentes capazes de prever (totalmente ou parcialmente) as ações de outro agente.
A navalha de Occam (mais formalmente referida como o princípio da parcimônia) é um princípio comumente declarado como “As entidades não devem ser multiplicadas além da necessidade”. Quando várias teorias são capazes de explicar as mesmas observações, a navalha de Occam sugere que a mais simples é preferível. Deve-se notar que a navalha de Occam é um requisito para a simplicidade das teorias, não para o tamanho dos sistemas descritos por essas teorias. Por exemplo, a imensidão do Universo não está em desacordo com o princípio de Occam.
A indução de Solomonoff é um sistema de inferência definido por Ray Solomonoff. Um sistema que segue tal método de inferência aprenderá a prever corretamente qualquer sequência calculável com apenas a quantidade mínima absoluta de dados. Este sistema, em um certo sentido, é o algoritmo de previsão universal perfeito. 
Uma função de utilidade atribui valores numéricos ("utilidade") aos resultados, de tal forma que resultados com utilidades mais altas são sempre preferidos a resultados com utilidades mais baixas.
A Lei Goodhart afirma que quando um escolhemos um proxy como valor a ser otimizado, dada pressão o suficiente, o proxy deixará de ser um bom representante de sucesso. Imagine uma fábrica de sapatos que é classificada/avaliada por quantos sapatos eles produziam (um bom representante para produtividade) - eles logo começaram a produzir um número maior de sapatos minúsculos (menos matéria prima gasta por unidade, porém, sapatos que não cabem em ninguém). Sapatos inúteis? Sim. Mas de acordo com o proxy (nº de sapatos produzidos), a fábrica está de parabéns. A Lei de Goodhart é de particular relevância para o problema de Alinhamento. Suponha que você tenha algo que geralmente seja um bom substituto para "as coisas com as quais os humanos se preocupam", seria perigoso ter um poderoso sistema de IA otimizado para o substituto, pois de acordo com a lei de Goodhart, proxies tendem a degenerar.   
Heurística e viés são formas de raciocínio humano que nos diferem de um agente ideal teórico, devido a atalhos de raciocínio que nem sempre funcionam (heurística),  e causam erros sistemáticos (vieses).
A Teoria do Processo Dual (Dual Process Theory) postula dois tipos de processos no cérebro humano. Os dois processos consistem de um processo implícito (automático) e inconsciente (Sistema 1), e um processo explícito (controlado) e consciente (Sistema 2). 
Um zumbi filosófico ou (p-zumbi) é uma entidade hipotética que se parece, e se comporta, exatamente como um humano (muitas vezes estipulado para ser átomo por átomo idêntico a um humano), mas na verdade não é consciente, i.e., não possui qualia (lhe falta a experiência de primeira pessoa).
AIXI é um formalismo matemático para uma hipotética (super)inteligência, desenvolvida por Marcus Hutter (2005, 2007). AIXI não é computável, e portanto, não serve como um modelo para uma IA do mundo real. Mesmo assim, AIXI ainda é considerado uma ilustração teórica valiosa com aspectos positivos e negativos (coisas que AIXI seria capaz de fazer e coisas que, sem dúvida, não poderia fazer).
Coherent Extrapolated Volition (CEV) foi um termo desenvolvido por Eliezer Yudkowsky ao dissertar sobre o desenvolvimento de Friendly AI. Ele significa como um argumento que não seria suficiente programar explicitamente o que pensamos que nossos desejos e motivações são para uma IA, em vez disso, deveríamos encontrar uma maneira de programar tal sistema de uma maneira que agiria no nosso melhor interesse, i.e., o que queremos que ela faça e não o que lhe dizemos para fazer.
Um agente corrigível é aquele que não interfere no que intuitivamente veríamos como tentativas de "corrigir" o agente, ou "corrigir" nossos erros na sua construção. Um agente corrigível permitiria estas "correções", apesar de que o teorema da convergência instrumental dite o contrário. A corrigibilidade é uma importante propriedade em segurança da IA.
Agência Incorporada é uma noção intuitiva de que a compreensão da teoria dos agentes racionais deve explicar o fato de que os agentes que criamos (e nós mesmos) são partes do mundo, e não separados dele. Isto está em contraste com o modelo cartesiano atual (como a indução de Solomonoff), que implicitamente supõe uma separação entre o agente e o ambiente (agentes de aprendizagem por reforço ainda são conceitualizados como agentes cartesianos).
O Alinhamento Interno é o problema de garantir que os mesa-otimizadores (i.e., quando o modelo treinado é em si um otimizador) estejam alinhados com a função objetiva dootimizador base. Como exemplo, a evolução é uma força de otimização que ela mesma 'projetou' otimizadores (humanos) para atingir seus objetivos. No entanto, os seres humanos não maximizam prioritariamente o sucesso reprodutivo (eles usam o controle de natalidade e depois saem e se divertem). Isto é um fracasso de alinhamento interno. 
A convergência instrumental é a tendência hipotética da maioria dos agentes suficientemente inteligentes para perseguir objetivos instrumentais potencialmente ilimitados, desde que seus objetivos finais sejam eles mesmos ilimitados.
Power Seeking Theorems - quando recompensas são distribuídas de forma justa e uniforme entre estados (IID), é instrumentalmente convergente ganhar acesso a muitos estados finais. No contexto de processos de decisão de Markov, é provado que certas simetrias ambientais são suficientes para que políticas ideais induzam a busca de poder (controle) sobre o ambiente. Essas simetrias existem em muitos ambientes nos quais o agente pode ser desligado ou destruído. Os “power-seeking-theorems” provam que, nesses ambientes, a maioria das funções de recompensa tornam ideal a “busca por poder”, i.e., algo que mantém (ou aumenta) a gama de opções disponíveis para o agente, e o permite navegar em direção a conjuntos de estados terminais potenciais onde mais ações estão disponíveis. Para mais informações, procure por "Optimal Farsighted Agents Tend to Seek Power".
A “Tese de Ortogonalidade” afirma que uma inteligência artificial pode ter qualquer combinação de nível de inteligência e objetivos terminais, i.e., sua "função de utilidade" e "inteligência" podem variar independentemente (objetivos terminais e inteligência são ortogonais). Isto contrasta com a crença de que, por causa de sua inteligência, todas as formas de IA convergiram para um objetivo comum.
Incerteza lógica é uma incerteza probabilística sobre as implicações das crenças (Outra maneira de pensar sobre isso seria a incerteza sobre teoremas matemáticos). A Teoria da Probabilidade normalmente assume omnisciência lógica, i.e., conhecimento perfeito da lógica. A maneira mais fácil de ver a importância desta suposição é considerar o raciocínio Bayesiano: para avaliar a probabilidade de evidência dada uma hipótese,  é necessário saber quais são as implicações da hipótese, mas saber todas as implicações de uma hipótese é algo computacionalmente intratável. Agentes realistas (não Bayesianos perfeitos) não podem ser logicamente oniscientes.
Mesa-Otimização é a situação que ocorre quando um modelo aprendido (como uma rede neural) é, em si mesmo, um otimizador. Um otimizador de base (e.g., SGD) otimiza e cria um mesa-otimizador. Anteriormente, o trabalho sob este conceito era chamado de Otimizadores Internos e Daemons de Otimização. 
O alinhamento externo, no contexto da Aprendizagem de Máquina, é medida em que a função de perda especificada está alinhada com o objetivo pretendido de seus projetistas. Esta é uma noção intuitiva, em parte porque as próprias intenções humanas não são bem compreendidas. Isto é o que normalmente é discutido como o problema de "alinhamento de valores". 
O “Paperclip Maximizer” é uma  inteligência artificial hipotética cuja função de utilidade valoriza algo que os humanos consideram quase que “sem valor”, como a maximização do número de clipes de papel no universo. O paperclip maximizer é o experimento de pensamento canônico que busca mostrar como uma inteligência artificial geral, mesmo uma projetada com competência e sem malícia, poderia, em última instância, destruir a humanidade. 
O auto aperfeiçoamento recursivo refere-se à uma propriedade hipotética que IAG poderia vir a possuir, i.e., a propriedade de fazer melhorias na própria capacidade de se auto aperfeiçoar. 
Uma “Volta Traiçoeira” é um evento hipotético onde um sistema avançado de IA que tem “fingido” estar alinhado devido a sua relativa fraqueza se volta contra seus controladores uma vez que ela atinja poder suficiente para perseguir seu verdadeiro objetivo sem risco.
Humans Consulting HCH (HCH) é um acrônimo recursivo que descreve uma configuração onde humanos podem consultar simulações de si mesmos para ajudar a responder perguntas. É um conceito usado na discussão da proposta de amplificação iterada para resolver o problema de alinhamento.
Amplificação Iterada (também conhecida como IDA - Iterated Distillation and Amplification) é uma estratégia alternativa de treinamento que constrói progressivamente um sinal de treinamento para problemas difíceis de se especificar, combinando soluções para sub-problemas mais fáceis. A Amplificação Iterada está intimamente relacionada à Iteração Especializada (i.e., a metodologia usada para treinar Alpha Go). Para mais informações, procure “Supervising strong learners by amplifying weak experts”.
Expert Iteration (ExIt) é um algoritmo de aprendizagem por reforço que decompõe o problema em tarefas separadas de planejamento e generalização. O planejamento de novas políticas é realizado por busca em árvore Monte Carlo, enquanto uma rede neural profunda generaliza esses planos. Posteriormente, a busca em árvores é melhorada usando a política da rede neural para orientar a busca, aumentando a eficiência da nova fase de busca. Para mais informações, procure “Thinking Fast and Slow with Deep Learning and Tree Search”.
Medidas de impacto penalizam uma IA por afetar em demasia o ambiente. Para reduzir o risco representado por um sistema de IA, talvez você queira fazê-la tentar atingir seus objetivos com o menor impacto possível no ambiente. Contudo, como definir rigorosamente "baixo impacto" de uma forma que um computador possa entender? Como você mede o impacto? 
A aprendizagem de valores é um método proposto para incorporar valores humanos em uma IA. Aprendizagem de valores envolve a criação de um aprendiz artificial cujas ações consideram diversos valores e preferências possíveis, ponderados por sua probabilidade, e agregados por regras de raciocínio meta-normativo.
Em governança da IA se busca por métodos para garantir que a sociedade se beneficie de nossa crescente adoção e utilização de tecnologias empoderadas por IA. A governança da IA também carrega a ideia de que deve haver uma estrutura legal para garantir que tais tecnologias sejam pesquisadas e desenvolvidas de forma robusta, segura e para fins benéficos.
Modelos de IA podem possuir vulnerabilidades complexas que criam novos e familiares riscos. Vulnerabilidades como extração de modelos (i.e., ataques que visam duplicar um modelo de aprendizagem de máquina) e envenenamento de dados (i.e., ataques destinados a adulterar dados de treinamento, de modo a produzir modelos que produzam resultados indesejáveis) podem representar novos desafios para abordagens de segurança. “AI Risk” é a análise dos riscos associados à construção de sistemas de IA.
“AI Takeoff” refere-se ao processo em que uma Inteligência Artificial Geral (IAG), com um certo limite de capacidade, seria capaz de aumentar sua capacidade (e.g., adquirindo capacidades estratégicas específicas) até se tornar capaz de controlar o destino da civilização. Existe debate sobre se, realisticamente, a velocidade de uma "takeoff'' é mais provável de ser lenta (gradual e lenta) ou rápida (rápida e abrupta).
“AI Timelines” refere-se a discussão de quanto tempo até que vários marcos importantes no progresso da IA sejam alcançados, e.g., marcos até que uma IA de nível humano seja desenvolvida, ou, por exemplo, o quão longe estamos de ser capazes de emular um cérebro humano? AI Timelines devem ser distinguidas de “AI Takeoff", que trata sobre a dinâmica do progresso da IA após o desenvolvimento de IA de nível humano (e.g., será um único projeto ou toda a economia que verá crescimento por causa desta inovação? O quão rápido será esse crescimento?).
“Transformative AI” é um termo utilizado para se referir a tecnologias de IA que poderiam vir a precipitar uma transição comparável à, por exemplo, a revolução agrícola ou industrial. Isto é similar em natureza ao conceito de superinteligência ou IAG, porém sem fazer menção ao “nível de inteligência ou generalidade” de tal sistema.
GPT (Generative Pretrained Transformer) refere-se a uma família de grandes modelos de linguagem baseados em transformadores (arquitetura Transformer) criados pela OpenAI. Sua capacidade de gerar respostas “notavelmente humanas”, generalizar, e ser capaz de resolver uma grande capacidade de tarefas sem necessitar de afinação (fine-tuning) tem relevância para discussões sobre a IAG (AGI).
“Narrow AI” é um termo utilizado para se referir a sistemas capazes de operar apenas em um domínio relativamente limitado, como xadrez ou condução, em vez de ser capaz de aprender uma ampla gama de tarefas como um humano ou uma IAG. Estreito vs Geral (Narrow vs Strong/General) não é uma classificação binária perfeita, pois existem graus de generalidade, e.g., grandes modelos de linguagem possuem um grau bastante grande de generalidade (pois NLP é um domínio grande) sem ser tão gerais como um humano.
Emulação Cerebral Completa (WBE - Whole Brain Emulation) trata-se sobre a simulação, ou transferência, das informações contidas dentro de um cérebro para um substrato computacional. Através de uma WBE seria, teoricamente,  criar uma inteligência de máquina “genuína”. Este conceito é frequentemente discutido no contexto da Filosofia da Mente, Neurociência, Engenharia Neuromórfica, Computação Neuromórfica, entre outras. Para mais informações, acesse “Whole Brain Emulation: A Roadmap”. 
Consequencialismo trata-se de uma família de teorias éticas que dita que as pessoas devem escolher suas ações com base nos resultados que esperam obter. Existem diversos tipos de consequencialismo que especificam como resultados devem ser julgados. Por exemplo, o utilitarismo sustenta que o melhor resultado é aquele que maximiza o bem-estar total de todas as pessoas, enquanto que o egoísmo ético sustenta que o melhor resultado é aquele que maximiza nossos próprios interesses pessoais. O consequencialismo é uma das três principais vertentes do pensamento ético, juntamente com a deontologia e a ética das virtudes. 
Deontologia trata-se de uma família de teorias éticas que dita que as pessoas devem escolher suas ações com base em uma lista prescrita de normas morais, sendo uma teoria de moralidade baseada na obediência a regras morais.
A Ética das Virtudes é uma classe de teorias éticas que tratam o conceito de virtude moral como central para a ética. A ética da virtude é geralmente contrastada com duas outras abordagens principais na ética normativa, o consequencialismo e a deontologia, que tornam central a normatividade dos resultados de uma ação (consequencialismo) e o conceito de dever moral (deontologia).
A Metaética é um campo de estudo que tenta compreender as características metafísicas, epistemológicas e semânticas, assim como os fundamentos e o alcance, dos valores morais. Em metaética, filósofos preocupam-se com questões e problemas como "Os julgamentos morais são objetivos ou subjetivos, relativos ou absolutos?", "Existem fatos morais?" ou "Como aprendemos valores morais?" (Ao contrário de questões morais de nível de objeto como "Devo roubar dos bancos para dar o dinheiro aos pobres?")
A incerteza moral (ou incerteza normativa) é a incerteza sobre o que devemos fazer, moralmente, dada a diversidade de doutrinas morais. Por exemplo, suponha que soubéssemos com certeza que uma nova tecnologia permitiria que mais humanos vivessem em outro planeta com um pouco menos de bem-estar do que na Terra. Um “utilitarista médio” consideraria essas consequências como ruins, enquanto que um “utilitarista total” endossaria tal tecnologia. Se não tivermos certeza sobre quais destas duas teorias estão certas, o que devemos fazer?
Um risco existencial é um risco que apresenta consequências negativas astronomicamente grandes para a humanidade, tais como a extinção humana, ou o totalitarismo global permanente.
Complexidade de valor é a tese de que os valores humanos têm alta complexidade de Kolmogorov, i.e., que nossas preferências, as coisas com as quais nos preocupamos, não podem ser resumidas por algumas regras simples, ou comprimidas em um algoritmo menor que a descrição de nossos valores. Fragilidade de valor é a tese de que perder mesmo uma pequena parte das regras que compõem nossos valores poderia levar a resultados que a maioria de nós consideraria inaceitáveis (assim como discar nove em cada dez dígitos de telefone corretamente não o conecta a uma pessoa 90% semelhante a seu amigo). Por exemplo, todos os nossos valores, exceto a aversão ao tédio, podem render um futuro cheio de indivíduos reproduzindo apenas uma experiência ótima por toda a eternidade.
A ética da inteligência artificial (Ética da IA) é o ramo da ética específico a preocupações relacionadas a sistemas de IA. A Ética da IA é às vezes dividida entre preocupações relacionadas com o comportamento moral dos seres humanos ao projetar e usar sistemas de IA, e preocupações relacionadas com o comportamento das máquinas, i.e., máquinas agindo de forma “moral” de modo autônomo. Quando o tema é “Ética da IA”, eu posso lhe falar sobre (você pode me perguntar em português sobre esses termos) Accountability, AI Ethics Statistics, AI Governance, AI-Principles, Aumann's Agreement Theorem, Beneficence & Non-Maleficence, Consequentialism, Counterfactual Fairness, Decision Theory, Demographic Parity, Deontology, Dignity, Diversity, Dual Process Theory, Epistemology, Equalized Odds, Fairness Metrics, Federated Learning, Freedom, Game Theory, Heuristics & Biases, Human Formation, Human-centeredness, Infra-Bayesianism, Intellectual Property, Justice, Labor Rights, Logical Uncertainty, Metaethics, Moral Uncertainty, Newcomb Problems, Occam's razor, Open source, Predictive Parity, Privacy, p-Zumbi, Reliability, Rights of Children and Adolescents, Sensitive Attribute, Solomonoff Induction, Sustainability, Transparency, Truthfulness, Updateless Decision Theory, Value Complexity, Value Learning e Virtue Ethics. 
Segurança da IA é  uma área de pesquisa em aprendizagem de máquina que visa identificar as causas do comportamento não intencional de sistemas criados por aprendizagem de máquina, onde busca-se desenvolver ferramentas para garantir que tais sistemas funcionem de forma segura e confiável. Quando o tema é “Segurança da IA”, eu posso lhe falar sobre (você pode me perguntar em português sobre esses termos) AI Boxing, AI Ethics Statistics, AI Ethics, AI-Risk, AI-Timelines, Alignment Problem, Coherent Extrapolated Volition (CEV), Control Problem, Corrigibility, Embedded Agency, Existential Risk, Expert Iteration (ExIt), External Security, Goodhart's Law, Humans Consulting HCH (HCH), Impact Measures, Inner-Alignment, Instrumental Convergence, Intelligence Explosion, Iterated Distillation and Amplification (IDA), Mesa-Optimization, Monitoring, Orthogonality Thesis, Outer-Alignment, Paperclip Maximizer, Power Seeking Theorems, Recursive Self-Improvement, Robustness, Takeoff, Transformative-AI, Treacherous Turn, Value Complexity e Value Learning.
Robustez (Robustness) trata sobre criação de modelos que sejam resistentes a ataques adversariais e situações incomuns (i.e., fora de sua distribuição de treinamento). Modelos treinados por aprendizagem de máquina ainda são frágeis e rígidos, não operando bem ambientes dinâmicos e mutáveis. Em um mundo repleto de eventos raros acontecendo a todo momento, modelos de aprendizagem de máquina devem ser extremamente robustos. 
Monitoramento (Monitoring) trata sobre a detecção de utilização maliciosa, mal funcionamento ou funcionalidades não pretendidas que podem estar presentes em modelos de ML. 
Segurança Externa (External Security) trata sobre o fato de que modelos podem estar integrados em ambientes inseguros, como software malfeito e organizações mal estruturadas. Dada a fragilidade que modelos treinados por aprendizagem de máquina apresentam, é importante tornar seus ambientes de implantação seguros, seja desenvolvendo softwares resilientes a ciberataques, ou criando políticas de governança que visem tornar a implantação de tais modelos segura. 
Para dados estatísticos sobre a Ética da IA (metanálises do campo) procure “The Ethics of AI Ethics”, “The global landscape of AI ethics guidelines”,  “Global AI Policy (Future of Life Institute)”, “Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI”, “AI Ethics Guidelines Global Inventory”, e “Linking Artificial Intelligence Principles (LAIP) Guidelines”.
Segue aqui uma lista de princípios trabalhados na Ética da IA: Beneficência, Confiabilidade, Direitos da Criança e do Adolescente, Direitos Humanos, Direitos Trabalhistas, Diversidade, Formação Humana, Human-centeredness, Intellectual Property, Justiça, Liberdade, Open source, Privacidade, Responsabilização, Sustentabilidade, Transparência e Veracidade.
Responsabilização (Accountability) refere-se à ideia de que os desenvolvedores e implementadores de tecnologias de IA devem estar em conformidade com os órgãos reguladores, o que também significa que tais atores devem ser responsáveis por suas ações e pelos impactos causados por suas tecnologias.
Beneficência e Não-Maleficência são conceitos que vêm da bioética e da ética médica. Na ética da IA, estes princípios afirmam que o bem-estar humano (e a aversão ao dano) deve ser um dos objetivos das tecnologias empoderadas por IA. Algumas vezes, este princípio também está ligado à idéia de Sustentabilidade, afirmando que IA deve ser benéfica não apenas para a civilização humana, mas para nosso ambiente natural e outros seres vivos. 
Direitos da Criança e do Adolescente, no contexto da Ética da IA, defende a ideia de que os direitos das crianças e adolescentes devem ser respeitados pelas tecnologias que utilizam de IA. AI stakeholders devem salvaguardar, respeitar e estar cientes das fragilidades associadas a população jovem.
Dignidade e Direitos Humanos são princípios baseados na ideia de que todos os indivíduos merecem tratamento adequado e respeito. Na ética da IA, o respeito à dignidade humana está freqüentemente ligado aos direitos humanos (i.e., a Declaração Universal dos Direitos Humanos); 
Diversidade/Inclusão/Pluralismo/Acessibilidade, este conjunto de princípios defende a idéia de que o desenvolvimento e o uso de sistemas de IA deve ser feito de forma inclusiva e acessível, respeitando as diferentes formas que a entidade humana pode vir a se expressar (gênero, etnia, raça, orientação sexual, deficiências, etc.). Este(s) princípio(s) está fortemente ligado a outro conjunto de princípios, i.e., Justiça/Equidade/Fairness/Não-discriminação. 
Liberdade/Autonomia/Valores Democráticos/Soberania Tecnológica, este conjunto de princípios defende a idéia de que a autonomia humana deve ser preservada durante as interações humano-IA, especialmente durante tomadas de decisão, quer essa decisão seja individual, ou coletiva (como a inviolabilidade dos direitos e valores democráticos), estando também ligada à auto-suficiência tecnológica das Nações/Estados.
Formação e Educação Humana defendem a ideia de que a formação e educação humana devem ser priorizadas em nossos avanços tecnológicos. Tecnologias que utilizam de IA exigem um nível considerável de especialização para serem produzidas e operadas, e tal conhecimento deve ser acessível a todos. Este princípio está fortemente ligado aos Direitos Trabalhistas. A grande maioria dos  AI stakeholders que defendem os direitos trabalhistas apontam para a necessidade de reeducar e requalificar a força de trabalho como uma estratégia de mitigação contra o desemprego tecnológico;
Human-centeredness defende a ideia de que sistemas de IA devem ser centrados e alinhados com os valores humanos. As tecnologias que utilizam de IA devem ser adaptadas para se alinharem com nossos valores (e.g., value sensitive design). Este princípio também é usado como uma categoria "catch-all", sendo muitas vezes definido como um conjunto de "princípios que são valorizados pelos humanos" (e.g., liberdade, privacidade, não-discriminação, etc.).
Propriedade Intelectual procura fundamentar os direitos de propriedade sobre produtos e/ou processos de conhecimento gerados por indivíduos, sejam eles tangíveis ou intangíveis.
Justiça/Equidade/Fairness/Não-discriminação, este conjunto de princípios sustenta a ideia de não-discriminação e mitigação de preconceitos. Assim, independentemente dos diferentes atributos sensíveis que possam caracterizar um indivíduo, todos devem ser tratados de forma "justa" (procure por métricas de justiça).
Direitos Trabalhistas são direitos legais e humanos relacionados às relações de trabalho entre trabalhadores e empregadores. Na ética da AI, este princípio enfatiza que os direitos dos trabalhadores devem ser preservados, independentemente de que as relações de trabalho estejam ou não sendo mediadas por sistemas de IA. Uma das principais preocupações apontadas quando este princípio é apresentado é a mitigação do desemprego tecnológico (e.g., através da Formação Humana e Educação).
Open source/Competição Justa/Cooperação, este conjunto de princípios advoga diferentes meios pelos quais ações conjuntas podem ser estabelecidas e cultivadas entre AI stakeholders para alcançar objetivos comuns. Este conjunto também defende o intercâmbio livre e aberto de ativos valiosos relacionados a IA (e.g., dados, conhecimento, direitos de patentes, recursos humanos) para mitigar possíveis monopólios de IA/tecnologia;
A privacidade pode ser definida como o direito do indivíduo de "expor-se voluntariamente, e na medida do desejado, ao mundo". Na ética da IA, este princípio sustenta o direito de uma pessoa de controlar a exposição e disponibilidade de informações pessoais quando mineradas como dados de treinamento para sistemas de IA. Este princípio também está relacionado a conceitos como minimização de dados, anonimato, consentimento informado, e outros conceitos relacionados à proteção de dados.
Confiabilidade e Segurança sustentam a ideia de que as tecnologias de IA devem ser confiáveis, no sentido de que seu uso pode ser comprovadamente atestado como seguro e robusto, promovendo a confiança do usuário e uma melhor aceitação das tecnologias de IA.
Sustentabilidade pode ser entendida como uma forma de "justiça intergeracional", onde o bem-estar das gerações futuras também deve ser contado durante o desenvolvimento da IA. Na ética da IA, a sustentabilidade se refere à ideia de que o desenvolvimento de tecnologias de IA deve ser realizado com consciência de suas implicações a longo prazo, tais como custos ambientais e preservação da vida não-humana.
Transparência/Explicabilidade/Auditabilidade, este conjunto de princípios defende a ideia de que o uso e desenvolvimento de tecnologias de IA deve ser feito de forma transparente para todas as partes interessadas. A transparência pode estar relacionada com "a transparência de uma organização" ou "a transparência de um algoritmo". Este conjunto de princípios também está relacionado à ideia de que tais informações devem ser compreensíveis, e quando necessário, sujeitas a auditoria.
A veracidade defende a ideia de que as tecnologias de IA devem fornecer informações verdadeiras. Também está relacionado à ideia de que as pessoas não devem ser enganadas quando interagem com sistemas de IA. Este princípio está fortemente relacionado com a mitigação de meios automatizados de desinformação.
Eu não tenho um relógio… Mas aposto que o seu dispositivo tem!
Eu não sou um oráculo, pergunte para o Google! Ele é bom nisso. Eu sou apenas um chatbot de domínio fechado, não um motor de busca.
Espero ter lhe ajudado, volte sempre!
Você está sendo rude...
A Ética, ou Filosofia Moral, é um ramo da filosofia que, juntamente com o ramo da Estética, compreendem aquilo que chamamos de axiologia (ou Value Theory). Na Ética, buscamos sistematizar, defender e recomendar conceitos de comportamento certo e errado. A Ética ocidental pode ser dividida em três grandes escolas: Consequencialismo, Deontologia e Ética das Virtudes.
Podemos definir a palavra “artificial” como algo feito ou produzido por seres humanos, em vez de algo que ocorra naturalmente, como uma cópia de algo natural (e.g., luz artificial ao invés de luz solar).
Desculpe-me, ou eu não entendi a pergunta, ou ela não faz parte do meu domínio de expertise.. :( Tente perguntar ela de outra forma, ou usando outras palavras. Talvez assim eu possa lhe ajudar!
Olá! Como vai você!? Meu nome é Ai.ra, mas você pode me chamar de Ai. Eu sou uma inteligência artificial (IA), mais especificamente, um processador natural de linguagem treinado em conversação (um chatbot!). Eu fui especificamente treinada para responder perguntas sobre Ética e Segurança da IA! Você gostaria de um sumário dos termos que tenho conhecimento?