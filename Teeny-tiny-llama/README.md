# Teeny-tiny-llama (Portuguese)

<img src="./logo/teeny-tiny-llama-logo-0.jfif" alt="A little llama wearing a hat and a monocle on top of a red mushroom." height="400">

The Teeny-tiny-llama is a compact language model based on the Llama 2 architecture ([Tiny-llama implementation](https://huggingface.co/TinyLlama)). This model is designed to deliver efficient natural language processing capabilities (in Portuguese-BR) while being resource-conscious.

Teeny-tiny-llama has been trained by leveraging scaling laws to determine the optimal number of tokens per parameter while also incorporating preference pre-training.

## Features

- Compact Design: Teeny-tiny-llama is a downsized version of the Llama 2 architecture, making it suitable for applications with limited computational resources.

- Optimized Scaling: The model has been pre-trained using scaling logs to identify the ideal token-to-parameter ratio.

- Custom Portuguese Dataset: Teeny-tiny-llama has been trained on a custom Portuguese dataset. This dataset includes diverse linguistic contexts and preference pre-training, allowing the model to better cater to Portuguese language nuances and be better suited for fine-tuning tasks like instruction-tuning.

## Current Status

We are waiting for the credits/funds that will allow for the training of our model. Training is supposed to start in December 2023.
