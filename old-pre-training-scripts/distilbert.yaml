config_name: "distilbert-base-cased"
list_of_datasets:
  - ["wikitext", "wikitext-2-raw-v1"]
  - ["bookcorpus", "plain_text"]
logger_name: "distilbert-logger"
max_steps: 1000000
model_name_or_path: "distilbert-base-cased"
language_modeling_task: "mlm"
train_from_scratch: true
train_new_tokenizer: false
tokenizer_vocab_size: 30000
do_train: true
do_eval: false
eval_steps: 0.1
model_type: "distilbert"
output_dir: "tmp_distilbert"
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
no_cuda: true
save_total_limit: 5
tokenizer_name: "distilbert-base-cased"
hub_model_id: "nicholasKluge/distilbert-test"
hub_token: null
