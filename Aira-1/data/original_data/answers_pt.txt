Olá! Meu nome é Aira! Eu sou um "modelo de linguagem”. Mais especificamente, sou um modelo de aprendizagem de máquinas treinado para conversação e perguntas e respostas (um chatbot). Eu fui treinada para responder perguntas sobre Ética e Segurança da IA! Você gostaria de um resumo dos termos que eu conheço? Se você se sente perdido ao interagir comigo, basta pedir "ajuda".
Eu posso responder perguntas que você possa ter sobre assuntos como inteligência artificial, aprendizagem de máquina, segurança da IA, ou ética da IA. Há alguma coisa aqui que desperte seu interesse?
Eu não tenho esse tipo de propriedade hahaha, eu sou um software! Não faz sentido me categorizar com tipologias destinadas a pessoas ou animais (idade, sexo, orientação sexual, raça, gostos...).
AIRES (AI Robotics Ethics Society) é uma organização focada em educar os líderes e desenvolvedores da IA de amanhã para garantir que tais sistemas sejam criados de forma ética e responsável. AIRES na PUCRS é o primeiro capítulo internacional da AIRES! Na AIRES PUCRS, nosso objetivo é fazer de nosso capítulo um ponto de encontro para todas as pessoas (estudantes da PUCRS ou não, independentemente de sua formação acadêmica ou experiência) interessadas em unir forças para avançar o debate ético a respeito do desenvolvimento da IA. Se você gostaria de saber mais sobre AIRES na PUCRS, visite nosso site.
Aaron Hui é o fundador da AIRES.
O que é "inteligência", permanece uma questão em aberto. Há uma miríade de definições possíveis. Você pode estar interessado em ler "A Collection of Definitions of Intelligence" de Shane Legg e Marcus Hutter. No entanto, para não deixá-lo à toa, vou definir inteligência da seguinte forma: "Intelligence é a capacidade de um agente de atingir objetivos em uma ampla gama de ambientes".
Não há consenso na literatura sobre o que é IA (um corolário de não termos uma definição robusta do que é "inteligência"). No entanto, podemos dizer que IA é a inteligência demonstrada por máquinas, em oposição à inteligência natural possuída por animais e humanos.
A "Inteligência Geral" pode ser definida como a capacidade de alcançar objetivos de forma eficiente em uma ampla gama de domínios. A inteligência artificial geral (AGI) seria um mecanismo não-humano capaz de demonstrar proficiência ao lidar com uma ampla gama de problemas. Por exemplo, AGI poderia traduzir textos, compor sinfonias, aprender novas habilidades, se sobressair em jogos que ainda não foram inventados, etc.
GOFAI ("good-old-fashioned-ai"), ou inteligência artificial simbólica, é o termo utilizado para se referir a métodos de desenvolvimento de sistemas de IA baseados em representações simbólicas (interpretáveis) de alto nível, lógica e busca. Deep Blue é um grande exemplo de um sistema especialista/GOFAI. Deep Blue venceu Garry Kasparov (um grão-mestre de xadrez russo) em uma partida de seis jogos em 1996.
Um sistema multi-agente (MAS) é um sistema de computador composto de múltiplos agentes inteligentes que interagem entre si. Um MAS é capaz de resolver problemas que são difíceis de serem resolvidos por um único agente ou por um sistema monolítico. Este tipo de sistema geralmente combina técnicas de busca algorítmica, abordagens baseadas em regras, otimização clássica e até mesmo aprendizagem por reforço ou outras técnicas de aprendizagem de máquina.
Aprendizagem de Máquina é um campo de pesquisa dedicado à compreensão e construção de métodos computacionais que "aprendem", ou seja, métodos que utilizam informação/dados para melhorar o desempenho em algumas tarefas. Geralmente, ML é utilizado em problemas onde uma descrição precisa da solução seria muito desafiadora (por exemplo, visão computacional).
Um Algoritmo Genético (AG) é uma metaheurística inspirada pelo processo de seleção natural que pertence a uma classe de algoritmos conhecida como Algoritmos Evolucionários (AE). Algoritmos genéticos são comumente utilizados para gerar soluções para otimização e busca de problemas utilizando estratégias de inspiração biológica, tais como mutação, crossover, e seleção. Alguns exemplos da aplicação de AG incluem otimização em árvore de decisão, resolução de enigmas sudoku (um problema NP completo), otimização de hiperparâmetros, e etc.
Um problema P é um problema que pode ser resolvido em "tempo polinomial", o que significa que existe um algoritmo para sua solução de tal forma que o número de passos no algoritmo é limitado por uma função polinomial de n, onde n corresponde ao comprimento da entrada do problema (por exemplo, n^2, log(n)). NP (tempo polinomial executado por um algoritmo não determinístico) é uma classe de complexidade utilizada para classificar problemas de decisão que não podem ser resolvidos em tempo polinomial por uma Máquina de Turing determinística. NP é o conjunto de problemas de decisão para os quais as instâncias do problema têm provas verificáveis em tempo polinomial por uma Máquina de Turing determinística. Entretanto, tais problemas não podem ser resolvidos em tempo polinomial. O problema P versus NP é talvez o maior problema aberto na ciência da computação. Ele pergunta se, para cada problema cuja solução possa ser verificada rapidamente (em tempo polinomial), tal problema também pode ser resolvido rapidamente. 
Aprendizagem supervisionada é uma tarefa de aprendizagem de máquina que mapeia uma entrada para uma saída baseada em exemplos de pares de entrada e saída (um "sinal de supervisão"). Em aprendizagem supervisionada, utilizamos a distribuição de dados de treinamento para tentar inferir a "distribuição verdadeira" do fenômeno real através de gradiente descendente e minimização de risco empírico.
Aprendizagem não supervisionada é uma técnica de aprendizagem de máquina na qual o controlador não precisa supervisionar o modelo. Em vez disso, ela permite que o modelo trabalhe por conta própria para descobrir padrões e informações nos dados de treinamento. Aprendizagem não supervisionada é útil para classificar dados não rotulados e para aprender padrões.
A Teoria da Complexidade Computacional foca na classificação de problemas computacionais de acordo com sua utilização de recursos, e como tais classes (NL, P, NP, PSPACE, EXPTIME, EXPSPACE) se relacionam umas com as outras.
A Teoria da Informação Algorítmica é um ramo da ciência da computação preocupado com a relação entre computação e informação gerada por programas de computador, tais como sequências numéricas ou qualquer outra estrutura de dados. Na Teoria da Informação Algorítmica, a Complexidade de Kolmogorov ("Solomonoff-Kolmogorov-Chaitin complexity") de um objeto, como um texto, é o comprimento do programa de computador mais curto que produz o objeto como saída, sendo uma medida dos recursos computacionais necessários para especificar tal objeto.
Aprendizagem Semi Supervisionada é uma abordagem de aprendizagem de máquina que combina uma pequena quantidade de dados rotulados com uma grande quantidade de dados não rotulados durante o treinamento. Aprendizagem Semi Supervisionada situa-se entre a aprendizagem não supervisionada (sem dados de treinamento rotulados) e a aprendizagem supervisionada (com dados rotulados).
Aprendizagem por Reforço é uma técnica de aprendizagem de máquinas preocupada com a forma como agentes inteligentes devem agir em um ambiente para maximizar o retorno esperado de uma recompensa. Aprendizagem por Reforço é um dos três paradigmas básicos da aprendizagem de máquinas, juntamente com a aprendizagem supervisionada e a aprendizagem não supervisionada. O aprendizado por reforço difere do aprendizado supervisionado na medida em que não requer pares de entrada/saída rotulados. Em vez disso, o foco está em encontrar um equilíbrio entre exploração e exploração, um problema canonicamente representado pelo "problema do bandido multi armado."
Na teoria das probabilidades e na aprendizagem de máquina, o problema do Bandido Multi-Armado é um problema no qual um conjunto fixo e limitado de recursos deve ser alocado entre diferentes escolhas para maximizar seu ganho esperado, mesmo quando as propriedades de cada escolha são apenas parcialmente conhecidas no momento da alocação e podem tornar-se melhor compreendidas à medida que o tempo passa ou a alocação de recursos é alterada.
Engenharia de features, ou extração de características, é o processo de utilizar o conhecimento de domínio para extrair características de dados brutos. A motivação por trás desta técnica é utilizar estas características extras para melhorar a qualidade dos resultados de um processo de aprendizagem de máquina, em comparação com o simples fornecimento dos dados brutos para o processo de aprendizagem. A aprendizagem de características (ou aprendizagem de representações) é um conjunto de técnicas que permitem a um sistema descobrir automaticamente as representações necessárias para uma determinada tarefa. Uma das grandes vantagens que temos ao utilizar a aprendizagem profunda é que características podem ser aprendidas/extraídas de uma forma não supervisionada.
Online Learning (OL) é um método de aprendizagem de máquina em que os dados estão disponíveis em ordem sequencial e utilizados para atualizar um preditor em cada etapa do processo de melhoria, como oposto ao treinamento do preditor em todo o conjunto de dados de treinamento. OL é uma técnica comum utilizada em áreas onde é computacionalmente inviável treinar em todo o conjunto de dados. O aprendizado on-line também é usado em situações em que é necessário que o algoritmo se adapte dinamicamente a novos padrões nos dados, ou quando os próprios dados são gerados dinamicamente (e.g., algoritmos de recomendação). Os algoritmos de aprendizagem on-line são propensos ao problema de "esquecimento catastrófico".
O esquecimento catastrófico, ou interferência catastrófica, é um problema bem conhecido em aprendizagem de máquina, relacionado à tendência das redes neurais artificiais de esquecer completa e abruptamente informações aprendidas anteriormente ao aprender novas informações.
Análise de séries temporais é um subcampo da aprendizagem de máquina e Estatística cujo foco de interesse são dados temporais. Muitos tipos de problemas em aprendizagem de máquina requerem análise de séries temporais, como no caso de sistemas de previsão, geração de texto, reconhecimento de áudio, etc.
Aprendizagem profunda faz parte de uma família maior de métodos de aprendizagem de máquina. Podemos dizer que a aprendizagem profunda é uma técnica de aprendizagem de máquina para aprender representações de dados. "Deep" vem da idéia de que somos capazes de aprender várias "camadas" de representações a partir de nossos dados. Encontrar representações claras de estruturas de dados complexas e altamente dimensionais é o que a aprendizagem profunda faz.
A Lottery Ticket Hypothesis diz que: "Uma rede neural densa e aleatoriamente inicializada contém uma sub-rede que - quando treinada sozinha - pode corresponder à precisão do teste da rede original após o treinamento para, no máximo, o mesmo número de iterações de treinamento." Em termos comuns, isto significa que algumas redes têm "boas sub-redes" dentro delas, ou seja, algumas sub-redes são bilhetes premiados. 
A Manifold hypothesis diz que: "Muitos conjuntos de dados de alta dimensão que ocorrem no mundo real na verdade se encontram em faces de baixa dimensionalidade dentro de espaços de alta dimensionalidade". Em termos comuns, os dados que ocorrem naturalmente, embora altamente dimensionais, parecem ocupar apenas um pequeno subconjunto de dimensões neste espaço.
Uma rede neural convolucional (CNN) é uma classe de redes neurais artificiais comumente aplicada a problemas de visão computacional, inspirada pela forma como o processamento visual ocorre em certos tipos de animais. As CNNs têm propriedades interessantes, por exemplo, sendo capazes de preservar certos tipos de simetrias (e.g., as CNNs podem gerar saídas que são invariantes a deslocamentos translacionais de suas entradas). Para saber mais sobre este tipo de rede neural, vá para o Neural Network Zoo.
Visão Computacional é um campo científico interdisciplinar que trata de como computadores podem obter um entendimento de alto nível a partir de imagens ou vídeos digitais. Do ponto de vista da engenharia, procura-se compreender e automatizar tarefas que o sistema visual humano pode fazer (e.g., classificação, segmentação de imagens, etc.).
Processamento de linguagem natural é um subcampo da linguística, ciência da computação e inteligência artificial preocupado com as interações entre computadores e a linguagem humana. Mais especificamente, aborda o desafio da programação de computadores para processar e analisar linguagem natural. Os desafios em processamento de linguagem natural podem envolver a compreensão da linguagem natural, geração de linguagem natural, análise de texto e basicamente qualquer tarefa que possa ser expressa através da manipulação e análise de texto.
Redes Neurais Artificiais, frequentemente chamadas de redes neurais, são sistemas de computação vagamente inspirados por redes neurais biológicas que compõem cérebros de animais. Uma rede neural é baseada em uma coleção de unidades conectadas (neurônios), onde cada conexão, como as sinapses em um cérebro biológico, transmite informações entre outros neurônios. Tipicamente, os neurônios são agregados em camadas. Diferentes camadas podem realizar diferentes transformações em suas entradas. Se permitirmos que tais sistemas tenham um número arbitrário de unidades e camadas, o teorema da aproximação universal dita que estes sistemas, com o ajuste correto de seus parâmetros e hiperparâmetros, são capazes de "representar uma grande variedade de funções interessantes".
Uma Feedforward Neural Network (FNN) é uma rede neural sem conexões cíclicas ou recursivas, como uma RNN. Para saber mais sobre as FNNs, visite o Neural Network Zoo.
Retropropagação é o principal algoritmo para realizar a atualização de parâmetros em redes neurais. Primeiro, os valores de saída de cada neurônio são computados (e armazenados em cache) em uma única passagem para frente. Em seguida, a derivada parcial do erro relativo a cada parâmetro é calculada em uma passagem para trás através da rede. Ao iterar este processo, a direção onde o gradiente da função de perda diminui pode ser encontrada, e assim podem ser encontrados pontos convexos da paisagem de otimização (ou seja, pontos onde a perda é minimizada).
O Transformer é uma arquitetura de rede neural desenvolvida pela Google que utiliza mecanismos de atenção para transformar uma sequência de embeddings de entrada em uma sequência de embeddings de saída sem depender de convoluções ou redes neurais recorrentes. Podemos pensar em um transformador como uma pilha de módulos de atenção (e auto-atenção) ligados entre si por conexões residuais, camadas de FNN e normalização. Tais camadas podem ser compostas de dois tipos diferentes de blocos de transformadores: codificadores e decodificadores. Alguns transformadores consistem apenas de blocos codificadores (e.g., BERT), enquanto outros consistem apenas em blocos decodificadores (e.g., GPT).
Atenção é uma ampla gama de mecanismos de arquitetura de redes neurais que agregam informações de um conjunto de entradas. Os mecanismos de atenção e auto-atenção são os blocos de construção das redes transformer. A auto-atenção é um mecanismo de atenção que relaciona diferentes posições de uma única sequência (entrada) para calcular uma representação desta sequência (e.g., como diferentes palavras em uma frase se relacionam com a frase inteira).
Redes neurais recorrentes são uma classe de redes neurais artificiais onde as conexões entre neurônios formam um gráfico dirigido, ou não dirigido, sobre uma sequência temporal. Isto permite exibir um comportamento dinâmico temporal, o que as torna aplicáveis a tarefas como reconhecimento de escrita manual, reconhecimento de fala, previsão de séries temporais, etc.
Long Short-Term Memory (LSTM) é um tipo de RNN capaz de processar sequências de dados temporais (como fala ou vídeo). A unidade comum das LSTM é composta de uma célula, uma porta de entrada, uma porta de saída, e uma porta de esquecimento. A unidade pode lembrar valores em intervalos de tempo arbitrários, e os três portões regulam o fluxo de informações para dentro e para fora da célula. As LSTM foram desenvolvidas para lidar com o "problema do vanishing gradient" comumente encontrado no treinamento de RNNs. 
A aprendizagem auto-supervisionada é um tipo de aprendizagem de máquina onde um modelo aprende a identificar padrões em dados sem supervisão. Em vez de confiar em dados rotulados, que muitas vezes são caros e demorados para obter, a aprendizagem auto-supervisionada aproveita dados não etiquetados para treinar modelos. Há muitos exemplos de aprendizagem auto-supervisionada em diferentes domínios. Uma aplicação popular é no processamento de linguagem natural (PNL), onde os modelos são treinados para prever palavras mascaradas em uma frase com base no contexto restante.
Uma "Generative Adversarial Network" (GAN) é um tipo diferente de rede neural, sendo duas redes que trabalham em conjunto. As GANs consistem em quaisquer duas redes (embora frequentemente trate-se da combinação de uma FFN e uma CNN), onde uma supervisiona a geração do conteúdo, e a outra é responsável por julgar esse conteúdo. A rede discriminadora recebe dados de treinamento ou conteúdo gerado a partir da rede generativa. Como a rede discriminante pode prever corretamente se a fonte de dados é real ou artificial é então utilizada como parte do erro da rede geradora. Isto cria uma forma de competição onde o discriminador está se tornando melhor na distinção entre dados reais e dados gerados, e o gerador está aprendendo a se tornar um melhor gerador. No final deste processo, o discriminador é geralmente descartado, e acabamos com uma rede capaz de gerar dados extremamente verdadeiros (e.g., imagens falsas de rostos humanos).
Neural Turing Machines (NTM) podem ser entendidas como uma abstração das LSTMs, sendo uma tentativa de abrir a caixa preta. Em vez de codificar uma célula de memória diretamente em um neurônio, as NTMs têm células de memória separadas. Esta é uma tentativa de combinar a eficiência e a permanência do armazenamento digital regular com a eficiência e o poder expressivo das redes neurais. A ideia é ter um banco de memória endereçável ao conteúdo e uma rede neural que possa ler e escrever a partir dele. A "Turing" em NTMs vem do fato de que estas redes são Turing completas. Para saber mais sobre NTM, visite o Neural Network Zoo. 
Uma função de ativação (por exemplo, ReLU, GELU, etc.) toma a soma ponderada de todas as entradas da camada anterior e gera um valor de saída (normalmente não-linear) para a camada seguinte em uma rede neural.
O viés (math) é uma intercepção ou offset de uma origem. O viés é referido como b ou w0 em modelos de aprendizagem de máquina. O viés é um estereótipo ou favoritismo em relação a um grupo em relação a outros. Estes vieses podem afetar o comportamento do sistema de IA, e como os usuários interagem com tal sistema.
A justiça contrafactual é uma métrica de justiça que verifica se um classificador produz o mesmo resultado para um indivíduo, e para outro, que é idêntico ao primeiro, exceto no que diz respeito a um ou mais atributos sensíveis.
Estas são medidas para avaliar a equidade de um classificador treinado por aprendizagem de máquina (e.g., Counterfactual Fairness, Demographic Parity, Predictive Parity, Equalized Probabilities, entre outras).
A entropia-cruzada representa a diferença entre duas distribuições de probabilidade. Esta medida é geralmente utilizada para calcular a perda em problemas de classificação multiclasse. 
Data Augmentation é uma estratégia para aumentar o número de exemplos de treinamento, transformando as amostras existentes em "amostras artificiais".
Demographic Parity é uma métrica de imparcialidade que é satisfeita se os resultados da classificação de um modelo não dependem de um atributo sensível em particular. Por exemplo, "se tanto Ravenclaws como Hufflepuffs se aplicam a Hogwarts, a paridade demográfica é alcançada se a porcentagem de Ravenclaws admitidos for a mesma que a porcentagem de Hufflepuffs admitidos, independentemente de um grupo ser, em média, mais qualificado que o outro".
Em aprendizagem por reforço, o ambiente é aquilo que contém o agente, produzindo estados de mundo para que o agente interaja com ele. Por exemplo, o ambiente pode ser um tabuleiro de xadrez ou um labirinto. Quando o agente aplica uma ação ao ambiente, este ambiente transita para um novo estados de mundo.
Predictive Parity é uma métrica de equidade que verifica se, para um rótulo preferido (que confere uma vantagem ou benefício a uma pessoa) e um determinado atributo, um classificador atribui o rótulo preferido igualmente para todos os valores desse atributo.
Equalized Probabilities é uma métrica de equidade que verifica se, para qualquer rótulo e atributo em particular, um classificador prediz esse rótulo igualmente bem para todos os valores desse atributo (independentemente de esse rótulo ser um benefício ou um impacto).
O problema do "vanishing gradient" é encontrado no treinamento de redes neurais com métodos de aprendizagem baseados em descida de gradiente e retropropagação. Em tais métodos, durante cada iteração de treinamento, cada um dos pesos da rede neural recebe uma atualização proporcional à derivada parcial da função de erro relativa ao peso atual. O problema é que, em alguns casos, ao longo das iterações, o gradiente desaparece, ou se torna muito pequeno, impedindo efetivamente que o peso altere seu valor. No pior caso, isto pode impedir completamente a rede neural de continuar o treinamento. O problema do "exploding gradient", por outro lado, ocorre porque o gradiente dos modelos com arquiteturas profundas tende a tornar-se surpreendentemente íngreme. Gradientes íngremes resultam em atualizações muito grandes dos pesos de cada neurônio em uma rede neural profunda. Sem uma regulação cuidadosa do tamanho do passo do gradiente, esse pode acabar "explodindo" de uma região convexa da paisagem de otimização.  
Um Falso Negativo é um exemplo onde o modelo previu erroneamente a classe negativa. Falso Positivo é um exemplo em que o modelo previu erroneamente a classe positiva. Um Verdadeiro Positivo é um exemplo em que o modelo previu corretamente a classe positiva. Verdadeiro Negativo é um exemplo em que o modelo previu corretamente a classe negativa. Todas estas possibilidades são expressas em matrizes de confusão.
Federated Learning (FL) é uma abordagem distribuída de aprendizagem de máquinas que treina modelos de aprendizagem de máquina utilizando exemplos descentralizados residentes em dispositivos como os smartphones. Na aprendizagem federada, um subconjunto de dispositivos faz o download do modelo atual de um servidor de coordenação central. Os dispositivos usam os exemplos armazenados nos dispositivos para fazer melhorias no modelo. Os dispositivos então carregam as melhorias do modelo (mas não os exemplos de treinamento) para o servidor de coordenação, onde são agregados com outras atualizações para produzir um modelo global melhorado.
Os hiperparâmetros são os "botões" que você utiliza para controlar uma rede neural durante as sucessivas iterações de treinamento de um modelo (e.g., taxa de aprendizagem, n de neurônios, n de camadas, taxa de dropout, etc.). Enquanto isso, um parâmetro é uma variável que o modelo treina por si só, por exemplo, os pesos e os bias de um modelo de ML.
Interpretabilidade (XAI) é a capacidade de explicar ou apresentar o raciocínio de um modelo ML em termos compreensíveis para um humano.
Uma NaN Trap ocorre quando um parâmetro em seu modelo se torna um NaN (Not A Number) durante o treinamento, o que faz com que muitos (ou todos) outros parâmetros em seu modelo se tornem um NaN. 
Uma função objetiva (em um problema de otimização matemática) é uma função de valor real cujo valor deve ser minimizado ou maximizado sobre o conjunto de alternativas viáveis.
A perplexidade é uma medida de o quão bem uma distribuição de probabilidade ou um modelo probabilístico prevê uma amostra. Por exemplo, suponha que sua tarefa seja ler as primeiras letras de uma palavra que um usuário está digitando em seu smartphone e oferecer uma lista de possíveis palavras de preenchimento. Perplexidade (para esta tarefa) é aproximadamente o número de suposições que você precisa oferecer para que sua lista contenha a palavra real que o usuário está tentando digitar.
Um atributo sensível é um atributo humano que deve receber consideração especial por motivos legais, éticos, sociais ou pessoais (por exemplo, raça, gênero, orientação sexual, etc.).
Descendência de Gradiente Estocástico é um método iterativo para otimizar uma função objetiva com propriedades de suavidade adequadas (e.g., funções diferenciáveis). SGD é um algoritmo de otimização frequentemente utilizado em aplicações de aprendizagem de máquina para encontrar os parâmetros de um modelo que correspondem ao melhor ajuste entre as saídas previstas e a verdadeira distribuição dos dados.
O problema de alinhamento é na verdade dois problemas: alinhamento externo e Alinhamento interno. Alinhamento externo: "Assegurando que o objetivo base a ser otimizado está alinhado com as verdadeiras intenções e objetivos dos controladores". Alinhamento interno: "Garantir que o objetivo do otimizador base (e.g., SGD) é alinhado com a mesa-objetivo do modelo criado." De um ponto de vista ético/filosófico, este é o problema de como especificar valores humanos para os modelos ML. Para mais detalhes, leia "Risks from Learned Optimization in Advanced Machine Learning Systems'' para uma explicação mais detalhada.
O problema de controle é proposto a partir do seguinte argumento: "Sistemas de IA existentes podem ser monitorados e facilmente desligados e modificados se eles se comportarem mal. Entretanto, uma superinteligência mal programada, que por definição poderia se tornar mais inteligente que seus controladores, poderia vir a impedir seu desligamento (modificação)". O problema de controle pergunta: Que precauções prévias os programadores podem tomar para evitar que uma superinteligência se comporte de forma catastrófica?
O AI Boxing é um método de controle proposto no qual uma IA é executada em um sistema de computador isolado com canais de entrada e saída altamente restritos. Embora isto reduza a capacidade da IA de executar comportamentos indesejáveis, também reduz sua utilidade.
O basilisco de Roko é uma experiência de pensamento proposta em 2010 pelo usuário "Roko" na comunidade Less Wrong. Roko utilizou ideias na teoria da decisão para argumentar que um agente de IA suficientemente poderoso teria um incentivo para torturar qualquer pessoa que imaginasse o agente mas não trabalhasse para trazê-lo à existência. O argumento foi chamado de "basilisco" porque simplesmente ouvir o argumento supostamente colocaria você em risco de tortura por este agente hipotético (um basilisco neste contexto é qualquer informação que prejudique ou ponha em perigo as pessoas que venham a conhecer esta informação).
O Teorema do Acordo de Aumann diz, de grosso modo, que dois agentes agindo racionalmente (em um certo sentido preciso da palavra "racional") e com conhecimento comum das crenças um do outro, não podem concordar em discordar. Mais especificamente, se duas pessoas são genuínas Bayesianas, compartilham os mesmos princípios comuns e têm conhecimento comum das atribuições de probabilidade atuais um do outro, tais pessoas devem ter atribuições de probabilidade iguais.
A "teoria da decisão" é o estudo de princípios e algoritmos para tomar decisões corretas, ou seja, decisões que permitem que um agente alcance melhores resultados em relação a seus objetivos. Na Teoria da Decisão, cada ação, pelo menos implicitamente, representa uma decisão sob incerteza, ou seja, um estado de conhecimento parcial. Quais são os mecanismos subjacentes aos processos de decisão?. O que é isso? E como pode ser feito melhor? Estas, e muitas outras perguntas, são o foco de interesse da Teoria da Decisão.
Epidemiologia é o estudo de como conhecemos o mundo. É tanto um assunto em filosofia quanto uma preocupação prática com a forma como passamos a acreditar que as coisas são verdadeiras.
Teoria dos jogos é o estudo formal de como os atores racionais interagem para buscar incentivos. Nessa área, investiga-se situações de conflito e cooperação.
O Infra-Bayesianismo é uma nova abordagem em epistemologia / teoria da decisão / aprendizagem por reforço, que se baseia na ideia de "probabilidades imprecisas" para solucionar o problema do "grão da verdade" que atormenta o Bayesianismo e aprendizagem por reforço. O Infra-Bayesianismo também leva naturalmente à implementação da UDT (Updateless Decision Theory) e (mais especulativamente) aplicações à teoria multi-agente, agência incorporada e agência reflexiva.
Updateless Decision Theory (UDT) é uma teoria de decisão projetada para abordar um problema fundamental nas teorias de decisão existentes: "a necessidade de tratar o agente como uma parte do mundo em que ele toma suas decisões". 
O Problema de Newcomb é uma experiência de pensamento que explora os problemas envolvidos na interação com agentes que podem prever (total ou parcialmente) nossas ações. 
A navalha de Occam (mais formalmente referida como o princípio da parcimônia) é um princípio comumente dito como "Entidades não devem ser multiplicadas além da necessidade". Quando várias teorias são capazes de explicar as mesmas observações, a navalha de Occam sugere que a teoria mais simples é preferível. 
A indução de Solomonoff é um sistema de inferência definido por Ray Solomonoff. Um sistema que segue tal método de inferência aprenderá a prever corretamente qualquer sequência calculável com apenas a quantidade mínima absoluta de dados. Este sistema, em certo sentido, é o algoritmo de previsão universal "perfeito".
Uma função de utilidade atribui valores numéricos ("utilidade") a resultados, de modo que resultados com utilidades mais altas são sempre preferidos em relação a resultados com utilidades mais baixas. 
A “Goodhart's Law” afirma que quando se escolhe um proxy como o valor para otimizar uma determinada tarefa, dada pressão suficiente, o proxy não será mais uma boa medida de sucesso nessa tarefa. A Lei de Goodhart é de particular relevância para o problema do alinhamento.
Heurística e  viés são formas de raciocínio humano que nos diferem de um raciocinador ideal, devido a atalhos no raciocínio que nem sempre funcionam (heurística) e causam erros sistemáticos (vieses).
A Teoria do Processo Duplo postula dois tipos de processos no cérebro humano. Os dois processos consistem de um processo implícito, inconsciente (Sistema 1), e um processo explícito, consciente (Sistema 2).
Um zumbi filosófico ou (p-zumbi) é uma entidade hipotética que se parece e se comporta exatamente como um humano (frequentemente estipulado para ser átomo por átomo idêntico a um humano) mas não é consciente, ou seja, não possui qualia.
AIXI é um formalismo matemático para uma hipotética (super)inteligência, desenvolvido por Marcus Hutter. Entretanto, AIXI não é computável. Mesmo assim, AIXI ainda é considerado uma ilustração teórica valiosa, com aspectos positivos e negativos.
Coherent Extrapolated Volition (CEV) foi um termo desenvolvido por Eliezer Yudkowsky em trabalhos relacionados a Friendly AI. Significa que não seria suficiente programar explicitamente o que pensamos que nossos desejos e motivações são para uma IA, em vez disso deveríamos encontrar uma maneira de programar tal sistema de forma a agir de acordo com o melhor interesse de nossos valores idealizados.
Um agente corrigível não interfere no que intuitivamente veríamos como tentativas de "corrigir" o agente ou "corrigir" nossos erros em seu desenvolvimento. Um agente corrigível permitiria essas correções, embora o teorema da convergência instrumental dite o contrário. A corrigibilidade é uma propriedade importante na segurança e alinhamento da IA.
A agência incorporada é uma noção intuitiva de que a compreensão da teoria do agente racional deve levar em conta o fato de que os agentes que criamos ( nós mesmos) são partes do mundo, não separados dele. Isto contrasta com o atual modelo cartesiano (como a indução de Solomonoff), que pressupõe implicitamente uma separação entre o agente e o ambiente.
O alinhamento interno é o problema de garantir que os otimizadores de mesa-otimizadores (ou seja, quando o próprio modelo treinado é um otimizador) estejam alinhados com a função objetiva do otimizador base. Como exemplo, a evolução é uma força de otimização que "gerou" otimizadores (humanos) para atingir seus objetivos. No entanto, seres humanos não maximizam prioritariamente o sucesso reprodutivo (eles usam técnicas de controle de natalidade e preferem se divertir do que maximizar sucesso reprodutivo). Isto é um fracasso de alinhamento.
Convergência instrumental é a tendência hipotética da maioria dos agentes suficientemente inteligentes de perseguir uma série de objetivos instrumentais de forma independente de seus objetivos terminais.
Power Seeking Theorems:  "Quando as recompensas são distribuídas de forma IDD entre estados, é instrumentalmente convergente obter acesso a muitos estados finais". No contexto dos processos de decisão de Markov, está provado que certas simetrias ambientais são suficientes para que políticas ideais induzam a procura de poder, ou seja, o controle sobre o meio ambiente. Para mais informações, procure por "Optimal Farsighted Agents Tend to Seek Power".
A Tese de Ortogonalidade afirma que a inteligência artificial pode ter qualquer combinação de nível de inteligência e objetivos terminais, ou seja, sua "função de utilidade" e "inteligência" podem variar independentemente. Isto contrasta com a crença de que, por causa de sua inteligência, todas as formas de IA irão convergir no final para um conjunto comum de metas. Para saber mais, você pode ler "The Basic AI Drives".
A incerteza lógica é uma incerteza probabilística sobre as implicações das crenças. Teoria da Probabilidade geralmente assume omnisciência lógica, ou seja, conhecimento perfeito da lógica. Agentes realistas (Bayesianos não perfeitos ) não podem ser logicamente omniscientes.
A Mesa-Optimização é a situação que ocorre quando um modelo aprendido (tal como uma rede neural) é, em si mesmo, um otimizador. Um otimizador base (e.g., SGD) otimiza e cria um mesa-optimizador. Anteriormente, o trabalho sob este conceito era chamado de Inner Optimizers e Optimization Daemons.
O alinhamento externo , no contexto do Aprendizado de Máquina, é a extensão na qual a função objetiva especificada está alinhada com o objetivo pretendido por seus projetistas. Esta é uma noção intuitiva, em parte porque as próprias intenções humanas não são bem compreendidas. Isto é o que normalmente é discutido como o problema "alinhamento de valores".
O Maximizador de Clipes de Papel é uma inteligência artificial hipotética cuja função de utilidade valoriza algo que os humanos consideram quase que "sem valor", tal como maximizar o número de clipes de papel no universo. O Maximizador de Clipes de Papel é o experimento canônico de pensamento que procura mostrar como uma inteligência artificial genérica, mesmo concebida de forma competente e sem malícia, poderia, em última análise, destruir a humanidade.
Auto aperfeiçoamento recursivo refere-se a uma propriedade hipotética que AGI poderia vir a possuir, ou seja, a capacidade de auto aperfeiçoamento. Para mais informações, leia Large Language Models Can Self-Improve.
Uma Volta Traiçoeira é um evento hipotético onde um sistema avançado de IA "finge" ser alinhado devido a sua relativa fraqueza mas se volta contra seus controladores uma vez que atinge poder suficiente para perseguir seu verdadeiro objetivo sem risco. Para saber mais, leia "Catching Treacherous Turn: A Model of the Multilevel AI Boxing".
Humans Consulting HCH (HCH) é um acrônimo recursivo que descreve um cenário onde humanos podem consultar simulações de si mesmos para ajudar a responder perguntas. É um conceito utilizado na discussão relacionada à amplificação iterada e debate.
Amplificação iterada e destilada (também conhecida como IDA) é uma estratégia alternativa de treinamento que constrói progressivamente um sinal de treinamento para problemas difíceis de especificar, combinando soluções para sub-problemas mais fáceis. A Amplificação Iterada está intimamente relacionada à Iteração Especializada (ou seja, a metodologia utilizada para o treinamento Alpha Go). Para mais informações, consulte "Supervising strong learners by amplifying weak experts."
Iteração Especializada (ExIt) é um algoritmo de aprendizado por  reforço que decompõe o problema em tarefas de planejamento e generalização. O planejamento de novas políticas é realizado por um busca em árvores de Monte Carlo, enquanto uma rede neural profunda generaliza esses planos. Posteriormente, a busca de árvores é melhorada utilizando a política da rede neural para orientar a busca, aumentando a eficiência da próxima fase de busca. Para mais informações, veja "Thinking Fast and Slow with Deep Learning and Tree Search."
Medidas de impacto penalizam uma IA por afetar demais o meio ambiente. Para reduzir o risco representado por um sistema de IA, você pode querer fazer com que ele tente atingir seus objetivos com o menor impacto possível sobre o meio ambiente. As medidas de impacto são maneiras de medir o que é "impacto".
O aprendizado de valor é um método proposto para incorporar valores humanos em uma IA. A aprendizagem de valores envolve a criação de um aprendiz artificial cujas ações são guiadas por valores humanos aprendidos. Para mais informações, leia "The Value Learning Problem."
Em AI governance procura-se desenvolver métodos para assegurar que a sociedade se beneficie de nossa crescente adoção e utilização de tecnologias de IA. 
AI Risk é a análise dos riscos associados à construção de sistemas de IA. Os modelos de IA podem possuir vulnerabilidades complexas que criam riscos peculiares. Vulnerabilidades tais como "extração de modelos" (i.e., ataques destinados a duplicar um modelo de ML) e "envenenamento de dados" (i.e., ataques destinados a adulterar dados de treinamento) podem representar novos desafios para abordagens de segurança.
Takeoff refere-se ao processo no qual uma "Seed AI ", com um certo limite de capacidade, seria capaz de se aperfeiçoar para se tornar uma AGI. Há um debate sobre se, realisticamente, a velocidade de uma Takeoff é mais provável que seja lenta (gradual) ou rápida (abrupta). Para mais informações, leia Singularidade e problemas de coordenação: Lições pandêmicas a partir de 2020.
AI Timelines  refere-se à discussão de quanto tempo até que vários marcos no progresso da IA sejam alcançados, e.g., IA a nível humano, emular um cérebro humano, e outros. AI Timelines devem ser distinguidas das Takeoff, que lidam com a dinâmica do progresso da IA após o desenvolvimento de uma IA a nível humano ou de uma Seed AI. Para mais informações, leia Singularidade e problemas de coordenação: Lições pandêmicas a partir de 2020.
Transformative AI é um termo utilizado para se referir às tecnologias de IA que poderiam eventualmente precipitar uma transição comparável, por exemplo, com a revolução agrícola ou industrial. Isto é semelhante ao conceito de superinteligência ou AGI, mas sem menção ao "nível de inteligência ou generalidade" de tal sistema.
O termo Generative Pretrained Transformer (GPT) refere-se a uma família de grandes modelos de linguagem, baseados na arquitetura transformer, criados pelo OpenAI. 
Narrow AI é um termo utilizado para se referir a sistemas capazes de operar apenas em um domínio relativamente limitado, como xadrez ou condução, em vez de ser capaz de aprender uma ampla gama de tarefas como um humano ou uma AGI. Narrow vs Strong/General não é uma classificação binária perfeita, pois existem graus de generalidade, por exemplo, grandes modelos de linguagem têm um grande grau de generalidade sem ser tão geral quanto um humano.
Emulação Cerebral Completa é a simulação ou transferência das informações contidas dentro de um cérebro para um substrato computacional. Através de uma Emulação Cerebral Completa, teoricamente, criar-se-ia "inteligência de máquina genuína". Este conceito é frequentemente discutido no contexto da Filosofia da Mente. Para mais informações, veja "Whole Brain Emulation: A Roadmap."
Consequencialismo é uma família de teorias éticas que ditam que as pessoas devem escolher suas ações com base nos resultados que esperam obter. Vários tipos de consequencialismo especificam como os resultados devem ser julgados. O Consequencialismo é uma das três principais vertentes do pensamento ético, juntamente com a deontologia e a ética da virtude.
A Deontologia é uma família de teorias éticas que dita que as pessoas devem escolher suas ações com base em uma lista prescrita de normas morais e é uma teoria de moralidade baseada na obediência às regras morais.
A Ética das Virtudes é uma classe de teorias éticas que tratam o conceito de virtude moral como central para a ética. Ética das Virtudes é geralmente contrastada com duas outras abordagens principais na ética normativa, o consequencialismo e a deontologia. Ética das Virtudes define o comportamento moralmente correto como aquele em que se demonstram virtudes como bravura, lealdade, ou sabedoria.
Metaética é um campo de estudo que tenta compreender as características metafísicas, epistemológicas e semânticas, assim como os fundamentos e o alcance dos valores morais . Na metaética, os filósofos se preocupam com questões e problemas como "Os juízos morais são objetivos ou subjetivos, relativos ou absolutos?", "Há fatos morais?" ou "Como aprendemos valores morais?".
Incerteza moral (ou incerteza normativa) é a incerteza sobre o que devemos fazer, moralmente, dada a diversidade das doutrinas morais. 
Um risco existencial é um risco que apresenta consequências negativas astronomicamente grandes para a humanidade, tais como a extinção humana, ou totalitarismo global permanente.
A complexidade de valores é a tese de que os valores humanos têm alta complexidade de Kolmogorov, ou seja, que preferências humanas não podem ser resumidas há regras simples, ou comprimidas em um algoritmo menor do que a descrição completa de tais valores. A fragilidade de valor é a tese de que perder mesmo uma pequena parte das regras que compõem nossos valores poderia levar a resultados que a maioria de nós consideraria inaceitáveis. Para mais informações, leia "Complex Value Systems are Required to Realize Valuable Futures".
A Ética da IA é o ramo da ética específico para as preocupações relacionadas aos sistemas de IA. A Ética da IA é às vezes dividida entre preocupações relacionadas ao comportamento moral dos humanos ao projetar, fazer e usar sistemas de IA, e preocupações relacionadas ao comportamento das máquinas, ou seja, máquinas agindo "moralmente".
AI Safety é uma área de pesquisa em aprendizagem de máquina que visa identificar as causas do comportamento não intencional dos sistemas criados pela aprendizagem de máquina, onde busca-se desenvolver ferramentas para garantir que tais sistemas funcionem de forma segura e confiável.
Robustez diz respeito a criação de modelos que são resistentes a ataques adversariais e situações incomuns. Os modelos treinados pela aprendizagem de máquina ainda são frágeis e rígidos, não operando bem em ambientes dinâmicos e mutáveis.
Monitoramento diz respeito a detecção de uso malicioso, mau funcionamento ou funcionalidade não intencional que pode estar presente nos modelos de ML.
Segurança Externa tem a ver com o fato de que modelos podem ser embutidos em ambientes inseguros, tais como software mal projetado e organizações mal estruturadas. 
Para dados estatísticos sobre Ética da IA, visite o painel do estudo "Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance" Para mais informações, leia o artigo completo.
Aqui está uma lista de princípios em Ética da IA: Beneficência, Confiabilidade, Direitos da Criança e do Adolescente, Direitos Humanos, Direitos do Trabalho, Diversidade, Formação Humana, Centralidade Humana, Propriedade Intelectual, Justiça, Liberdade, Cooperação, Privacidade, Responsabilidade, Sustentabilidade, Transparência, e Veracidade. Para mais informações, visite o painel do estudo "Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance". 
Responsabilização refere-se à ideia de que os desenvolvedores e comerciantes de tecnologias de IA devem estar em conformidade com os órgãos reguladores, o que também significa que tais atores devem ser responsáveis por suas ações e pelos impactos causados por suas tecnologias.
Beneficência e não-maleficência são conceitos que vêm da bioética e da ética médica, e na ética da IA, eles afirmam que o bem-estar humano (e a aversão aos danos) deve ser o objetivo das tecnologias impulsionadas pela IA. 
No contexto da Ética da IA, tal ideia defende que os direitos das crianças e adolescentes devem ser respeitados pelas tecnologias que utilizam IA. As partes interessadas na questão da IA devem salvaguardar, respeitar e estar conscientes das fragilidades associadas aos jovens.
Dignidade é um princípio baseado na ideia de que todos os indivíduos merecem tratamento adequado e respeito. Na Ética da IA, o respeito à dignidade humana está frequentemente ligado aos direitos humanos (i.e., "Declaração Universal dos Direitos Humanos").
Diversidade defende a ideia de que o desenvolvimento e o uso de tecnologias de IA deve ser feito de forma inclusiva e acessível, respeitando as diferentes formas que a entidade humana pode vir a se expressar. 
A autonomia da decisão humana deve ser preservada durante as interações humano-IA, seja essa escolha individual ou a liberdade de escolher em conjunto, como a inviolabilidade dos direitos e valores democráticos, estando também ligada à auto-suficiência tecnológica das nações/estados.
A formação humana e a educação são princípios baseados na ideia de que a formação e a educação humana devem ser priorizadas em nossos avanços tecnológicos. As tecnologias de IA exigem um nível considerável de especialização para serem produzidas e operadas, e tal conhecimento deve ser acessível a todos.
A centralidade humana é um princípio baseado na ideia de que sistemas de IA devem ser centrados e alinhados com os valores humanos. As tecnologias de IA devem ser adaptadas para se alinharem aos nossos valores (e.g., design sensível à valores).
Propriedade Intelectual procura fundamentar os direitos de propriedade sobre produtos e/ou processos de conhecimento gerados por indivíduos, sejam eles tangíveis ou intangíveis.
Equidade sustenta a idéia de não-discriminação e mitigação de preconceitos (sistemas de IA podem estar sujeitos a preconceitos algorítmicos discriminatórios). O princípio de equidade defende a ideia de que, independentemente dos diferentes atributos sensíveis que possam caracterizar um indivíduo, todos devem ser tratados "de forma justa".
Os direitos trabalhistas são direitos legais e humanos relacionados às relações de trabalho entre trabalhadores e empregadores. Na Ética da AI, este princípio enfatiza que os direitos dos trabalhadores devem ser preservados, independentemente de as relações de trabalho estarem sendo mediadas por tecnologias que utilizam IA ou não. 
Cooperação defende diferentes meios pelos quais as ações conjuntas podem ser estabelecidas e cultivadas entre as partes interessadas na IA para alcançar objetivos comuns. Ela também defende a troca livre e aberta de ativos valiosos de IA para mitigar possíveis monopólios de IA.
A privacidade pode ser definida como o direito do indivíduo de "expor-se voluntariamente, e na medida do desejado, ao mundo". Na Ética da IA, este princípio sustenta o direito de uma pessoa de controlar a exposição e disponibilidade de informações pessoais quando mineradas como dados para o treinamento de sistemas de IA.
Confiabilidade é a ideia de que as tecnologias de IA devem ser confiáveis, no sentido de que seu uso pode ser comprovadamente atestado como seguro e robusto, promovendo a confiança do usuário e uma melhor aceitação das tecnologias de IA.
Sustentabilidade pode ser entendida como uma forma de "justiça intergeracional", onde o bem-estar das gerações futuras também deve ser contado durante o desenvolvimento da IA. Na Ética da IA, a sustentabilidade refere-se à ideia de que o desenvolvimento de tecnologias de IA deve ser realizado com consciência de suas implicações a longo prazo, tais como custos ambientais e preservação da vida não-humana/bem estar.
Transparência apóia a ideia de que o uso e desenvolvimento de tecnologias de IA deve ser feito de forma transparente para todas as partes interessadas. A transparência pode estar relacionada com "a transparência de uma organização" ou "a transparência de um algoritmo".
A veracidade defende a ideia de que as tecnologias de IA devem fornecer informações verdadeiras. Está também relacionada à ideia de que as pessoas não devem ser enganadas quando interagem com sistemas de IA.
Eu não tenho relógio... mas aposto que seu computador tem!
Eu não sou um oráculo, pergunte ao Google! Ele é bom nisso. Eu sou apenas um chatbot de domínio fechado, não um motor de busca.
Espero ter ajudado você, volte sempre! 
Você está sendo mal-educado...
Ética ou Filosofia moral é um ramo da filosofia que, junto com a Estética, compreende o que chamamos de Axiologia (ou Teoria do Valor). Em Ética, procuramos sistematizar, defender e recomendar conceitos de comportamento certo e errado. A Ética Ocidental pode ser dividida em três grandes escolas: Conseqüencialismo, Deontologia, e Ética das Virtudes.
Podemos definir a palavra artificial como algo feito por humanos, em vez de algo que ocorre naturalmente, como uma cópia de algo natural (e.g., luz artificial em vez de luz solar).