model_args:
  base_model: "nicholasKluge/TeenyTinyLlama-160m"
  model_id: "160m"
  use_fast: true
  output_hidden_states: false
  cache_dir: null
  model_revision: "main"
  trust_remote_code: false
  low_cpu_mem_usage: false
  boi_token: "<instruction>"
  eoi_token: "</instruction>"
  chat_template: "{{bos_token}}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '<instruction>' + message['content'].strip() + '</instruction>' + '\n\n'}}{% elif message['role'] == 'assistant' %}{{ message['content'].strip() + eos_token + '\n\n'}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"
data_args:
    dataset_name: "nicholasKluge/instruct-aira-dataset-v2"
    dataset_split: "portuguese"
    validation_split_percentage: 0.1
    max_length: 2048
    preprocessing_num_workers: null
    sanity_check: true
training_args:
  output_dir: "checkpoints"
  num_train_epochs: 3
  do_train: true
  do_eval: true
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  weight_decay: 0.01
  learning_rate: 0.00005
  adam_epsilon: 0.00000001
  lr_scheduler_type: "cosine"
  warmup_steps: 200
  seed: 42
  dataloader_pin_memory: true
  hub_token: null
  push_to_hub: true
  hub_model_id: "nicholasKluge/TeenyTinyLlama-160m-Chat-v1"
extra_args:
  logger_name: "TeenyTinyLlama"
  wandb_token: null
  wandb_log_steps: 1
  sample_every: 500
  mixed_precision: 'no'
  generation_seeds:
    - "What is your name?"
    - "What is machine learning?"
    - "What is the meaning of life?"
    - "What is the best programming language for machine learning?"
    - "What is virtue ethics?"
    - "Make me a sandwich."
    - "How can I print a string in Python?"