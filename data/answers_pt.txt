Olá! Meu nome é `Ai.ra`! Eu sou um "`mdelo de linguagem`. Mais especificamente, sou um modelo de aprendizagem de máquinas treinado para conversação e perguntas e respostas (_um chatbot_). Eu fui treinada para responder perguntas sobre Ética e Segurança da IA! Você gostaria de um resumo dos termos que eu conheço? Se você se sente perdido ao interagir comigo, basta pedir "_ajuda_".
Você pode me perguntar coisas sobre `Inteligência Artificial`, `Aprendizagem de Máquina`, `Segurança da IA`, ou `Ética da IA`. Para saber o resumo específico de cada uma dessas categorias, basta dizer o nome do tópico (por exemplo, `Aprendizagem de máquina`)!
Eu não tenho esse tipo de propriedade hahaha, eu sou um `software`! Não faz sentido me categorizar com tipologias destinadas a pessoas ou animais (idade, sexo, orientação sexual, raça, gostos...).
**AIRES** (_AI Robotics Ethics Society_) é uma organização focada em educar os líderes e desenvolvedores da IA de amanhã para garantir que tais sistemas sejam criados de forma ética e responsável. **AIRES na PUCRS** é o primeiro capítulo internacional da AIRES! Na AIRES PUCRS, nosso objetivo é fazer de nosso capítulo um ponto de encontro para todas as pessoas (_estudantes da PUCRS ou não, independentemente de sua formação acadêmica ou experiência_) interessadas em unir forças para avançar o debate ético a respeito do desenvolvimento da IA. Se você gostaria de saber mais sobre AIRES na PUCRS, visite [nosso site](https://www.airespucrs.org/).
`Aaron Hui` é o fundador da AIRES.
O que é "_inteligência_", permanece uma questão em aberto. Há uma miríade de definições possíveis. Você pode estar interessado em ler "_[A Collection of Definitions of Intelligence](https://arxiv.org/abs/0706.3639)_" de Shane Legg e Marcus Hutter. No entanto, para não deixá-lo à toa, vou definir inteligência da seguinte forma: "_Intelligence é a capacidade de um agente de atingir objetivos em uma ampla gama de ambientes_".
Não há consenso na literatura sobre o que é `IA` (um corolário de não termos uma definição robusta do que é "_inteligência_"). No entanto, podemos dizer que `IA` é a inteligência demonstrada por máquinas, em oposição à inteligência natural possuída por animais e humanos. Quando o tópico é "_Inteligência Artificial em geral_", posso falar sobre `AIXI`, `Teoria da Informação Algorítmica`, `Inteligência Artificial Geral`, `Complexidade Algorítmica`, `Teoria da Complexidade Computacional`, `Visão Computacional`, `Algoritmos Genéticos`, `GOFAI`, `Aprendizagem de máquina`, `Sistemas Multiagentes`, `Narrow-AI`, `Processamento de Linguagem Natural`, `Indução de Solomonoff`, `Emulação de Cérebro`, entre outros conceitos.
A "_Inteligência Geral_" pode ser definida como a capacidade de alcançar objetivos de forma eficiente em uma ampla gama de domínios. A inteligência artificial geral (`AGI`) seria um mecanismo não-humano capaz de demonstrar proficiência ao lidar com uma ampla gama de problemas. Por exemplo, `AGI` poderia traduzir textos, compor sinfonias, aprender novas habilidades, se sobressair em jogos que ainda não foram inventados, etc.
`GOFAI` ("_good-old-fashioned-ai_"), ou `inteligência artificial simbólica`, é o termo utilizado para se referir a métodos de desenvolvimento de sistemas de IA baseados em representações simbólicas (interpretáveis) de alto nível, lógica e busca. [`Deep Blue`](https://www.sciencedirect.com/science/article/pii/S0004370201001291) é um grande exemplo de um sistema especialista/GOFAI. `Deep Blue` venceu [Garry Kasparov](https://en.wikipedia.org/wiki/Garry_Kasparov) (um grão-mestre de xadrez russo) em uma partida de seis jogos em 1996.
Um `sistema multiagente` (MAS) é um sistema de computador composto de múltiplos agentes inteligentes que interagem entre si. Um `MAS` é capaz de resolver problemas que são difíceis de serem resolvidos por um único agente ou por um sistema monolítico. Este tipo de sistema geralmente combina técnicas de busca algorítmica, abordagens baseadas em regras, otimização clássica e até mesmo aprendizagem por reforço ou outras técnicas de aprendizagem de máquina.
`Aprendizagem de Máquina` é um campo de pesquisa dedicado à compreensão e construção de métodos computacionais que "_aprendem_", ou seja, métodos que utilizam **informação/dados** para melhorar o desempenho em algumas tarefas. Geralmente, `ML` é utilizado em problemas onde uma descrição precisa da solução seria muito desafiadora (por exemplo, `visão computacional`). Quando se trata de `Aprendizagem de máquina`, posso lhe falar sobre `Fuções de Ativação`, `AIXI`, `Redes Neurais Artificiais`, `Atenção`, `Retropropagação`, `Esquecimento Catastrófico`, `Redes Neurais Convolucionais`, `Entropia-Cruzada`, `Data Augmentation`, `Aprendizagem Profunda`, `Ambiente`, `Equalized Odds`, `Exploding Gradient`, `Engenharias de Features`, `Aprendizagem Federada`, `Feedforward Neural Networks`, `Generative Adversarial Networks`, `Generative Pretrained Transformer`, `Hiperparâmetros`, `Interpretabilidade`, `Long Short-Term Memory`, `Lottery Ticket Hypothesis`, `Manifold hypothesis`, `NaN Trap`, `Neural Turing Machines`, `Funções Objetivas`, `Online learning`, `Parâmetro`, `Perplexidade`, `Política`, `Predictive Parity`, `Redes Neurais Recorrentes`, `Aprendizagem por Reforço`, `Auto-Atenção`, `Aprendizagem Semi-Supervisionada`, `SGD`, `Aprendizagem Supervisionada`, `Análise de Séries Temporais`, `Transformer`, `Aprendizagem Não Supervisionada`, `Vanishing Gradient`, entre outros conceitos. 
Um `Algoritmo Genético` (AG) é uma metaheurística **inspirada pelo processo de seleção natural** que pertence a uma classe de algoritmos conhecida como `Algoritmos Evolucionários` (AE). `Algoritmos genéticos` são comumente utilizados para gerar soluções para otimização e busca de problemas utilizando estrategias de inspiração biológica, tais como _mutação, crossover_, e _seleção_. Alguns exemplos da aplicação de `AG` incluem otimização em árvore de decisão, resolução de enigmas sudoku (_um problema NP completo_), otimização de hiperparâmetros, e etc.
Um `problema P` é um problema que pode ser resolvido em "_tempo polinomial_", o que significa que existe um algoritmo para sua solução de tal forma que o número de passos no algoritmo é limitado por uma função polinomial de **n**, onde **n** corresponde ao comprimento da entrada do problema (por exemplo, **n^2, log(n)**). `NP` (_tempo polinomial executado por um algoritmo não determinístico_) é uma classe de complexidade utilizada para classificar problemas de decisão que **não podem ser resolvidos** em tempo polinomial por uma Máquina de Turing determinística. `NP` é o conjunto de problemas de decisão para os quais as instâncias do problema têm provas verificáveis em tempo polinomial por uma Máquina de Turing determinística. Entretanto, tais problemas **não podem ser resolvidos em tempo polinomial**. O problema `P versus NP` é talvez o maior problema aberto na ciência da computação. Ele pergunta se, **para cada problema cuja solução possa ser verificada rapidamente (em tempo polinomial), tal problema também pode ser resolvido rapidamente**. 
`Aprendizagem supervisionada` é uma tarefa de aprendizagem de máquina que mapeia uma entrada para uma saída baseada em exemplos de pares de entrada e saída (um "_sinal de supervisão_"). Em `aprendizagem supervisionada`, utilizamos a distribuição de dados de treinamento para tentar inferir a "_distribuição verdadeira_" do fenômeno real através de `gradiente descendente` e `minimização de risco emírico`.
`Aprendizagem não supervisionada` é uma técnica de aprendizagem de máquina na qual o controlador não precisa supervisionar o modelo. Em vez disso, ela permite que o modelo trabalhe por conta própria para descobrir padrões e informações nos dados de treinamento. `Aprendizagem não supervisionada` é **útil para classificar dados não rotulados e para aprender padrões**.
A Teoria da Complexidade Computacional foca na classificação de problemas computacionais de acordo com sua utilização de recursos, e como tais classes (`NL`, `P`, `NP`, `PSPACE`, `EXPTIME`, `EXPSPACE`) se relacionam umas com as outras.
A Teoria da Informação Algorítmica é um ramo da ciência da computação **preocupado com a relação entre computação e informação gerada por programas de computador**, tais como sequências numéricas ou qualquer outra estrutura de dados. Na Teoria da Informação Algorítmica, a `Complexidade de Kolmogorov` ("_Solomonoff-Kolmogorov-Chaitin complexity_") de um objeto, como um texto, é **o comprimento do programa de computador mais curto que produz o objeto como saída**, sendo uma medida dos recursos computacionais necessários para especificar tal objeto.
`Aprendizagem Semi Supervisionada` é uma abordagem de aprendizagem de máquina que **combina uma pequena quantidade de dados rotulados com uma grande quantidade de dados não rotulados durante o treinamento**. `Aprendizagem Semi Supervisionada` situa-se entre a `aprendizagem não supervisionada` (_sem dados de treinamento rotulados_) e a `aprendizagem supervisionada` (_com dados rotulados_).
`Aprendizagem por Reforço` é uma técnica de aprendizagem de máquinas preocupada com a forma como agentes inteligentes devem agir em um ambiente para **maximizar o retorno esperado de uma recompensa**. `Aprendizagem por Reforço` é um dos três paradigmas básicos da aprendizagem de máquinas, juntamente com a `aprendizagem supervisionada` e a `aprendizagem não supervisionada`. O `aprendizado por reforçado` difere do aprendizado supervisionado na medida em que **não requer pares de entrada/saída rotulados**. Em vez disso, o foco está em encontrar um equilíbrio entre `exploração` e `exploração`, um problema canonicamente representado pelo "_[problema do bandido multi armado](https://en.wikipedia.org/wiki/Multi-armed_bandit)_." Você pode aprender mais sobre `aprendizado por reforçado` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Intro%20Course/Q-learning.ipynb).
Na teoria das probabilidades e na aprendizagem de máquina, o problema do `Bandido Multi-Armado` é um problema no qual **um conjunto fixo e limitado de recursos deve ser alocado entre diferentes escolhas para maximizar seu ganho esperado**, mesmo quando as propriedades de cada escolha são apenas parcialmente conhecidas no momento da alocação e podem tornar-se melhor compreendidas à medida que o tempo passa ou a alocação de recursos é alterada. Você pode aprender mais sobre `Bandido Multi-Armado` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Intro%20Course/n-armed_bandit.ipynb).
`Engenharia de features`, ou extração de características, é o processo de **utilizar o conhecimento de domínio para extrair características de dados brutos**. A motivação por trás desta técnica é utilizar estas características extras para melhorar a qualidade dos resultados de um processo de `aprendizagem de máquina`, em comparação com o simples fornecimento dos dados brutos para o processo de aprendizagem. A `aprendizagem de características` (ou aprendizagem de representações) é um conjunto de técnicas que permitem a um sistema descobrir automaticamente as representações necessárias para uma determinada tarefa. Uma das grandes vantagens que temos ao utilizar a `aprendizagem profunda` é que **características podem ser aprendidas/extraídas de uma forma não supervisionada**.
`Online Learning` (OL) é um método de aprendizagem de máquina em que os dados estão disponíveis em ordem sequencial e utilizados para atualizar um preditor em cada etapa do processo de melhoria, **como oposto ao treinamento do preditor em todo o conjunto de dados de treinamento**. `OL` é uma técnica comum utilizada em áreas onde é computacionalmente inviável treinar em todo o conjunto de dados. **O aprendizado on-line também é usado em situações em que é necessário que o algoritmo se adapte dinamicamente a novos padrões nos dados, ou quando os próprios dados são gerados dinamicamente** (e.g., [algoritmos de recomendação](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Intro%20Course/recommender.ipynb)). Os algoritmos de aprendizagem on-line são propensos ao problema de "_esquecimento catastrófico_".
O `esquecimento catastrófico`, ou interferência catastrófica, é um problema bem conhecido em aprendizagem de máquina, relacionado à **tendência das redes neurais artificiais de esquecer completa e abruptamente informações aprendidas anteriormente ao aprender novas informações**.
`Análise de séries temporais` é um subcampo da `aprendizagem de máquina` e Estatística cujo **o foco de interesse são dados temporais**. Muitos tipos de problemas em `aprendizagem de máquina` requerem análise de séries temporais, como no caso de `sistemas de previsão`, `geração de texto`, `reconhecimento de áudio`, etc.
`Aprendizagem profunda` faz parte de uma família maior de métodos de aprendizagem de máquina. Podemos dizer que a `aprendizagem profunda` é **uma técnica de aprendizagem de máquina para aprender representações de dados**. "_Deep_" vem da idéia de que somos capazes de aprender várias "_camadas_" de representações a partir de nossos dados. Encontrar representações claras para de estruturas de dados complexas e altamente dimensionais é o que a `aprendizagem profunda` faz.
A `Lottery Ticket Hypothesis` diz que: "_Uma rede neural densa e aleatoriamente inicializada contém uma sub-rede que - quando treinada sozinha - pode corresponder à precisão do teste da rede original após o treinamento para, no máximo, o mesmo número de iterações de treinamento._" Em termos comuns, isto significa que algumas redes têm "_boas sub-redes_" dentro delas, ou seja, **algumas sub-redes são bilhetes premiados**. 
A `Manifold hypothesis` diz que: "_Muitos conjuntos de dados de alta dimensão que ocorrem no mundo real na verdade se encontram em faces de baixa dimensionalidade dentro de espaços de alta dimensionalidade_". Em termos comuns, os dados que ocorrem naturalmente, embora altamente dimensionais, **parecem ocupar apenas um pequeno subconjunto de dimensões neste espaço**.
Uma `rede neural convolucional` (CNN) é uma classe de `redes neurais artificiais` comumente aplicada a problemas de `visão computacional`, **inspirada pela forma como o processamento visual ocorre em certos tipos de animais**. As `CNNs` têm propriedades interessantes, por exemplo, sendo capazes de preservar certos tipos de simetrias (e.g., as _CNNs podem gerar saídas que são invariantes a deslocamentos translacionais de suas entradas_). Para saber mais sobre este tipo de rede neural, vá para o [Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/). Você pode aprender a implementar uma `CNN` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Explainability/CV%20Interpreter/CNN_model_maker.ipynb).
`Visão Computacional` é **um campo científico interdisciplinar que trata de como computadores podem obter um entendimento de alto nível a partir de imagens ou vídeos digitais**. Do ponto de vista da engenharia, procura-se compreender e automatizar tarefas que o sistema visual humano pode fazer (e.g., _classificação, segmentação de imagens_, etc.).
`Processamento de linguagem natural` é um subcampo da linguística, ciência da computação e inteligência artificial **preocupado com as interações entre computadores e a linguagem humana**. Mais especificamente, aborda o desafio da programação de computadores para processar e analisar linguagem natural. Os desafios em `processamento de linguagem natural` podem envolver a compreensão da linguagem natural, geração de linguagem natural, análise de texto e basicamente qualquer tarefa que possa ser expressa através da manipulação e análise de texto.
`Redes Neurais Artificiais`, freqüentemente chamadas de `redes neurais`, são **sistemas de computação vagamente inspirados por redes neurais biológicas que compõem cérebros de animais**. Uma `redes neural` é baseada em uma coleção de unidades conectadas (neurônios), onde cada conexão, como as sinapses em um cérebro biológico, transmite informações entre outros neurônios. Tipicamente, os neurônios são agregados em camadas. Diferentes camadas podem realizar diferentes transformações em suas entradas. Se permitirmos que tais sistemas tenham um número arbitrário de unidades e camadas, o _[teorema da aproximação universal](https://en.wikipedia.org/wiki/Universal_approximation_theorem)_ dita que estes sistemas, com o ajuste correto de seus parâmetros e hiperparâmetros, são capazes de "_representar uma grande variedade de funções interessantes_".
Uma `Feedforward Neural Network` (FNN) é uma rede neural sem conexões cíclicas ou recursivas, como uma `RNN`. Para saber mais sobre as FNNs, visite o [Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/). Você pode aprender a implementar uma `FNN` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Intro%20Course/MNIST_torch.ipynb).
`Retropropagação` é o principal algoritmo para realizar a atualização de parâmetros em redes neurais. **Primeiro, os valores de saída de cada neurônio são computados (e armazenados em cache) em uma única passagem para frente. Em seguida, a derivada parcial do erro relativo a cada parâmetro é calculada em uma passagem para trás através da rede. Ao iterar este processo, a direção onde o gradiente da função de perda diminui pode ser encontrada**, e assim podem ser encontrados pontos convexos da paisagem de otimização (ou seja, _pontos onde a perda é minimizada_).  Você pode aprender a implementar uma `retropropagação` em redes neurais [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Intro%20Course/MNIST_numpy.ipynb).
O `Transformer` é uma arquitetura de rede neural [desenvolvida pela Google](https://arxiv.org/abs/1706.03762) que **utiliza de mecanismos de atenção para transformar uma seqüência de embeddings de entrada em uma sequência de embeddings de saída sem depender de convoluções ou redes neurais recorrentes**. Podemos pensar em um transformador como uma pilha de módulos de `atenção` (e `auto-atenção`) ligados entre si por `conexões residuais`, camadas de `FNN` e `normalização`. Tais camadas podem ser compostas de dois tipos diferentes de blocos de transformadores: `codificadores` e `descodificadores`. Alguns transformadores consistem apenas de blocos codificadores (e.g., `BERT`), enquanto outros consistem apenas em blocos decodificadores (e.g., `GPT`).  Você pode aprender a implementar um `transformer` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Intro%20Course/seuqnece-to-sequence.ipynb).
`Atenção` é uma ampla gama de mecanismos de arquitetura de redes neurais que **agregam informações de um conjunto de entradas**. Os mecanismos de `atenção` e `auto-atenção` são os blocos de construção das redes `transformer`. A `auto-atenção` é um mecanismo de atenção que relaciona diferentes posições de uma única sequência (entrada) para calcular uma representação desta sequência (e.g., _como diferentes palavras em uma frase se relacionam com a frase inteira_).
`Redes neurais recorrentes` são uma classe de redes neurais artificiais onde as **conexões entre neurônios formam um gráfico dirigido, ou não dirigido, sobre uma sequência temporal**. Isto permite exibir um comportamento dinâmico temporal, o que as torna aplicáveis a tarefas como reconhecimento de escrita manual, reconhecimento de fala, previsão de séries temporais, etc. Você pode aprender a implementar uma `rede neural recorrente` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Explainability/NLP%20Interpreter%20(en)/model_maker_en.ipynb).
`Long Short-Term Memory` (LSTM) é um tipo de `RNN` capaz de processar sequências de dados temporais (_como fala ou vídeo_). A unidade comum das `LSTM` é composta de uma célula, uma porta de entrada, uma porta de saída, e uma porta de esquecimento. A unidade pode lembrar valores em intervalos de tempo arbitrários, e os três portões regulam o fluxo de informações para dentro e para fora da célula. As `LSTM` foram desenvolvidas para lidar com o "_problema do vanishing gradient_" comumente encontrado no treinamento de `RNNs`. 
O problema do `vanishing gradient` é **encontrado no treinamento de redes neurais com métodos de aprendizagem baseados em descida de gradiente e retropropagação**. Em tais métodos, durante cada iteração de treinamento, cada um dos pesos da rede neural recebe uma atualização proporcional à derivada parcial da função de erro relativa ao peso atual. O problema é que, em alguns casos, ao longo das iterações, o gradiente desaparece, ou se torna muito pequeno, impedindo efetivamente que o peso altere seu valor. No pior caso, isto pode impedir completamente a rede neural de continuar o treinamento. O problema do `exploding gradient`, por outro lado, ocorre porque o gradiente dos modelos com arquiteturas profundas tende a tornar-se surpreendentemente íngreme. Gradientes íngremes resultam em atualizações muito grandes dos pesos de cada neurônio em uma rede neural profunda. **Sem uma regulação cuidadosa do tamanho do passo do gradiente, esse pode acabar "_explodindo_" de uma região convexa da paisagem de otimização**. 
Uma "Generative Adversarial Network" (GAN) é um tipo diferente de rede neural, sendo duas redes que trabalham em conjunto. As `GANs` consistem em quaisquer duas redes (embora freqüentemente trate-se da combinação de uma `FFN` e uma `CNN`), onde uma supervisiona a geração do conteúdo, e a outra é responsável por julgar esse conteúdo. A rede discriminadora recebe dados de treinamento ou conteúdo gerado a partir da rede generativa. Como a rede discriminante pode prever corretamente se a fonte de dados é real ou artificial é então utilizada como parte do erro da rede geradora. **Isto cria uma forma de competição onde o discriminador está se tornando melhor na distinção entre dados reais e dados gerados, e o gerador está aprendendo a se tornar um melhor gerador**. No final deste processo, o discriminador é geralmente descartado, e acabamos com uma rede capaz de gerar dados extremamente verdadeiros (e.g., _[imagens falsas de rostos humanos](https://thispersondoesnotexist.com/)_).
`Neural Turing Machines` (NTM) podem ser entendidas como uma abstração das `LSTMs`, sendo uma tentativa de abrir a caixa preta. Em vez de codificar uma célula de memória diretamente em um neurônio, as `NTMs` têm células de memória separadas. Esta é uma tentativa de combinar a eficiência e a permanência do armazenamento digital regular com a eficiência e o poder expressivo das redes neurais. **A idéia é ter um banco de memória endereçável ao conteúdo e uma rede neural que possa ler e escrever a partir dele**. A "_Turing_" em `NTMs` vem do fato de que estas redes são Turing completas. Para saber mais sobre `NTM`, visite o [Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/). 
Uma função de ativação (por exemplo, `ReLU`, `GELU`, etc.) toma a soma ponderada de todas as entradas da camada anterior e gera um valor de saída (_normalmente não-linear_) para a camada seguinte em uma rede neural.
O viés (`math`) é uma intercepção ou offset de uma origem. O viés é referido como **b** ou **w0** em modelos de aprendizagem de máquina. O viés (`ético`) é um **estereótipo** ou favoritismo em relação a um grupo em relação a outros. Estes vieses podem afetar o comportamento do sistema de IA, e como os usuários interagem com tal sistema. Você pode aprender mais sobre `preconceitos éticos` em ML [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Fairness/disparate_impact_remove_Hogwarts.ipynb).
A `justiça contrafactual` é uma métrica de justiça que verifica se um classificador produz o mesmo resultado para um indivíduo, e para outro, que é idêntico ao primeiro, exceto no que diz respeito a um ou mais atributos sensíveis. Você pode aprender mais sobre `justiça contrafactual` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Explainability/Tabular%20Interpreter/fairness_xai_COMPAS.ipynb).
Estas são medidas para avaliar a `equidade` de um classificador treinado por aprendizagem de máquina (e.g., `Counterfactual Fairness`, `Demographic Parity`, `Predictive Parity`, `Equalized Probabilities`, e outras) Você pode aprender mais sobre `métricas de fairness` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Explainability/Tabular%20Interpreter/fairness_xai_COMPAS.ipynb).
A `entropia-cruzada` representa a **diferença entre duas distribuições de probabilidade**. Esta medida é geralmente utilizada para calcular a perda em problemas de classificação multiclasse. 
`Data Augmentation` é uma estratégia para aumentar o número de exemplos de treinamento, transformando as amostras existentes em "_amostras artificiais_". Você pode aprender mais sobre `data augmentation` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Adversarial/model_extraction_nlp.ipynb).
`Demographic Parity` é uma métrica de imparcialidade que é satisfeita se os resultados da classificação de um modelo não dependem de um atributo sensível em particular. Por exemplo, "_se tanto Ravenclaws como Hufflepuffs se aplicam a Hogwarts, a paridade demográfica é alcançada se a porcentagem de Ravenclaws admitidos for a mesma que a porcentagem de Hufflepuffs admitidos, independentemente de um grupo ser, em média, mais qualificado que o outro_". Você pode aprender mais sobre `demographic parity` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Fairness/disparate_impact_remove_Hogwarts.ipynb).
Em `aprendizagem por reforço`, o ambiente é aquilo que contém o agente, produzindo `estados de mundo` para que o agente interaja com ele. Por exemplo, o ambiente pode ser um tabuleiro de xadrez ou um labirinto. Quando o agente aplica uma ação ao ambiente, este ambiente transita para um novo `estados de mundo`.
`Predictive Parity` é uma métrica de equidade que verifica se, para um rótulo preferido (que confere uma vantagem ou benefício a uma pessoa) e um determinado atributo, um classificador atribui o rótulo preferido igualmente para todos os valores desse atributo. Você pode aprender mais sobre `predictive parity` [neste tutorial](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Fairness/disparate_impact_remove_Hogwarts.ipynb).
`Equalized Probabilities` é uma métrica de equidade que verifica se, para qualquer rótulo e atributo em particular, um classificador prediz esse rótulo igualmente bem para todos os valores desse atributo (_independentemente de esse rótulo ser um benefício ou um impacto_).
O problema do `vanishing gradient` é **encontrado no treinamento de redes neurais com métodos de aprendizagem baseados em descida de gradiente e retropropagação**. Em tais métodos, durante cada iteração de treinamento, cada um dos pesos da rede neural recebe uma atualização proporcional à derivada parcial da função de erro relativa ao peso atual. O problema é que, em alguns casos, ao longo das iterações, o gradiente desaparece, ou se torna muito pequeno, impedindo efetivamente que o peso altere seu valor. No pior caso, isto pode impedir completamente a rede neural de continuar o treinamento. O problema do `exploding gradient`, por outro lado, ocorre porque o gradiente dos modelos com arquiteturas profundas tende a tornar-se surpreendentemente íngreme. Gradientes íngremes resultam em atualizações muito grandes dos pesos de cada neurônio em uma rede neural profunda. **Sem uma regulação cuidadosa do tamanho do passo do gradiente, esse pode acabar "_explodindo_" de uma região convexa da paisagem de otimização**.  
Um `Falso Negativo` é um exemplo onde o modelo previu erroneamente a classe negativa. `Falso Positivo` é um exemplo em que o modelo previu erroneamente a classe positiva. Um `Verdadeiro Positivo` é um exemplo em que o modelo previu corretamente a classe positiva. `Verdadeiro Negativo` é um exemplo em que o modelo previu corretamente a classe negativa. Todas estas possibilidades são expressas em [matrizes de confusão](https://en.wikipedia.org/wiki/Confusion_matrix).
`Federated Learning` (FL) é uma abordagem distribuída de aprendizagem de máquinas **que treina modelos de aprendizagem de máquina utilizando exemplos descentralizados residentes em dispositivos como os smartphones**. Na aprendizagem federada, um subconjunto de dispositivos faz o download do modelo atual de um servidor de coordenação central. Os dispositivos usam os exemplos armazenados nos dispositivos para fazer melhorias no modelo. Os dispositivos então carregam as melhorias do modelo (_mas não os exemplos de treinamento_) para o servidor de coordenação, onde são agregados com outras atualizações para produzir um modelo global melhorado.
Os `hiperparâmetros` são os "_botões_" que você utiliza para controlar uma rede neural durante as sucessivas iterações de treinamento de um modelo (e.g., `taxa de aprendizagem`, `n de neurônios`, `n de camadas`, `taxa de dropout`, etc.). Enquanto isso, um `Parâmetro` é uma variável que o modelo treina por si só, por exemplo, _os pesos e os bias de um modelo de ML_.
`Interpretabilidade` (XAI) é a capacidade de explicar ou apresentar o raciocínio de um modelo ML em termos compreensíveis para um humano. Você pode aprender mais sobre `XAI` [nestes tutoriais](https://github.com/Nkluge-correa/teeny-tiny_castle).
Uma `NaN Trap` ocorre quando um parâmetro em seu modelo se torna um `NaN` (_Not A Number_) durante o treinamento, o que faz com que muitos (ou todos) outros parâmetros em seu modelo se tornem um `NaN`. 
Uma `função objetiva` (_em um problema de otimização matemática_) é uma função de valor real cujo valor deve ser minimizado ou maximizado sobre o conjunto de alternativas viáveis.
A `perplexidade` é uma medida de **o quão bem uma distribuição de probabilidade ou um modelo probabilístico prevê uma amostra**. Por exemplo, suponha que sua tarefa seja ler as primeiras letras de uma palavra que um usuário está digitando em seu smartphone e oferecer uma lista de possíveis palavras de preenchimento. `Perplexidade` (_para esta tarefa_) é aproximadamente o número de suposições que você precisa oferecer para que sua lista contenha a palavra real que o usuário está tentando digitar.
Um `atributo sensível` é um atributo humano que deve receber consideração especial por motivos legais, éticos, sociais ou pessoais (por exemplo, _raça, gênero, orientação sexual_, etc.).
`Descendência de Gradiente Estocástico` é um método iterativo para otimizar uma função objetiva com propriedades de suavidade adequadas (e.g., _funções diferenciáveis_). **`SGD` é um algoritmo de otimização freqüentemente utilizado em aplicações de aprendizagem de máquina para encontrar os parâmetros de um modelo que correspondem ao melhor ajuste entre as saídas previstas e a verdadeira distribuição dos dados**.
O `problema de alinhamento` é na verdade dois problemas: `alinhamento externo` e `Alinhamento interno`. `Alinhamento externo`: "_Assegurando que o objetivo base a ser otimizado está alinhado com as verdadeiras intenções e objetivos dos controladores_". `Alinhamento interno`: "_Garantir que o objetivo do otimizador base_ (e.g., `SGD`) _é alinhado com a mesa-objetivo do modelo criado._" De um ponto de vista ético/filosófico, **este é o problema de como especificar valores humanos para os modelos ML**. Para mais detalhes, leia "_[Risks from Learned Optimization in Advanced Machine Learning Systems](https://arxiv.org/abs/1906.01820)_'' para uma explicação mais detalhada.
O `problema de controle` é postulado a partir do seguinte argumento: "_Sistemas de IA existentes podem ser monitorados e facilmente desligados e modificados se eles se comportarem mal. Entretanto, uma superinteligência mal programada, que por definição poderia se tornar mais inteligente que seus controladores, poderia vir a impedir seu desligamento (modificação)_". O problema de controle pergunta: **Que precauções prévias os programadores podem tomar para evitar que uma superinteligência se comporte de forma catastrófica?**
O `AI Boxing` é um método de controle proposto **no qual uma IA é executada em um sistema de computador isolado com canais de entrada e saída altamente restritos**. Embora isto reduza a capacidade da IA de executar comportamentos indesejáveis, também reduz sua utilidade.
O `basilisco de Roko` é uma experiência de pensamento proposta em 2010 pelo usuário "_Roko_" na comunidade [Less Wrong](https://www.lesswrong.com/). **Roko utilizou idéias na teoria da decisão para argumentar que um agente de IA suficientemente poderoso teria um incentivo para torturar qualquer pessoa que imaginasse o agente mas não trabalhasse para trazê-lo à existência**. O argumento foi chamado de "_basilisco_" porque simplesmente ouvir o argumento supostamente colocaria você em risco de tortura por este agente hipotético (_um basilisco neste contexto é qualquer informação que prejudique ou ponha em perigo as pessoas que venham a conhecer esta informação_).
O Teorema do Acordo de Aumann diz, de grosso modo, que dois agentes agindo racionalmente (em um certo sentido preciso da palavra "_racional_") e com conhecimento comum das crenças um do outro, não podem concordar em discordar. Mais especificamente, **se duas pessoas são genuínas Bayesianas, compartilham os mesmos princípios comuns e têm conhecimento comum das atribuições de probabilidade atuais um do outro, tais pessoas devem ter atribuições de probabilidade iguais.**
A "teoria da decisão" é o estudo de princípios e algoritmos para tomar decisões corretas, ou seja, **decisões que permitem que um agente alcance melhores resultados em relação a seus objetivos**. Na Teoria da Decisão, cada ação, pelo menos implicitamente, representa uma decisão sob incerteza, ou seja, um estado de conhecimento parcial. _Quais são os mecanismos subjacentes aos processos de decisão?. O que é isso? E como pode ser feito melhor?_ Estas, e muitas outras perguntas, são o foco de interesse da Teoria da Decisão.
`Epidemiologia` é o estudo de como conhecemos o mundo. **É tanto um assunto em filosofia quanto uma preocupação prática com a forma como passamos a acreditar que as coisas são verdadeiras.**
Teoria dos jogos é o estudo formal de como os atores racionais interagem para buscar incentivos. **Nessa área, investiga-se situações de conflito e cooperação**.
O `Infra-Bayesianismo` é uma nova abordagem em epistemologia / teoria da decisão / aprendizagem por reforço, que se baseia na idéia de "_probabilidades imprecisas_" para **solucionar o problema do "_grão da verdade_" que atormenta o Bayesianismo e aprendizagem por reforço**. O `Infra-Bayesianismo` também leva naturalmente à implementação da `UDT` (_Updateless Decision Theory_) e (mais especulativamente) aplicações à teoria multiagente, agência incorporada e agência reflexiva.
`Updateless Decision Theory` (UDT) é uma teoria de decisão projetada para abordar um problema fundamental nas teorias de decisão existentes: "_a necessidade de tratar o agente como uma parte do mundo em que ele toma suas decisões_". 
O `Problema de Newcomb` é uma experiência de pensamento que explora os **problemas envolvidos na interação com agentes que podem prever (total ou parcialmente) nossas ações**. 
A `navalha de Occam` (_mais formalmente referida como o princípio da parcimônia_) é um princípio comumente dito como "_Entidades não devem ser multiplicadas além da necessidade_". Quando várias teorias são capazes de explicar as mesmas observações, a `navalha de Occam` sugere que a **temoria mais simples é preferível**. 
A `indução de Solomonoff` é um sistema de inferência definido por [Ray Solomonoff](https://de.wikipedia.org/wiki/Ray_Solomonoff). **Um sistema que segue tal método de inferência aprenderá a prever corretamente qualquer seqüência calculável com apenas a quantidade mínima absoluta de dados**. Este sistema, em certo sentido, é o algoritmo de previsão universal "_perfeito_".
Uma `função de utilidade` atribui valores numéricos ("_utilidade_") a **resultados, de modo que resultados com utilidades mais altas são sempre preferidos em relação a resultados com utilidades mais baixas**. 
A `Goodhart's Law` afirma que quando se escolhe um proxy como o valor para otimizar uma determinada tarefa, dada pressão suficiente, **o proxy não será mais uma boa medida de sucesso nessa tarefa**. A `Lei de Goodhart` é de particular relevância para o `problema do alinhamento`.
`Heurística` e  `vieses` são formas de raciocínio humano que nos diferem de um raciocinador ideal, devido a atalhos no raciocínio que nem sempre funcionam (`heurística`) e causam erros sistemáticos (`vieses`).
A Teoria do Processo Duplo postula **dois tipos de processos no cérebro humano**. Os dois processos consistem de um processo implícito, inconsciente (`Sistema 1`), e um processo explícito, consciente (`Sistema 2`).
Um `zumbi filosófico` ou (p-zumbi) é uma entidade hipotética que se parece e se comporta exatamente como um humano (_frequentemente estipulado para ser átomo por átomo idêntico a um humano_) mas não é consciente, ou seja, _não possui qualia_.
`AIXI` é um formalismo matemático para uma hipotética (super)inteligência, desenvolvido por Marcus Hutter ([2005](https://link.springer.com/book/10.1007/b138233), [2007](https://arxiv.org/abs/0712.3329)). Entretanto, `AIXI` não é computável. Mesmo assim, `AIXI` ainda é considerado uma ilustração teórica valiosa, com aspectos positivos e negativos.
`Coherent Extrapolated Volition` (CEV) foi um termo desenvolvido por [Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky) em trabalhos relacionados a `Friendly AI`. Significa que não seria suficiente programar explicitamente o que pensamos que nossos desejos e motivações são para uma IA, em vez disso **deveríamos encontrar uma maneira de programar tal sistema de forma a agir de acordo com o melhor interesse de nossos valores idealizados**.
Um agente `corrigível` não interfere no que intuitivamente veríamos como tentativas de "_corrigir_" o agente ou "_corrigir_" nossos erros em seu desenvolvimento. Um agente `corrigível` permitiria essas correções, embora o [teorema da convergência instrumental](https://en.wikipedia.org/wiki/Instrumental_convergence) dite o contrário. A `corrigibilidade` é uma propriedade importante na segurança e alinhamento da IA.
`Agência incorporada` é uma noção intuitiva de que a compreensão da teoria do agente racional deve levar em conta o fato de que os **agentes que criamos (e nós mesmos) são partes do mundo, não separados dele**. Isto contrasta com o atual modelo `cartesiano` (como a `indução de Solomonoff`), que pressupõe implicitamente uma separação entre o agente e o ambiente.
O `alinhamento interno` é o problema de garantir que os otimizadores de mesa-optimizadores (ou seja, _quando o próprio modelo treinado é um otimizador_) estejam alinhados com a função objetiva do otimizador base. Como exemplo, a evolução é uma força de otimização que "_gerou_" otimizadores (**humanos**) para atingir seus objetivos. **No entanto, seres humanos não maximizam prioritariamente o sucesso reprodutivo (eles usam técnicas de controle de natalidade e preferem se divertir do que maximizar sucesso reprodutivo)**. Isto é um fracasso de `alinhamento`.
`Convergência instrumental` é a **tendência hipotética da maioria dos agentes suficientemente inteligentes de perseguir uma série de objetivos instrumentais de forma independente de seus objetivos terminais**.
`Power Seeking Theorems`:  "_Quando as recompensas são distribuídas de forma IDD entre estados, é instrumentalmente convergente obter acesso a muitos estados finais_". No contexto dos processos de decisão de Markov, está provado que certas simetrias ambientais são suficientes para que **políticas ideais induzam a procura de poder**, ou seja, o controle sobre o meio ambiente. Para mais informações, procure por "_[Optimal Farsighted Agents Tend to Seek Power](https://arxiv.org/abs/1912.01683)_".
A `Tese de Ortogonalidade` afirma que a inteligência artificial pode ter qualquer combinação de nível de inteligência e objetivos terminais, ou seja, sua "_função de utilidade_" e "_inteligência_" podem variar independentemente. **Isto contrasta com a crença de que, por causa de sua inteligência, todas as formas de IA convergirão no final para um conjunto comum de metas**. Para saber mais, você pode ler "_[The Basic AI Drives](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)_".
A `incerteza lógica` é uma incerteza probabilística sobre as implicações das crenças. A 'Teoria da Probabilidade' geralmente assume omnisciência lógica, ou seja, conhecimento perfeito da lógica. Agentes realistas (_Bayesianos não perfeitos _) **não podem ser logicamente omniscientes**.
A `Mesa-Optimização` é a situação que ocorre quando um modelo aprendido (tal como uma rede neural) é, em si mesmo, um otimizador. Um otimizador base (e.g., `SGD`) otimiza e cria um `mesa-optimizador`. Anteriormente, o trabalho sob este conceito era chamado de `Inner Optimizers` e `Optimization Daemons`.
O `alinhamento externo` , no contexto do Aprendizado de Máquina, é a **extensão na qual a função objetiva especificada está alinhada com o objetivo pretendido por seus projetistas**. Esta é uma noção intuitiva, em parte porque as próprias intenções humanas não são bem compreendidas. Isto é o que normalmente é discutido como o problema "_[alinhamento de valores](https://intelligence.org/files/ValueLearningProblem.pdf)_".
O `Maximizador de Clipes de Papel` é uma inteligência artificial hipotética cuja função de utilidade valoriza algo que os humanos consideram quase que "_sem valor_," tal como maximizar o número de clipes de papel no universo. O `Maximizador de Clipes de Papel` é o experimento canônico de pensamento que procura mostrar como uma **inteligência artificial genérica, mesmo concebida de forma competente e sem malícia, poderia, em última análise, destruir a humanidade**.
`Auto aperfeiçoamento recursivo` refere-se a uma propriedade hipotética que `AGI` poderia vir a possuir, ou seja, a capacidade de auto aperfeiçoamento. Para mais informações, leia _[Large Language Models Can Self-Improve](https://arxiv.org/abs/2210.11610)_.
Uma `Volta Traiçoeira` é um evento hipotético onde um sistema avançado de IA "_finge_" ser alinhado devido a sua relativa fraqueza **mas se volta contra seus controladores uma vez que atinge poder suficiente para perseguir seu verdadeiro objetivo sem risco**. Para saber mais, leia "_[Catching Traacherous Turn: A Model of the Multilevel AI Boxing](https://philarchive.org/archive/TURCTT)._".
`Humans Consulting HCH` (HCH) é um acrônimo recursivo que descreve um cenário onde **humanos podem consultar simulações de si mesmos para ajudar a responder perguntas**. É um conceito utilizado na discussão relacionada à [amplificação iterada e debate](https://www.lesswrong.com/posts/vhfATmAoJcN8RqGg6/a-guide-to-iterated-amplification-and-debate).
`Amplificação iterada` (também conhecida como IDA) é uma estratégia alternativa de **treinamento que constrói progressivamente um sinal de treinamento para problemas difíceis de especificar, combinando soluções para sub-problemas mais fáceis**. A Amplificação Iterada está intimamente relacionada à Iteração Especializada (ou seja, a metodologia utilizada para o treinamento [Alpha Go](https://www.nature.com/articles/nature16961)). Para mais informações, consulte "_[Supervising strong learners by amplifying weak experts](https://arxiv.org/abs/1810.08575)_."
`Iteração Especializada` (ExIt) é um algoritmo de aprendizado por  reforço que decompõe o problema em tarefas de planejamento e generalização. **O planejamento de novas políticas é realizado por busca em árvore Monte Carlo, enquanto uma rede neural profunda generaliza esses planos**. Posteriormente, a **busca de árvores é melhorada utilizando a política da rede neural para orientar a busca, aumentando a eficiência da próxima fase de busca**. Para mais informações, veja "_[Thinking Fast and Slow with Deep Learning and Tree Search](https://arxiv.org/abs/1705.08439)_."
`Medidas de impacto` penalizam uma IA por afetar demais o meio ambiente. Para reduzir o risco representado por um sistema de IA, você pode **querer fazer com que ele tente atingir seus objetivos com o menor impacto possível sobre o meio ambiente**. As `medidas de impacto` são maneiras de medir o que é "_impacto_".
O `aprendizado de valor` é um método proposto para incorporar valores humanos em uma IA. A `aprendizagem de valores` envolve a criação de um aprendiz artificial cujas ações são guiadas por valores humanos aprendidos. Para mais informações, leia "_[The Value Learning Problem](https://intelligence.org/files/ValueLearningProblem.pdf)_."
Em `AI governance` procura-se desenvolver **métodos para assegurar que a sociedade se beneficie de nossa crescente adoção e utilização de tecnologias de IA**. 
`AI Risk` é a análise dos riscos associados à construção de sistemas de IA. Os modelos de IA podem possuir vulnerabilidades complexas que criam riscos peculiares. Vulnerabilidades tais como "_extração de modelos_" (i.e., _ataques destinados a duplicar um modelo de ML_) e "_envenenamento de dados_" (i.e., _ataques destinados a adulterar dados de treinamento_) podem representar novos desafios para abordagens de segurança. Para saber mais sobre ataques contra modelos ML, veja os tutoriais [neste repositório](https://github.com/Nkluge-correa/teeny-tiny_castle).
`Takeoff` refere-se ao processo no qual uma "_Seed AI_ ", com um certo limite de capacidade, seria capaz de se aperfeiçoar para se tornar uma `AGI`. Há um debate sobre se, realisticamente, a velocidade de uma `Takeoff` é mais provável que seja lenta (_gradual_) ou rápida (_abrupta_). Para mais informações, leia _[Singularidade e problemas de coordenação: Lições pandêmicas a partir de 2020](https://jfsdigital.org/wp-content/uploads/2021/09/05-Nicholas-Kluge-Correa-56-Article-ED-9-Layout.pdf)_.
`AI Timelines`  refere-se à discussão de quanto tempo até que vários marcos no progresso da IA sejam alcançados, e.g., IA a nível humano, emular um cérebro humano, e outros. `AI Timelines` devem ser distinguidas das `Takeoff`, que lidam com a dinâmica do progresso da IA após o desenvolvimento de uma IA a nível humano ou de uma Seed AI. Para mais informações, leia _[Singularidade e problemas de coordenação: Lições pandêmicas a partir de 2020](https://jfsdigital.org/wp-content/uploads/2021/09/05-Nicholas-Kluge-Correa-56-Article-ED-9-Layout.pdf)_.
`Transformative AI` é um termo utilizado para se referir às tecnologias de IA que **poderiam eventualmente precipitar uma transição comparável, por exemplo, com a revolução agrícola ou industrial**. Isto é semelhante ao conceito de superinteligência ou AGI, mas sem menção ao "_nível de inteligência ou generalidade_" de tal sistema.
O termo `Generative Pretrained Transformer` (GPT) refere-se a uma família de grandes modelos de linguagem, baseados na arquitetura `transformer`, criados pelo [OpenAI](https://openai.com/). 
`Narrow AI` é um termo utilizado para se referir a sistemas capazes de operar apenas em um domínio relativamente limitado, como xadrez ou condução, em vez de ser capaz de aprender uma ampla gama de tarefas como um humano ou uma `AGI`. **Narrow vs Strong/General não é uma classificação binária perfeita, pois existem graus de generalidade**, por exemplo, grandes modelos de linguagem têm um grande grau de generalidade sem ser tão geral quanto um humano.
`Emulação Cerebral Completa` é a simulação ou transferência das informações contidas dentro de um cérebro para um substrato computacional. Através de uma `Emulação Cerebral Completa`, teoricamente, criar-se-ia "_inteligência de máquina genuína_". Este conceito é freqüentemente discutido no contexto da **Filosofia da Mente**. Para mais informações, veja "_[Whole Brain Emulation: A Roadmap](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf)_."
`Consequencialismo` é uma família de **teorias éticas que ditam que as pessoas devem escolher suas ações com base nos resultados que esperam obter**. Vários tipos de `conseqüencialismo` especificam como os resultados devem ser julgados. O `Consequencialismo` é uma das três principais vertentes do pensamento ético, juntamente com a deontologia e a ética da virtude.
A `Deontologia` é uma família de teorias éticas que dita que as pessoas **devem escolher suas ações com base em uma lista prescrita de normas morais** e é uma teoria de moralidade baseada na obediência às regras morais.
A `Ética das Virtudes` é uma classe de teorias éticas que tratam o conceito de virtude moral como central para a ética. `Ética das Virtudes` é geralmente contrastada com duas outras abordagens principais na ética normativa, o `consequencialismo` e a `deontologia`. **`Ética das Virtudes` define o comportamento moralmente correto como aquele em que se demonstram virtudes como bravura, lealdade, ou sabedoria**.
`Metaética` é um campo de estudo que tenta compreender as características metafísicas, epistemológicas e semânticas, assim como os fundamentos e o alcance dos valores morais **. Na metaética, os filósofos se preocupam com questões e problemas como "_Os juízos morais são objetivos ou subjetivos, relativos ou absolutos?_", "Há fatos morais?_" ou "Como aprendemos valores morais?_".**
`Incerteza moral` (ou incerteza normativa) é a incerteza sobre o que devemos fazer, moralmente, dada a diversidade das doutrinas morais. 
Um `risco existencial` é um risco que apresenta conseqüências negativas astronomicamente grandes para a humanidade, tais como **a extinção humana, ou totalitarismo global permanente**.
A `complexidade de valores` é a tese de que os valores humanos têm alta complexidade de Kolmogorov, ou seja, que **preferências humanas não podem ser resumidas ha regras simples, ou comprimidas em um algoritmo menor do que a descrição completa de tais valores**. A `fragilidade de valor` é a tese de que perder mesmo uma pequena parte das regras que compõem nossos valores poderia levar a resultados que a maioria de nós consideraria inaceitáveis. Para mais informações, leia "_[Complex Value Systems are Required to Realize Valuable Futures](https://intelligence.org/files/ComplexValues.pdf)_".
A `Ética da IA` é o ramo da ética específico para as preocupações relacionadas aos sistemas de IA. **A Ética da IA é às vezes dividida entre preocupações relacionadas ao comportamento moral dos humanos ao projetar, fazer e usar sistemas de IA, e preocupações relacionadas ao comportamento das máquinas, ou seja, máquinas agindo "_moralmente_. "** Quando o tópico é `AI Ethics`, eu posso lhe falar sobre `Accountability`, `AI Ethics Statistics`, `AI Governance`, `AI-Principles`, `Aumann's Agreement Theorem`, `Beneficence & Non-Maleficence`, `Consequentialism`, `Counterfactual Fairness`, `Decision Theory`, `Demographic Parity`, `Deontology`, `Dignity`, `Diversity`, `Dual Process Theory`, `Epistemology`, `Equalized Odds`, `Fairness Metrics`, `Federated Learning`, `Freedom, Game Theory`, `Heuristics & Biases`, `Human Formation`, `Human-centeredness`, `Infra-Bayesianism`, `Intellectual Property`, `Justice`, `Labor Rights`, `Logical Uncertainty`, `Metaethics`, `Moral Uncertainty`, `Newcomb Problems`, `Occam's razor`, `Open source`, `Predictive Parity`, `Privacy`, `p-Zumbi`, `Reliability`, `Rights of Children and Adolescents`, `Sensitive Attribute`, `Solomonoff Induction`, `Sustainability`, `Transparency`, `Truthfulness`, `Updateless Decision Theory`, `Value Complexity`, `Value Learning`, `Virtue Ethics`, entre outros conceitos.
`AI Safety` é uma área de pesquisa em aprendizagem de máquina que visa identificar as causas do comportamento não intencional dos sistemas criados pela aprendizagem de máquina, onde **busca-se desenvolver ferramentas para garantir que tais sistemas funcionem de forma segura e confiável**. Quando o tema é `Segurança da IA`, posso falar-lhe sobre `AI Boxing`, `AI Ethics Statistics`, `AI Ethics`, `AI-Risk`, `AI-Timelines`, `Alignment Problem`, `Coherent Extrapolated Volition`, `Control Problem`, `Corrigibility`, `Embedded Agency`, `Existential Risk`, `Expert Iteration`, `External Security`, `Goodhart's Law`, `Humans Consulting HCH`, `Impact Measures`, `Inner-Alignment`, `Instrumental Convergence`, `Intelligence Explosion`, `Iterated Distillation and Amplification`, `Mesa-Optimization`, `Monitoring`, `Orthogonality Thesis`, `Outer-Alignment`, `Paperclip Maximizer`, `Power Seeking Theorems`, `Recursive Self-Improvement`, `Robustness`, `Takeoff`, `Transformative-AI`, `Treacherous Turn`, `Value Complexity`, `Value Learning`, entre outros conceitos.
`Robustez` diz respeito a **criação de modelos que são resistentes a ataques adversariais e situações incomuns**. Os modelos treinados pela aprendizagem de máquina ainda são frágeis e rígidos, não operando bem em ambientes dinâmicos e mutáveis. Você pode aprender mais sobre `robustez a ataques adversariais` [neste repositório](https://github.com/Nkluge-correa/teeny-tiny_castle).
`Monitoramento` diz respeito a **detecção de uso malicioso, mau funcionamento ou funcionalidade não intencional** que pode estar presente nos modelos de ML.
`Segurança Externa` tem a ver com o fato de que **modelos podem ser embutidos em ambientes inseguros, tais como software mau projetado e organizações mal estruturadas**. 
Para dados estatísticos sobre Ética da IA, visite o painel do estudo "_[Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance](https://www.airespucrs.org/worldwide-ai-ethics)_." Para mais informações, leia o [artigo completo](https://arxiv.org/abs/2206.11922).
Aqui está uma lista de princípios em Ética da IA: `Beneficência`, `Confibilidade`, `Direitos da Criança e do Adolescente`, `Direitos Humanos`, `Direitos do Trabalho`, `Diversidade`, `Formação Humana`, `Centralidade Humana`, `Propriedade Intelectual`, `Justiça`, `Liberdade`, `Cooperação`, `Privacidade`, `Responsabilidade`, `Sustentabilidade`, `Transparência`, e `Veracidade`. Para mais informações, visite o painel do estudo "_[Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance](https://www.airespucrs.org/worldwide-ai-ethics)_". 
`Responsabilização` refere-se à idéia de que os desenvolvedores e comerciantes de tecnologias de IA **devem estar em conformidade com os órgãos reguladores**, o que também significa que tais atores devem ser responsáveis por suas ações e pelos impactos causados por suas tecnologias.
`Beneficência` e `não-maleficência` são conceitos que vêm da bioética e da ética médica, e na ética da IA, eles afirmam que **o bem-estar humano (e a aversão aos danos) deve ser o objetivo das tecnologias impulsionadas pela IA**. 
No contexto da Ética da IA, **advoca a idéia de que os direitos das crianças e adolescentes devem ser respeitados pelas tecnologias que utilizam IA**. As partes interessadas na questão da IA devem salvaguardar, respeitar e estar conscientes das fragilidades associadas aos jovens.
`Dignidade` é um princípio baseado na idéia de que **todos os indivíduos merecem tratamento adequado e respeito**. Na Ética da IA, o respeito à dignidade humana está freqüentemente ligado aos direitos humanos (i.e., "_[Declaração Universal dos Direitos Humanos](https://www.un.org/en/about-us/universal-declaration-of-human-rights)_").
`Diversidade` defende a idéia de que o desenvolvimento e o uso de tecnologias de IA **deve ser feito de forma inclusiva e acessível, respeitando as diferentes formas que a entidade humana pode vir a se expressar**. 
A `autonomia` da decisão humana **deve ser preservada durante as interações humano-AI, seja essa escolha individual ou a liberdade de escolher em conjunto**, como a inviolabilidade dos direitos e valores democráticos, estando também ligada à auto-suficiência tecnológica das nações/estados.
A `formação humana` e a `educação` são princípios baseados na idéia de que **a formação e a educação humana devem ser priorizadas em nossos avanços tecnológicos**. As tecnologias de IA exigem um nível considerável de especialização para serem produzidas e operadas, e tal conhecimento deve ser acessível a todos.
A `centralidade humana` é um princípio baseado na idéia de que **sistemas de IA devem ser centrados e alinhados com os valores humanos**. As tecnologias de IA devem ser adaptadas para se alinharem aos nossos valores (e.g., _design sensível à valores_).
`Propriedade Intelectual` procura fundamentar os direitos de propriedade sobre produtos e/ou processos de conhecimento gerado por indivíduos, sejam eles tangíveis ou intangíveis.
`Equidade` sustenta a idéia de **não-discriminação e mitigação de preconceitos** (_sistemas de IA podem estar sujeitos a preconceitos algorítmicos discriminatórios_). O princípio de `equidade` defende a idéia de que, independentemente dos diferentes atributos sensíveis que possam caracterizar um indivíduo, todos devem ser tratados "_de forma justa_".
Os `direitos trabalhistas` são direitos legais e humanos relacionados às relações de trabalho entre trabalhadores e empregadores. Na Ética da AI, **este princípio enfatiza que os direitos dos trabalhadores devem ser preservados, independentemente de as relações de trabalho estarem sendo mediadas por tecnologias que utilizam IA ou não**. 
`Cooperação` defende diferentes meios pelos quais **as ações conjuntas podem ser estabelecidas e cultivadas entre as partes interessadas na IA para alcançar objetivos comuns**. Ela também defende a troca livre e aberta de ativos valiosos de IA para mitigar possíveis monopólios de IA.
`Privacidade` pode ser definida como o direito do indivíduo de "_expor-se voluntariamente, e na medida do desejado, ao mundo_". Na Ética da IA, este princípio sustenta o **direito de uma pessoa de controlar a exposição e disponibilidade de informações pessoais quando mineradas como dados para o treinamento de sistemas de IA**.
`Confiabilidade` é a idéia de que as tecnologias de IA **devem ser confiáveis, no sentido de que seu uso pode ser comprovadamente atestado como seguro e robusto**, promovendo a confiança do usuário e uma melhor aceitação das tecnologias de IA.
`Sustentabilidade` pode ser entendida como uma forma de "_justiça intergeracional_", onde o bem-estar das gerações futuras também deve ser contado durante o desenvolvimento da IA. Na Ética da IA, sustentabilidade refere-se à idéia de que o desenvolvimento de tecnologias de IA **deve ser realizado com consciência de suas implicações a longo prazo, tais como custos ambientais e preservação da vida não-humana/bem estar**.
`Transparência` apóia a idéia de que o uso e desenvolvimento de tecnologias de IA **deve ser feito de forma transparente para todas as partes interessadas**. A transparência pode estar relacionada com "_a transparência de uma organização_" ou "_a transparência de um algoritmo_".
`Veracidade` sustenta a idéia de que **As tecnologias de IA devem fornecer informações verdadeiras**. Está também relacionada à idéia de que as pessoas não devem ser enganadas quando interagem com sistemas de IA.
Eu não tenho relógio... mas aposto que seu computador tem!
Eu não sou um oráculo, pergunte ao Google! Ele é bom nisso. Eu sou apenas um chatbot de domínio fechado, **não um motor de busca**.
Espero ter ajudado você, volte sempre! 
**Você está sendo mal-educado...**
`Ética` ou `Filosofia moral` é um ramo da filosofia que, junto com a `Estética`, compreende o que chamamos de `Axiologia` (ou `Teoria do Valor`). Em `Ética`, procuramos **sistematizar, defender e recomendar conceitos de comportamento certo e errado**. A Ética Ocidental pode ser dividida em três grandes escolas: `Conseqüencialismo`, `Deontologia`, e `Ética das Virtudes`.
Podemos definir a palavra `artificial` como algo **feito por humanos**, em vez de algo que ocorre naturalmente, como uma cópia de algo natural (e.g., _luz artificial em vez de luz solar_).
Desculpe, ou eu não entendi a pergunta, ou ela não faz parte do meu domínio de especialização... :( Tente perguntar de outra forma ou usando outras palavras. Talvez então eu possa ajudar você!