{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Domain Chatbot Maker\n",
    "\n",
    "![chatbot-gig](https://media2.giphy.com/media/S60CrN9iMxFlyp7uM8/giphy.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi lingual --> tags_bilingual.txt\n",
    "# pt --> tags_pt.txt\n",
    "# en --> tags_en.txt\n",
    "\n",
    "# 614 p\n",
    "\n",
    "with open('data\\\\tags_bilingual.txt', encoding='utf8') as file_in:\n",
    "    X = [' '.join(line.strip().split(' ')[:-1]) for line in file_in]\n",
    "with open('data\\\\tags_bilingual.txt', encoding='utf8') as file_in:\n",
    "    Y = [line.strip().split(' ')[-1] for line in file_in]\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "vocab_size = 2500 #2500 for bilingual or 1500 for english or portuguese\n",
    "embed_size = 256\n",
    "max_len = 10\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", ngrams=2)\n",
    "text_vectorization.adapt(X)\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "\n",
    "with open(r'pre_trained_aira\\\\vocabulary_bilingual.txt', 'w') as fp:\n",
    "    for word in vocabulary:\n",
    "        fp.write(\"%s\\n\" % word)\n",
    "    \n",
    "encoded_X = text_vectorization(X)\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "encoded_X_padded = pad_sequences(encoded_X, maxlen=max_len, truncating='post')\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "one_hot_encoded_Y = to_categorical(Y)[:,1:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(encoded_X_padded, one_hot_encoded_Y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Bi-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "vocab_size = 2500 #2500 for bilingual or 1500 for english or portuguese\n",
    "embed_size = 256\n",
    "max_len = 10\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "embedded = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, mask_zero=True)(inputs)\n",
    "\n",
    "x = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True))(embedded)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(x)\n",
    "x = tf.keras.layers.Dropout(0.8)(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(142, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"pre_trained_aira\\\\Aira_BiLSTM_bilingual.keras\",\n",
    "                                                save_best_only=True,\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                patience=10, \n",
    "                                                restore_best_weights=True)]\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_split = 0.2,\n",
    "          epochs=100,\n",
    "          batch_size=8,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"pre_trained_aira\\\\Aira_BiLSTM_bilingual.keras\")\n",
    "test_loss_score, test_acc_score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Final Loss: {round(test_loss_score, 1)}.')\n",
    "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (Dedocer-Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim \n",
    "        self.dense_dim = dense_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "        layers.Dense(embed_dim),]\n",
    "        )\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None): \n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            })\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding( \n",
    "        input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "        input_dim=sequence_length, output_dim=output_dim) \n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions \n",
    "\n",
    "    def compute_mask(self, inputs, mask=None): \n",
    "        return tf.math.not_equal(inputs, 0) \n",
    "\n",
    "    def get_config(self): \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"output_dim\": self.output_dim,\n",
    "        \"sequence_length\": self.sequence_length,\n",
    "        \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2500 #2500 for bilingual or 1500 for english or portuguese\n",
    "embed_size = 256\n",
    "max_len = 10\n",
    "num_heads = 6\n",
    "dense_dim = 128\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "x = PositionalEmbedding(max_len, vocab_size, embed_size)(inputs)\n",
    "x = TransformerEncoder(embed_size, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x) \n",
    "x = layers.Dropout(0.8)(x)\n",
    "\n",
    "outputs = layers.Dense(142, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"pre_trained_aira\\\\Aira_transformer_bilingual.keras\",\n",
    "                                                save_best_only=True,\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                patience=10, \n",
    "                                                restore_best_weights=True)]\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_split = 0.2,\n",
    "          epochs=40,\n",
    "          batch_size=8,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"pre_trained_aira\\\\Aira_transformer_bilingual.keras\", \n",
    "                                custom_objects={\"TransformerEncoder\": TransformerEncoder, \n",
    "                                                 \"PositionalEmbedding\": PositionalEmbedding})\n",
    "test_loss_score, test_acc_score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Final Loss: {round(test_loss_score, 1)}.')\n",
    "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "model = keras.models.load_model(\"pre_trained_aira\\\\Aira_BiLSTM_bilingual.keras\")\n",
    "\n",
    "# to load the transformer, pass the \n",
    "#\n",
    "# 'custom_objects={\"TransformerEncoder\": TransformerEncoder, \n",
    "#                   \"PositionalEmbedding\": PositionalEmbedding}' \n",
    "# \n",
    "# argument.\n",
    "\n",
    "with open(r'pre_trained_aira\\\\vocabulary_bilingual.txt', 'r') as fp:\n",
    "    vocabulary = [line[:-1] for line in fp]\n",
    "\n",
    "with open('data\\\\answers_en.txt', encoding='utf8') as file_in:\n",
    "    answers = [line.strip() for line in file_in]\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "vocab_size = 2500 #2500 for bilingual or 1500 for english or portuguese\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=vocab_size, \n",
    "                                        output_mode=\"int\", \n",
    "                                        ngrams=2,\n",
    "                                        vocabulary=vocabulary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions: what is Interpretability?\n",
      "\n",
      "Encoded Sentence: [   3    7 1281    8 1975]\n",
      "\n",
      "Padded Encoded Sentence: [[   0    0    0    0    0    3    7 1281    8 1975]]\n",
      "\n",
      "Answers: Interpretability is the ability to explain or present the reasoning of an ML model in terms understandable to a human. [Confidence:  100.00 %]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = '''what is Interpretability?'''\n",
    "#text = '''What is the problem of alignment?'''\n",
    "#text = '''O que é Interpretabilidade?'''\n",
    "#text = '''O que é o problema de Alinhamento?'''\n",
    "#text = '''What is Machine Learning?'''\n",
    "#text = '''O que é Ética das Virtudes?'''\n",
    "#text = '''What is your name?'''\n",
    "#text = '''Qual é o seu nome?'''\n",
    "#text = '''O que é SGD?'''\n",
    "#text = '''What is Stochastic Gradient Descent?'''\n",
    "\n",
    "print(f'Questions: {text}\\n')\n",
    "\n",
    "encoded_sentence = text_vectorization(text.lower().translate(str.maketrans('', '', string.punctuation)))\n",
    "print(f'Encoded Sentence: {encoded_sentence}\\n')\n",
    "\n",
    "encoded_sentence_padded = pad_sequences([encoded_sentence], maxlen=10, truncating='post')\n",
    "print(f'Padded Encoded Sentence: {encoded_sentence_padded}\\n')\n",
    "\n",
    "preds = model.predict(encoded_sentence_padded,verbose=0)[0]\n",
    "output = answers[np.argmax(preds)]\n",
    "print(f'Answers: {output} [Confidence: {max(preds) * 100: .2f} %]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
